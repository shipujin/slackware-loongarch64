diff --git a/Makefile.vex.am b/Makefile.vex.am
index c1244a6..8137bee 100644
--- a/Makefile.vex.am
+++ b/Makefile.vex.am
@@ -26,6 +26,7 @@ pkginclude_HEADERS = \
 	pub/libvex_guest_s390x.h \
 	pub/libvex_guest_mips32.h \
 	pub/libvex_guest_mips64.h \
+	pub/libvex_guest_loongarch64.h \
 	pub/libvex_s390x_common.h \
 	pub/libvex_ir.h \
 	pub/libvex_trc_values.h \
@@ -49,6 +50,7 @@ noinst_HEADERS = \
 	priv/guest_mips_defs.h \
 	priv/mips_defs.h \
 	priv/guest_nanomips_defs.h \
+	priv/guest_loongarch64_defs.h \
 	priv/host_generic_regs.h \
 	priv/host_generic_simd64.h \
 	priv/host_generic_simd128.h \
@@ -65,7 +67,8 @@ noinst_HEADERS = \
 	priv/s390_defs.h \
 	priv/host_mips_defs.h \
 	priv/host_nanomips_defs.h \
-	priv/common_nanomips_defs.h
+	priv/common_nanomips_defs.h \
+	priv/host_loongarch64_defs.h
 
 BUILT_SOURCES = pub/libvex_guest_offsets.h
 CLEANFILES    = pub/libvex_guest_offsets.h
@@ -94,7 +97,8 @@ pub/libvex_guest_offsets.h: auxprogs/genoffsets.c \
 			    pub/libvex_guest_arm64.h \
 			    pub/libvex_guest_s390x.h \
 			    pub/libvex_guest_mips32.h \
-			    pub/libvex_guest_mips64.h
+			    pub/libvex_guest_mips64.h \
+			    pub/libvex_guest_loongarch64.h
 	rm -f auxprogs/genoffsets.s
 	$(mkdir_p) auxprogs pub
 	$(CC) $(CFLAGS_FOR_GENOFFSETS) \
@@ -152,6 +156,8 @@ LIBVEX_SOURCES_COMMON = \
 	priv/guest_mips_toIR.c \
 	priv/guest_nanomips_helpers.c \
 	priv/guest_nanomips_toIR.c \
+	priv/guest_loongarch64_helpers.c \
+	priv/guest_loongarch64_toIR.c \
 	priv/host_generic_regs.c \
 	priv/host_generic_simd64.c \
 	priv/host_generic_simd128.c \
@@ -176,7 +182,9 @@ LIBVEX_SOURCES_COMMON = \
 	priv/host_mips_defs.c \
 	priv/host_nanomips_defs.c \
 	priv/host_mips_isel.c \
-	priv/host_nanomips_isel.c
+	priv/host_nanomips_isel.c \
+	priv/host_loongarch64_defs.c \
+	priv/host_loongarch64_isel.c
 
 LIBVEXMULTIARCH_SOURCES = priv/multiarch_main_main.c
 
diff --git a/VEX/auxprogs/genoffsets.c b/VEX/auxprogs/genoffsets.c
index 6b70cd0..9f4b237 100644
--- a/VEX/auxprogs/genoffsets.c
+++ b/VEX/auxprogs/genoffsets.c
@@ -53,6 +53,7 @@
 #include "../pub/libvex_guest_s390x.h"
 #include "../pub/libvex_guest_mips32.h"
 #include "../pub/libvex_guest_mips64.h"
+#include "../pub/libvex_guest_loongarch64.h"
 
 #define VG_STRINGIFZ(__str)  #__str
 #define VG_STRINGIFY(__str)  VG_STRINGIFZ(__str)
@@ -265,6 +266,41 @@ void foo ( void )
    GENOFFSET(MIPS64,mips64,PC);
    GENOFFSET(MIPS64,mips64,HI);
    GENOFFSET(MIPS64,mips64,LO);
+
+   // LOONGARCH64
+   GENOFFSET(LOONGARCH64,loongarch64,R0);
+   GENOFFSET(LOONGARCH64,loongarch64,R1);
+   GENOFFSET(LOONGARCH64,loongarch64,R2);
+   GENOFFSET(LOONGARCH64,loongarch64,R3);
+   GENOFFSET(LOONGARCH64,loongarch64,R4);
+   GENOFFSET(LOONGARCH64,loongarch64,R5);
+   GENOFFSET(LOONGARCH64,loongarch64,R6);
+   GENOFFSET(LOONGARCH64,loongarch64,R7);
+   GENOFFSET(LOONGARCH64,loongarch64,R8);
+   GENOFFSET(LOONGARCH64,loongarch64,R9);
+   GENOFFSET(LOONGARCH64,loongarch64,R10);
+   GENOFFSET(LOONGARCH64,loongarch64,R11);
+   GENOFFSET(LOONGARCH64,loongarch64,R12);
+   GENOFFSET(LOONGARCH64,loongarch64,R13);
+   GENOFFSET(LOONGARCH64,loongarch64,R14);
+   GENOFFSET(LOONGARCH64,loongarch64,R15);
+   GENOFFSET(LOONGARCH64,loongarch64,R16);
+   GENOFFSET(LOONGARCH64,loongarch64,R17);
+   GENOFFSET(LOONGARCH64,loongarch64,R18);
+   GENOFFSET(LOONGARCH64,loongarch64,R19);
+   GENOFFSET(LOONGARCH64,loongarch64,R20);
+   GENOFFSET(LOONGARCH64,loongarch64,R21);
+   GENOFFSET(LOONGARCH64,loongarch64,R22);
+   GENOFFSET(LOONGARCH64,loongarch64,R23);
+   GENOFFSET(LOONGARCH64,loongarch64,R24);
+   GENOFFSET(LOONGARCH64,loongarch64,R25);
+   GENOFFSET(LOONGARCH64,loongarch64,R26);
+   GENOFFSET(LOONGARCH64,loongarch64,R27);
+   GENOFFSET(LOONGARCH64,loongarch64,R28);
+   GENOFFSET(LOONGARCH64,loongarch64,R29);
+   GENOFFSET(LOONGARCH64,loongarch64,R30);
+   GENOFFSET(LOONGARCH64,loongarch64,R31);
+   GENOFFSET(LOONGARCH64,loongarch64,PC);
 }
 
 /*--------------------------------------------------------------------*/
diff --git a/VEX/priv/guest_loongarch64_defs.h b/VEX/priv/guest_loongarch64_defs.h
new file mode 100644
index 0000000..c3b572c
--- /dev/null
+++ b/VEX/priv/guest_loongarch64_defs.h
@@ -0,0 +1,131 @@
+
+/*---------------------------------------------------------------*/
+/*--- begin                          guest_loongarch64_defs.h ---*/
+/*---------------------------------------------------------------*/
+
+/*
+   This file is part of Valgrind, a dynamic binary instrumentation
+   framework.
+
+   Copyright (C) 2021-2022 Loongson Technology Corporation Limited
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, see <http://www.gnu.org/licenses/>.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+/* Only to be used within the guest-loongarch64 directory. */
+
+#ifndef __VEX_GUEST_LOONGARCH64_DEFS_H
+#define __VEX_GUEST_LOONGARCH64_DEFS_H
+
+#include "libvex_basictypes.h"
+#include "guest_generic_bb_to_IR.h"  /* DisResult */
+
+
+/*---------------------------------------------------------*/
+/*--- loongarch64 to IR conversion                      ---*/
+/*---------------------------------------------------------*/
+
+/* Convert one LOONGARCH64 insn to IR.  See the type DisOneInstrFn in
+   guest_generic_bb_to_IR.h. */
+extern DisResult disInstr_LOONGARCH64 ( IRSB*              irsb_IN,
+                                        const UChar*       guest_code_IN,
+                                        Long               delta,
+                                        Addr               guest_IP,
+                                        VexArch            guest_arch,
+                                        const VexArchInfo* archinfo,
+                                        const VexAbiInfo*  abiinfo,
+                                        VexEndness         host_endness_IN,
+                                        Bool               sigill_diag_IN );
+
+/* Used by the optimiser to specialise calls to helpers. */
+extern IRExpr* guest_loongarch64_spechelper ( const HChar* function_name,
+                                              IRExpr**     args,
+                                              IRStmt**     precedingStmts,
+                                              Int          n_precedingStmts );
+
+/* Describes to the optimser which part of the guest state require
+   precise memory exceptions.  This is logically part of the guest
+   state description. */
+extern Bool guest_loongarch64_state_requires_precise_mem_exns ( Int minoff,
+                                                                Int maxoff,
+                                                                VexRegisterUpdates pxControl );
+
+extern VexGuestLayout loongarch64Guest_layout;
+
+
+/*---------------------------------------------------------*/
+/*--- loongarch64 guest helpers                         ---*/
+/*---------------------------------------------------------*/
+
+enum fpop {
+   FADD_S, FADD_D, FSUB_S, FSUB_D,
+   FMUL_S, FMUL_D, FDIV_S, FDIV_D,
+   FMADD_S, FMADD_D, FMSUB_S, FMSUB_D,
+   FNMADD_S, FNMADD_D, FNMSUB_S, FNMSUB_D,
+   FMAX_S, FMAX_D, FMIN_S, FMIN_D,
+   FMAXA_S, FMAXA_D, FMINA_S, FMINA_D,
+   FABS_S, FABS_D, FNEG_S, FNEG_D,
+   FSQRT_S, FSQRT_D,
+   FRECIP_S, FRECIP_D,
+   FRSQRT_S, FRSQRT_D,
+   FSCALEB_S, FSCALEB_D,
+   FLOGB_S, FLOGB_D,
+   FCMP_CAF_S, FCMP_CAF_D, FCMP_SAF_S, FCMP_SAF_D,
+   FCMP_CLT_S, FCMP_CLT_D, FCMP_SLT_S, FCMP_SLT_D,
+   FCMP_CEQ_S, FCMP_CEQ_D, FCMP_SEQ_S, FCMP_SEQ_D,
+   FCMP_CLE_S, FCMP_CLE_D, FCMP_SLE_S, FCMP_SLE_D,
+   FCMP_CUN_S, FCMP_CUN_D, FCMP_SUN_S, FCMP_SUN_D,
+   FCMP_CULT_S, FCMP_CULT_D, FCMP_SULT_S, FCMP_SULT_D,
+   FCMP_CUEQ_S, FCMP_CUEQ_D, FCMP_SUEQ_S, FCMP_SUEQ_D,
+   FCMP_CULE_S, FCMP_CULE_D, FCMP_SULE_S, FCMP_SULE_D,
+   FCMP_CNE_S, FCMP_CNE_D, FCMP_SNE_S, FCMP_SNE_D,
+   FCMP_COR_S, FCMP_COR_D, FCMP_SOR_S, FCMP_SOR_D,
+   FCMP_CUNE_S, FCMP_CUNE_D, FCMP_SUNE_S, FCMP_SUNE_D,
+   FCVT_S_D, FCVT_D_S,
+   FTINTRM_W_S, FTINTRM_W_D, FTINTRM_L_S, FTINTRM_L_D,
+   FTINTRP_W_S, FTINTRP_W_D, FTINTRP_L_S, FTINTRP_L_D,
+   FTINTRZ_W_S, FTINTRZ_W_D, FTINTRZ_L_S, FTINTRZ_L_D,
+   FTINTRNE_W_S, FTINTRNE_W_D, FTINTRNE_L_S, FTINTRNE_L_D,
+   FTINT_W_S, FTINT_W_D, FTINT_L_S, FTINT_L_D,
+   FFINT_S_W, FFINT_D_W, FFINT_S_L, FFINT_D_L,
+   FRINT_S, FRINT_D
+};
+
+extern ULong loongarch64_calculate_cpucfg      ( ULong src );
+extern ULong loongarch64_calculate_revb_2h     ( ULong src );
+extern ULong loongarch64_calculate_revb_4h     ( ULong src );
+extern ULong loongarch64_calculate_revb_2w     ( ULong src );
+extern ULong loongarch64_calculate_revb_d      ( ULong src );
+extern ULong loongarch64_calculate_revh_2w     ( ULong src );
+extern ULong loongarch64_calculate_revh_d      ( ULong src );
+extern ULong loongarch64_calculate_bitrev_4b   ( ULong src );
+extern ULong loongarch64_calculate_bitrev_8b   ( ULong src );
+extern ULong loongarch64_calculate_bitrev_w    ( ULong src );
+extern ULong loongarch64_calculate_bitrev_d    ( ULong src );
+extern ULong loongarch64_calculate_crc         ( ULong old, ULong msg, ULong len );
+extern ULong loongarch64_calculate_crcc        ( ULong old, ULong msg, ULong len );
+extern ULong loongarch64_calculate_fclass_s    ( ULong src );
+extern ULong loongarch64_calculate_fclass_d    ( ULong src );
+extern ULong loongarch64_calculate_FCSR        ( enum fpop op, ULong src1,
+                                                 ULong src2, ULong src3 );
+extern ULong loongarch64_calculate_negative_id ( ULong insSz, ULong sHi, ULong sLo );
+
+#endif /* ndef __VEX_GUEST_LOONGARCH64_DEFS_H */
+
+
+/*---------------------------------------------------------------*/
+/*--- end                            guest_loongarch64_defs.h ---*/
+/*---------------------------------------------------------------*/
diff --git a/VEX/priv/guest_loongarch64_helpers.c b/VEX/priv/guest_loongarch64_helpers.c
new file mode 100644
index 0000000..e761539
--- /dev/null
+++ b/VEX/priv/guest_loongarch64_helpers.c
@@ -0,0 +1,905 @@
+
+/*---------------------------------------------------------------*/
+/*--- begin                       guest_loongarch64_helpers.c ---*/
+/*---------------------------------------------------------------*/
+
+/*
+   This file is part of Valgrind, a dynamic binary instrumentation
+   framework.
+
+   Copyright (C) 2021-2022 Loongson Technology Corporation Limited
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, see <http://www.gnu.org/licenses/>.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "libvex_basictypes.h"
+#include "libvex_emnote.h"
+#include "libvex_guest_loongarch64.h"
+#include "libvex_ir.h"
+#include "libvex.h"
+
+#include "main_util.h"
+#include "main_globals.h"
+#include "guest_generic_bb_to_IR.h"
+#include "guest_loongarch64_defs.h"
+
+
+/* This file contains helper functions for loongarch64 guest code.
+   Calls to these functions are generated by the back end. */
+
+IRExpr* guest_loongarch64_spechelper ( const HChar * function_name,
+                                       IRExpr ** args,
+                                       IRStmt ** precedingStmts,
+                                       Int n_precedingStmts )
+{
+   return NULL;
+}
+
+/* VISIBLE TO LIBVEX CLIENT */
+void LibVEX_GuestLOONGARCH64_initialise ( /*OUT*/
+                                          VexGuestLOONGARCH64State* vex_state )
+{
+   UInt i;
+
+   /* Event check fail addr and counter. */
+   vex_state->host_EvC_FAILADDR = 0;
+   vex_state->host_EvC_COUNTER  = 0;
+
+   /* CPU Registers */
+   vex_state->guest_R0  = 0; /* Constant zero */
+   vex_state->guest_R1  = 0; /* Return address */
+   vex_state->guest_R2  = 0; /* Thread pointer */
+   vex_state->guest_R3  = 0; /* Stack pointer */
+   vex_state->guest_R4  = 0; /* Argument registers / Return value */
+   vex_state->guest_R5  = 0;
+   vex_state->guest_R6  = 0; /* Argument registers */
+   vex_state->guest_R7  = 0;
+   vex_state->guest_R8  = 0;
+   vex_state->guest_R9  = 0;
+   vex_state->guest_R10 = 0;
+   vex_state->guest_R11 = 0;
+   vex_state->guest_R12 = 0; /* Temporary registers */
+   vex_state->guest_R13 = 0;
+   vex_state->guest_R14 = 0;
+   vex_state->guest_R15 = 0;
+   vex_state->guest_R16 = 0;
+   vex_state->guest_R17 = 0;
+   vex_state->guest_R18 = 0;
+   vex_state->guest_R19 = 0;
+   vex_state->guest_R20 = 0;
+   vex_state->guest_R21 = 0; /* Reserved */
+   vex_state->guest_R22 = 0; /* Frame pointer / Static register */
+   vex_state->guest_R23 = 0; /* Static registers */
+   vex_state->guest_R24 = 0;
+   vex_state->guest_R25 = 0;
+   vex_state->guest_R26 = 0;
+   vex_state->guest_R27 = 0;
+   vex_state->guest_R28 = 0;
+   vex_state->guest_R29 = 0;
+   vex_state->guest_R30 = 0;
+   vex_state->guest_R31 = 0;
+
+   vex_state->guest_PC = 0; /* Program counter */
+
+   /* FPU/SIMD Registers */
+   for (i = 0; i < 8; i++) {
+      vex_state->guest_X0[i]  = 0xffffffff;
+      vex_state->guest_X1[i]  = 0xffffffff;
+      vex_state->guest_X2[i]  = 0xffffffff;
+      vex_state->guest_X3[i]  = 0xffffffff;
+      vex_state->guest_X4[i]  = 0xffffffff;
+      vex_state->guest_X5[i]  = 0xffffffff;
+      vex_state->guest_X6[i]  = 0xffffffff;
+      vex_state->guest_X7[i]  = 0xffffffff;
+      vex_state->guest_X8[i]  = 0xffffffff;
+      vex_state->guest_X9[i]  = 0xffffffff;
+      vex_state->guest_X10[i] = 0xffffffff;
+      vex_state->guest_X11[i] = 0xffffffff;
+      vex_state->guest_X12[i] = 0xffffffff;
+      vex_state->guest_X13[i] = 0xffffffff;
+      vex_state->guest_X14[i] = 0xffffffff;
+      vex_state->guest_X15[i] = 0xffffffff;
+      vex_state->guest_X16[i] = 0xffffffff;
+      vex_state->guest_X17[i] = 0xffffffff;
+      vex_state->guest_X18[i] = 0xffffffff;
+      vex_state->guest_X19[i] = 0xffffffff;
+      vex_state->guest_X20[i] = 0xffffffff;
+      vex_state->guest_X21[i] = 0xffffffff;
+      vex_state->guest_X22[i] = 0xffffffff;
+      vex_state->guest_X23[i] = 0xffffffff;
+      vex_state->guest_X24[i] = 0xffffffff;
+      vex_state->guest_X25[i] = 0xffffffff;
+      vex_state->guest_X26[i] = 0xffffffff;
+      vex_state->guest_X27[i] = 0xffffffff;
+      vex_state->guest_X28[i] = 0xffffffff;
+      vex_state->guest_X29[i] = 0xffffffff;
+      vex_state->guest_X30[i] = 0xffffffff;
+      vex_state->guest_X31[i] = 0xffffffff;
+   }
+
+   vex_state->guest_FCC0 = 0; /* Condition Flag Registers */
+   vex_state->guest_FCC1 = 0;
+   vex_state->guest_FCC2 = 0;
+   vex_state->guest_FCC3 = 0;
+   vex_state->guest_FCC4 = 0;
+   vex_state->guest_FCC5 = 0;
+   vex_state->guest_FCC6 = 0;
+   vex_state->guest_FCC7 = 0;
+   vex_state->guest_FCSR = 0; /* FP Control and Status Register */
+
+   /* Various pseudo-regs mandated by Vex or Valgrind. */
+   /* Emulation notes */
+   vex_state->guest_EMNOTE = 0;
+
+   /* For clflush: record start and length of area to invalidate */
+   vex_state->guest_CMSTART = 0;
+   vex_state->guest_CMLEN   = 0;
+
+   /* Used to record the unredirected guest address at the start of
+      a translation whose start has been redirected.  By reading
+      this pseudo-register shortly afterwards, the translation can
+      find out what the corresponding no-redirection address was.
+      Note, this is only set for wrap-style redirects, not for
+      replace-style ones. */
+   vex_state->guest_NRADDR = 0;
+}
+
+
+/*-----------------------------------------------------------*/
+/*--- Describing the loongarch64 guest state, for the     ---*/
+/*--- benefit of iropt and instrumenters                  ---*/
+/*-----------------------------------------------------------*/
+
+/* Figure out if any part of the guest state contained in minoff
+   .. maxoff requires precise memory exceptions.  If in doubt return
+   True (but this generates significantly slower code).
+
+   We enforce precise exns for guest SP, PC and FP.
+
+   Only SP is needed in mode VexRegUpdSpAtMemAccess.
+*/
+
+Bool guest_loongarch64_state_requires_precise_mem_exns ( Int minoff,
+                                                         Int maxoff,
+                                                         VexRegisterUpdates pxControl )
+{
+   Int sp_min = offsetof(VexGuestLOONGARCH64State, guest_R3);
+   Int sp_max = sp_min + 8 - 1;
+   if ( maxoff < sp_min || minoff > sp_max ) {
+      /* no overlap with sp */
+      if (pxControl == VexRegUpdSpAtMemAccess)
+         return False;  /* We only need to check stack pointer. */
+   } else {
+      return True;
+   }
+
+   Int pc_min = offsetof(VexGuestLOONGARCH64State, guest_PC);
+   Int pc_max = pc_min + 8 - 1;
+   if ( maxoff < pc_min || minoff > pc_max ) {
+      /* no overlap with pc */
+   } else {
+      return True;
+   }
+
+   Int fp_min = offsetof(VexGuestLOONGARCH64State, guest_R22);
+   Int fp_max = fp_min + 8 - 1;
+   if ( maxoff < fp_min || minoff > fp_max ) {
+      /* no overlap with fp */
+   } else {
+      return True;
+   }
+
+   return False;
+}
+
+#define ALWAYSDEFD64(field)                            \
+   { offsetof(VexGuestLOONGARCH64State, field),        \
+      (sizeof ((VexGuestLOONGARCH64State*)0)->field) }
+
+VexGuestLayout loongarch64Guest_layout = {
+   /* Total size of the guest state, in bytes. */
+   .total_sizeB = sizeof(VexGuestLOONGARCH64State),
+   /* Describe the stack pointer. */
+   .offset_SP = offsetof(VexGuestLOONGARCH64State, guest_R3),
+   .sizeof_SP = 8,
+   /* Describe the frame pointer. */
+   .offset_FP = offsetof(VexGuestLOONGARCH64State, guest_R22),
+   .sizeof_FP = 8,
+   /* Describe the instruction pointer. */
+   .offset_IP = offsetof(VexGuestLOONGARCH64State, guest_PC),
+   .sizeof_IP = 8,
+   /* Describe any sections to be regarded by Memcheck as
+      'always-defined'. */
+   .n_alwaysDefd = 6,
+   /* ? :(  */
+   .alwaysDefd = {
+                  /* 0 */ ALWAYSDEFD64(guest_R0),
+                  /* 1 */ ALWAYSDEFD64(guest_PC),
+                  /* 2 */ ALWAYSDEFD64(guest_EMNOTE),
+                  /* 3 */ ALWAYSDEFD64(guest_CMSTART),
+                  /* 4 */ ALWAYSDEFD64(guest_CMLEN),
+                  /* 5 */ ALWAYSDEFD64(guest_NRADDR),
+                  }
+};
+
+
+/*-----------------------------------------------------------*/
+/*--- loongarch64 guest helpers                           ---*/
+/*-----------------------------------------------------------*/
+
+/* Claim to be the following CPU, which is probably representative of
+   the earliest loongarch64 offerings.
+
+   CPU Family          : Loongson-64bit
+   Model Name          : Loongson-3A5000LL
+   CPU Revision        : 0x10
+   FPU Revision        : 0x00
+   CPU MHz             : 2300.00
+   BogoMIPS            : 4600.00
+   TLB Entries         : 2112
+   Address Sizes       : 48 bits physical, 48 bits virtual
+   ISA                 : loongarch32 loongarch64
+   Features            : cpucfg lam ual fpu lsx lasx complex crypto lvz
+   Hardware Watchpoint : yes, iwatch count: 8, dwatch count: 8
+*/
+ULong loongarch64_calculate_cpucfg ( ULong src )
+{
+   ULong res;
+   switch (src) {
+      case 0x0:
+         res = 0x0014c010;
+         break;
+      case 0x1:
+         res = 0x03f2f2fe;
+         break;
+      case 0x2:
+         res = 0x007ccfc7;
+         break;
+      case 0x3:
+         res = 0x0000fcff;
+         break;
+      case 0x4:
+         res = 0x05f5e100;
+         break;
+      case 0x5:
+         res = 0x00010001;
+         break;
+      case 0x6:
+         res = 0x00007f33;
+         break;
+      case 0x10:
+         res = 0x00002c3d;
+         break;
+      case 0x11:
+         res = 0x06080003;
+         break;
+      case 0x12:
+         res = 0x06080003;
+         break;
+      case 0x13:
+         res = 0x0608000f;
+         break;
+      case 0x14:
+         res = 0x060e000f;
+         break;
+      default:
+         res = 0x00000000;
+         break;
+   }
+   return (ULong)(Long)(Int)res;
+}
+
+static void swap_UChar ( UChar* a, UChar* b )
+{
+   UChar t = *a;
+   *a = *b;
+   *b = t;
+}
+
+ULong loongarch64_calculate_revb_2h ( ULong src )
+{
+   UChar* s = (UChar*)&src;
+   swap_UChar(&s[0], &s[1]);
+   swap_UChar(&s[2], &s[3]);
+   return (ULong)(Long)(Int)src;
+}
+
+ULong loongarch64_calculate_revb_4h ( ULong src )
+{
+   UChar* s = (UChar*)&src;
+   swap_UChar(&s[0], &s[1]);
+   swap_UChar(&s[2], &s[3]);
+   swap_UChar(&s[4], &s[5]);
+   swap_UChar(&s[6], &s[7]);
+   return src;
+}
+
+ULong loongarch64_calculate_revb_2w ( ULong src )
+{
+   UChar* s = (UChar*)&src;
+   swap_UChar(&s[0], &s[3]);
+   swap_UChar(&s[1], &s[2]);
+   swap_UChar(&s[4], &s[7]);
+   swap_UChar(&s[5], &s[6]);
+   return src;
+}
+
+ULong loongarch64_calculate_revb_d ( ULong src )
+{
+   UChar* s = (UChar*)&src;
+   swap_UChar(&s[0], &s[7]);
+   swap_UChar(&s[1], &s[6]);
+   swap_UChar(&s[2], &s[5]);
+   swap_UChar(&s[3], &s[4]);
+   return src;
+}
+
+static void swap_UShort ( UShort* a, UShort* b )
+{
+   UShort t = *a;
+   *a = *b;
+   *b = t;
+}
+
+ULong loongarch64_calculate_revh_2w ( ULong src )
+{
+   UShort* s = (UShort*)&src;
+   swap_UShort(&s[0], &s[1]);
+   swap_UShort(&s[2], &s[3]);
+   return src;
+}
+
+ULong loongarch64_calculate_revh_d ( ULong src )
+{
+   UShort* s = (UShort*)&src;
+   swap_UShort(&s[0], &s[3]);
+   swap_UShort(&s[1], &s[2]);
+   return src;
+}
+
+static ULong bitrev ( ULong src, ULong start, ULong end )
+{
+   int i, j;
+   ULong res = 0;
+   for (i = start, j = 1; i < end; i++, j++)
+      res |= ((src >> i) & 1) << (end - j);
+   return res;
+}
+
+ULong loongarch64_calculate_bitrev_4b ( ULong src )
+{
+   ULong res = bitrev(src, 0, 8);
+   res |= bitrev(src, 8, 16);
+   res |= bitrev(src, 16, 24);
+   res |= bitrev(src, 24, 32);
+   return (ULong)(Long)(Int)res;
+}
+
+ULong loongarch64_calculate_bitrev_8b ( ULong src )
+{
+   ULong res = bitrev(src, 0, 8);
+   res |= bitrev(src, 8, 16);
+   res |= bitrev(src, 16, 24);
+   res |= bitrev(src, 24, 32);
+   res |= bitrev(src, 32, 40);
+   res |= bitrev(src, 40, 48);
+   res |= bitrev(src, 48, 56);
+   res |= bitrev(src, 56, 64);
+   return res;
+}
+
+ULong loongarch64_calculate_bitrev_w ( ULong src )
+{
+   ULong res = bitrev(src, 0, 32);
+   return (ULong)(Long)(Int)res;
+}
+
+ULong loongarch64_calculate_bitrev_d ( ULong src )
+{
+   return bitrev(src, 0, 64);
+}
+
+static ULong crc32 ( ULong old, ULong msg, ULong width, ULong poly )
+{
+   int i;
+   ULong new;
+   if (width == 8)
+      msg &= 0xff;
+   else if (width == 16)
+      msg &= 0xffff;
+   else if (width == 32)
+      msg &= 0xffffffff;
+   new = (old & 0xffffffff) ^ msg;
+   for (i = 0; i < width; i++) {
+      if (new & 1)
+         new = (new >> 1) ^ poly;
+      else
+         new >>= 1;
+   }
+   return new;
+}
+
+ULong loongarch64_calculate_crc ( ULong old, ULong msg, ULong len )
+{
+   ULong res = crc32(old, msg, len, 0xedb88320);
+   return (ULong)(Long)(Int)res;
+}
+
+ULong loongarch64_calculate_crcc ( ULong old, ULong msg, ULong len )
+{
+   ULong res = crc32(old, msg, len, 0x82f63b78);
+   return (ULong)(Long)(Int)res;
+}
+
+ULong loongarch64_calculate_fclass_s ( ULong src )
+{
+   UInt f = src;
+   Bool sign = toBool(f >> 31);
+   if ((f & 0x7fffffff) == 0x7f800000) {
+      return sign ? 1 << 2 : 1 << 6;
+   } else if ((f & 0x7fffffff) == 0) {
+      return sign ? 1 << 5 : 1 << 9;
+   } else if ((f & 0x7f800000) == 0) {
+      return sign ? 1 << 4 : 1 << 8;
+   } else if ((f & ~(1 << 31)) > 0x7f800000) {
+      return ((UInt)(f << 1) >= 0xff800000) ? 1 << 1 : 1 << 0;
+   } else {
+      return sign ? 1 << 3 : 1 << 7;
+   }
+}
+
+ULong loongarch64_calculate_fclass_d ( ULong src )
+{
+   ULong f = src;
+   Bool sign = toBool(f >> 63);
+   if ((f & 0x7fffffffffffffffULL) == 0x7ff0000000000000ULL) {
+      return sign ? 1 << 2 : 1 << 6;
+   } else if ((f & 0x7fffffffffffffffULL) == 0) {
+      return sign ? 1 << 5 : 1 << 9;
+   } else if ((f & 0x7ff0000000000000ULL) == 0) {
+      return sign ? 1 << 4 : 1 << 8;
+   } else if ((f & ~(1ULL << 63)) > 0x7ff0000000000000ULL) {
+      return ((f << 1) >= 0xfff0000000000000ULL) ? 1 << 1 : 1 << 0;
+   } else {
+      return sign ? 1 << 3 : 1 << 7;
+   }
+}
+
+ULong loongarch64_calculate_negative_id ( ULong insSz, ULong sHi, ULong sLo )
+{
+   V128 src;
+   UInt i;
+
+   src.w64[1] = sHi;
+   src.w64[0] = sLo;
+
+   switch (insSz) {
+      case 0b00: {
+         for (i = 0; i < 16; i++) {
+            if ((Char)src.w8[i] < 0)
+               break;
+         }
+         break;
+      }
+      case 0b01: {
+         for (i = 0; i < 8; i++) {
+            if ((Short)src.w16[i] < 0)
+               break;
+         }
+         break;
+      }
+      default:
+         vassert(0);
+         break;
+   }
+
+   return (ULong)i;
+}
+
+#if defined(__loongarch__)
+#define ASM_VOLATILE_UNARY(inst)                         \
+   __asm__ volatile("movfcsr2gr $s0, $r0         \n\t"   \
+                    "movgr2fcsr $r2, $zero       \n\t"   \
+                    #inst"      $f24, %1         \n\t"   \
+                    "movfcsr2gr %0, $r2          \n\t"   \
+                    "movgr2fcsr $r0, $s0         \n\t"   \
+                    : "=r" (fcsr2)                       \
+                    : "f" (src1)                         \
+                    : "$s0", "$f24"                      \
+                   )
+
+#define ASM_VOLATILE_BINARY(inst)                        \
+   __asm__ volatile("movfcsr2gr $s0, $r0         \n\t"   \
+                    "movgr2fcsr $r2, $zero       \n\t"   \
+                    #inst"      $f24, %1, %2     \n\t"   \
+                    "movfcsr2gr %0, $r2          \n\t"   \
+                    "movgr2fcsr $r0, $s0         \n\t"   \
+                    : "=r" (fcsr2)                       \
+                    : "f" (src1), "f" (src2)             \
+                    : "$s0", "$f24"                      \
+                   )
+
+#define ASM_VOLATILE_TRINARY(inst)                       \
+   __asm__ volatile("movfcsr2gr $s0, $r0         \n\t"   \
+                    "movgr2fcsr $r2, $zero       \n\t"   \
+                    #inst"      $f24, %1, %2, %3 \n\t"   \
+                    "movfcsr2gr %0, $r2          \n\t"   \
+                    "movgr2fcsr $r0, $s0         \n\t"   \
+                    : "=r" (fcsr2)                       \
+                    : "f" (src1), "f" (src2), "f" (src3) \
+                    : "$s0", "$f24"                      \
+                   )
+
+#define ASM_VOLATILE_FCMP(inst)                          \
+   __asm__ volatile("movfcsr2gr $s0, $r0         \n\t"   \
+                    "movgr2fcsr $r2, $zero       \n\t"   \
+                    #inst"      $fcc0, %1, %2    \n\t"   \
+                    "movfcsr2gr %0, $r0          \n\t"   \
+                    "movgr2fcsr $r0, $s0         \n\t"   \
+                    : "=r" (fcsr2)                       \
+                    : "f" (src1), "f" (src2)             \
+                    : "$s0", "$fcc0"                     \
+                   )
+#endif
+
+/* Calculate FCSR and return whether an exception needs to be thrown */
+ULong loongarch64_calculate_FCSR ( enum fpop op, ULong src1,
+                                   ULong src2, ULong src3 )
+{
+   UInt fcsr2 = 0;
+#if defined(__loongarch__)
+   switch (op) {
+      case FADD_S:
+         ASM_VOLATILE_BINARY(fadd.s);
+         break;
+      case FADD_D:
+         ASM_VOLATILE_BINARY(fadd.d);
+         break;
+      case FSUB_S:
+         ASM_VOLATILE_BINARY(fsub.s);
+         break;
+      case FSUB_D:
+         ASM_VOLATILE_BINARY(fsub.d);
+         break;
+      case FMUL_S:
+         ASM_VOLATILE_BINARY(fmul.s);
+         break;
+      case FMUL_D:
+         ASM_VOLATILE_BINARY(fmul.d);
+         break;
+      case FDIV_S:
+         ASM_VOLATILE_BINARY(fdiv.s);
+         break;
+      case FDIV_D:
+         ASM_VOLATILE_BINARY(fdiv.d);
+         break;
+      case FMADD_S:
+         ASM_VOLATILE_TRINARY(fmadd.s);
+         break;
+      case FMADD_D:
+         ASM_VOLATILE_TRINARY(fmadd.d);
+         break;
+      case FMSUB_S:
+         ASM_VOLATILE_TRINARY(fmsub.s);
+         break;
+      case FMSUB_D:
+         ASM_VOLATILE_TRINARY(fmsub.d);
+         break;
+      case FNMADD_S:
+         ASM_VOLATILE_TRINARY(fnmadd.s);
+         break;
+      case FNMADD_D:
+         ASM_VOLATILE_TRINARY(fnmadd.d);
+         break;
+      case FNMSUB_S:
+         ASM_VOLATILE_TRINARY(fnmsub.s);
+         break;
+      case FNMSUB_D:
+         ASM_VOLATILE_TRINARY(fnmsub.s);
+         break;
+      case FMAX_S:
+         ASM_VOLATILE_BINARY(fmax.s);
+         break;
+      case FMAX_D:
+         ASM_VOLATILE_BINARY(fmax.d);
+         break;
+      case FMIN_S:
+         ASM_VOLATILE_BINARY(fmin.s);
+         break;
+      case FMIN_D:
+         ASM_VOLATILE_BINARY(fmin.d);
+         break;
+      case FMAXA_S:
+         ASM_VOLATILE_BINARY(fmaxa.s);
+         break;
+      case FMAXA_D:
+         ASM_VOLATILE_BINARY(fmaxa.d);
+         break;
+      case FMINA_S:
+         ASM_VOLATILE_BINARY(fmina.s);
+         break;
+      case FMINA_D:
+         ASM_VOLATILE_BINARY(fmina.s);
+         break;
+      case FABS_S:
+         ASM_VOLATILE_UNARY(fabs.s);
+         break;
+      case FABS_D:
+         ASM_VOLATILE_UNARY(fabs.d);
+         break;
+      case FNEG_S:
+         ASM_VOLATILE_UNARY(fneg.s);
+         break;
+      case FNEG_D:
+         ASM_VOLATILE_UNARY(fneg.d);
+         break;
+      case FSQRT_S:
+         ASM_VOLATILE_UNARY(fsqrt.s);
+         break;
+      case FSQRT_D:
+         ASM_VOLATILE_UNARY(fsqrt.d);
+         break;
+      case FRECIP_S:
+         ASM_VOLATILE_UNARY(frecip.s);
+         break;
+      case FRECIP_D:
+         ASM_VOLATILE_UNARY(frecip.d);
+         break;
+      case FRSQRT_S:
+         ASM_VOLATILE_UNARY(frsqrt.s);
+         break;
+      case FRSQRT_D:
+         ASM_VOLATILE_UNARY(frsqrt.d);
+         break;
+      case FSCALEB_S:
+         ASM_VOLATILE_BINARY(fscaleb.s);
+         break;
+      case FSCALEB_D:
+         ASM_VOLATILE_BINARY(fscaleb.d);
+         break;
+      case FLOGB_S:
+         ASM_VOLATILE_UNARY(flogb.s);
+         break;
+      case FLOGB_D:
+         ASM_VOLATILE_UNARY(flogb.d);
+         break;
+      case FCMP_CAF_S:
+         ASM_VOLATILE_FCMP(fcmp.caf.s);
+         break;
+      case FCMP_CAF_D:
+         ASM_VOLATILE_FCMP(fcmp.caf.d);
+         break;
+      case FCMP_SAF_S:
+         ASM_VOLATILE_FCMP(fcmp.saf.s);
+         break;
+      case FCMP_SAF_D:
+         ASM_VOLATILE_FCMP(fcmp.saf.d);
+         break;
+      case FCMP_CLT_S:
+         ASM_VOLATILE_FCMP(fcmp.clt.s);
+         break;
+      case FCMP_CLT_D:
+         ASM_VOLATILE_FCMP(fcmp.clt.d);
+         break;
+      case FCMP_SLT_S:
+         ASM_VOLATILE_FCMP(fcmp.slt.s);
+         break;
+      case FCMP_SLT_D:
+         ASM_VOLATILE_FCMP(fcmp.slt.d);
+         break;
+      case FCMP_CEQ_S:
+         ASM_VOLATILE_FCMP(fcmp.ceq.s);
+         break;
+      case FCMP_CEQ_D:
+         ASM_VOLATILE_FCMP(fcmp.ceq.d);
+         break;
+      case FCMP_SEQ_S:
+         ASM_VOLATILE_FCMP(fcmp.seq.s);
+         break;
+      case FCMP_SEQ_D:
+         ASM_VOLATILE_FCMP(fcmp.seq.d);
+         break;
+      case FCMP_CLE_S:
+         ASM_VOLATILE_FCMP(fcmp.cle.s);
+         break;
+      case FCMP_CLE_D:
+         ASM_VOLATILE_FCMP(fcmp.cle.d);
+         break;
+      case FCMP_SLE_S:
+         ASM_VOLATILE_FCMP(fcmp.sle.s);
+         break;
+      case FCMP_SLE_D:
+         ASM_VOLATILE_FCMP(fcmp.sle.d);
+         break;
+      case FCMP_CUN_S:
+         ASM_VOLATILE_FCMP(fcmp.cun.s);
+         break;
+      case FCMP_CUN_D:
+         ASM_VOLATILE_FCMP(fcmp.cun.d);
+         break;
+      case FCMP_SUN_S:
+         ASM_VOLATILE_FCMP(fcmp.sun.s);
+         break;
+      case FCMP_SUN_D:
+         ASM_VOLATILE_FCMP(fcmp.sun.d);
+         break;
+      case FCMP_CULT_S:
+         ASM_VOLATILE_FCMP(fcmp.cult.s);
+         break;
+      case FCMP_CULT_D:
+         ASM_VOLATILE_FCMP(fcmp.cult.d);
+         break;
+      case FCMP_SULT_S:
+         ASM_VOLATILE_FCMP(fcmp.sult.s);
+         break;
+      case FCMP_SULT_D:
+         ASM_VOLATILE_FCMP(fcmp.sult.d);
+         break;
+      case FCMP_CUEQ_S:
+         ASM_VOLATILE_FCMP(fcmp.cueq.s);
+         break;
+      case FCMP_CUEQ_D:
+         ASM_VOLATILE_FCMP(fcmp.cueq.d);
+         break;
+      case FCMP_SUEQ_S:
+         ASM_VOLATILE_FCMP(fcmp.sueq.s);
+         break;
+      case FCMP_SUEQ_D:
+         ASM_VOLATILE_FCMP(fcmp.sueq.d);
+         break;
+      case FCMP_CULE_S:
+         ASM_VOLATILE_FCMP(fcmp.cule.s);
+         break;
+      case FCMP_CULE_D:
+         ASM_VOLATILE_FCMP(fcmp.cule.d);
+         break;
+      case FCMP_SULE_S:
+         ASM_VOLATILE_FCMP(fcmp.sule.s);
+         break;
+      case FCMP_SULE_D:
+         ASM_VOLATILE_FCMP(fcmp.sule.d);
+         break;
+      case FCMP_CNE_S:
+         ASM_VOLATILE_FCMP(fcmp.cne.s);
+         break;
+      case FCMP_CNE_D:
+         ASM_VOLATILE_FCMP(fcmp.cne.d);
+         break;
+      case FCMP_SNE_S:
+         ASM_VOLATILE_FCMP(fcmp.sne.s);
+         break;
+      case FCMP_SNE_D:
+         ASM_VOLATILE_FCMP(fcmp.sne.d);
+         break;
+      case FCMP_COR_S:
+         ASM_VOLATILE_FCMP(fcmp.cor.s);
+         break;
+      case FCMP_COR_D:
+         ASM_VOLATILE_FCMP(fcmp.cor.d);
+         break;
+      case FCMP_SOR_S:
+         ASM_VOLATILE_FCMP(fcmp.sor.s);
+         break;
+      case FCMP_SOR_D:
+         ASM_VOLATILE_FCMP(fcmp.sor.d);
+         break;
+      case FCMP_CUNE_S:
+         ASM_VOLATILE_FCMP(fcmp.cune.s);
+         break;
+      case FCMP_CUNE_D:
+         ASM_VOLATILE_FCMP(fcmp.cune.d);
+         break;
+      case FCMP_SUNE_S:
+         ASM_VOLATILE_FCMP(fcmp.sune.s);
+         break;
+      case FCMP_SUNE_D:
+         ASM_VOLATILE_FCMP(fcmp.sune.d);
+         break;
+      case FCVT_S_D:
+         ASM_VOLATILE_UNARY(fcvt.s.d);
+         break;
+      case FCVT_D_S:
+         ASM_VOLATILE_UNARY(fcvt.d.s);
+         break;
+      case FTINTRM_W_S:
+         ASM_VOLATILE_UNARY(ftintrm.w.s);
+         break;
+      case FTINTRM_W_D:
+         ASM_VOLATILE_UNARY(ftintrm.w.d);
+         break;
+      case FTINTRM_L_S:
+         ASM_VOLATILE_UNARY(ftintrm.l.s);
+         break;
+      case FTINTRM_L_D:
+         ASM_VOLATILE_UNARY(ftintrm.l.d);
+         break;
+      case FTINTRP_W_S:
+         ASM_VOLATILE_UNARY(ftintrp.w.s);
+         break;
+      case FTINTRP_W_D:
+         ASM_VOLATILE_UNARY(ftintrp.w.d);
+         break;
+      case FTINTRP_L_S:
+         ASM_VOLATILE_UNARY(ftintrp.l.s);
+         break;
+      case FTINTRP_L_D:
+         ASM_VOLATILE_UNARY(ftintrp.l.d);
+         break;
+      case FTINTRZ_W_S:
+         ASM_VOLATILE_UNARY(ftintrz.w.s);
+         break;
+      case FTINTRZ_W_D:
+         ASM_VOLATILE_UNARY(ftintrz.w.d);
+         break;
+      case FTINTRZ_L_S:
+         ASM_VOLATILE_UNARY(ftintrz.l.s);
+         break;
+      case FTINTRZ_L_D:
+         ASM_VOLATILE_UNARY(ftintrz.l.d);
+         break;
+      case FTINTRNE_W_S:
+         ASM_VOLATILE_UNARY(ftintrne.w.s);
+         break;
+      case FTINTRNE_W_D:
+         ASM_VOLATILE_UNARY(ftintrne.w.d);
+         break;
+      case FTINTRNE_L_S:
+         ASM_VOLATILE_UNARY(ftintrne.l.s);
+         break;
+      case FTINTRNE_L_D:
+         ASM_VOLATILE_UNARY(ftintrne.l.d);
+         break;
+      case FTINT_W_S:
+         ASM_VOLATILE_UNARY(ftint.w.s);
+         break;
+      case FTINT_W_D:
+         ASM_VOLATILE_UNARY(ftint.w.d);
+         break;
+      case FTINT_L_S:
+         ASM_VOLATILE_UNARY(ftint.l.s);
+         break;
+      case FTINT_L_D:
+         ASM_VOLATILE_UNARY(ftint.l.d);
+         break;
+      case FFINT_S_W:
+         ASM_VOLATILE_UNARY(ffint.s.w);
+         break;
+      case FFINT_D_W:
+         ASM_VOLATILE_UNARY(ffint.d.w);
+         break;
+      case FFINT_S_L:
+         ASM_VOLATILE_UNARY(ffint.s.l);
+         break;
+      case FFINT_D_L:
+         ASM_VOLATILE_UNARY(ffint.d.l);
+         break;
+      case FRINT_S:
+         ASM_VOLATILE_UNARY(frint.s);
+         break;
+      case FRINT_D:
+         ASM_VOLATILE_UNARY(frint.d);
+         break;
+      default:
+         break;
+   }
+#endif
+   return (ULong)fcsr2;
+}
+
+
+/*---------------------------------------------------------------*/
+/*--- end                         guest_loongarch64_helpers.c ---*/
+/*---------------------------------------------------------------*/
diff --git a/VEX/priv/guest_loongarch64_toIR.c b/VEX/priv/guest_loongarch64_toIR.c
new file mode 100644
index 0000000..50c24f1
--- /dev/null
+++ b/VEX/priv/guest_loongarch64_toIR.c
@@ -0,0 +1,12086 @@
+
+/*--------------------------------------------------------------------*/
+/*--- begin                               guest_loongarch64_toIR.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Valgrind, a dynamic binary instrumentation
+   framework.
+
+   Copyright (C) 2021-2022 Loongson Technology Corporation Limited
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, see <http://www.gnu.org/licenses/>.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+/* "Special" instructions.
+
+   This instruction decoder can decode four special instructions
+   which mean nothing natively (are no-ops as far as regs/mem are
+   concerned) but have meaning for supporting Valgrind.  A special
+   instruction is flagged by a 16-byte preamble:
+
+      00450c00  (srli.d $zero, $zero, 3
+      00453400   srli.d $zero, $zero, 13
+      00457400   srli.d $zero, $zero, 29
+      00454c00   srli.d $zero, $zero, 19)
+
+   Following that, one of the following 3 are allowed
+   (standard interpretation in parentheses):
+
+      001535ad  (or $t1, $t1, $t1)  $a7 = client_request ( $t0 )
+      001539ce  (or $t2, $t2, $t2)  $a7 = guest_NRADDR
+      00153def  (or $t3, $t3, $t3)  call-noredir $t8
+      00154210  (or $t4, $t4, $t4)  IR injection
+
+   Any other bytes following the 16-byte preamble are illegal and
+   constitute a failure in instruction decoding.  This all assumes
+   that the preamble will never occur except in specific code
+   fragments designed for Valgrind to catch.
+*/
+
+/* Translates LOONGARCH64 code to IR. */
+
+#include "libvex_basictypes.h"
+#include "libvex_ir.h"
+#include "libvex.h"
+#include "libvex_guest_loongarch64.h"
+
+#include "main_util.h"
+#include "main_globals.h"
+#include "guest_generic_bb_to_IR.h"
+#include "guest_loongarch64_defs.h"
+
+
+/*------------------------------------------------------------*/
+/*--- Globals                                              ---*/
+/*------------------------------------------------------------*/
+
+/* These are set at the start of the translation of a instruction, so
+   that we don't have to pass them around endlessly.  CONST means does
+   not change during translation of the instruction. */
+
+/* CONST: what is the host's endianness?  We need to know this in
+   order to do sub-register accesses to the SIMD/FP registers
+   correctly. */
+static VexEndness host_endness;
+
+/* CONST: The guest address for the instruction currently being
+   translated.  */
+static Addr64 guest_PC_curr_instr;
+
+/* MOD: The IRSB* into which we're generating code. */
+static IRSB* irsb;
+
+
+/*------------------------------------------------------------*/
+/*--- Debugging output                                     ---*/
+/*------------------------------------------------------------*/
+
+#define DIP(format, args...)           \
+   if (vex_traceflags & VEX_TRACE_FE)  \
+      vex_printf(format, ## args)
+
+static const HChar* nameIReg( UInt reg )
+{
+   vassert(reg < 32);
+   static const HChar* reg_names[32] = {
+      "$zero",
+      "$ra",
+      "$tp",
+      "$sp",
+      "$a0", "$a1", "$a2", "$a3", "$a4", "$a5", "$a6", "$a7",
+      "$t0", "$t1", "$t2", "$t3", "$t4", "$t5", "$t6", "$t7", "$t8",
+      "$r21", /* Reserved */
+      "$fp",
+      "$s0", "$s1", "$s2", "$s3", "$s4", "$s5", "$s6", "$s7", "$s8"
+   };
+   return reg_names[reg];
+}
+
+static const HChar* nameFReg( UInt reg )
+{
+   vassert(reg < 32);
+   static const HChar* reg_names[32] = {
+      "$fa0",  "$fa1",  "$fa2",  "$fa3",  "$fa4",  "$fa5",  "$fa6",  "$fa7",
+      "$ft0",  "$ft1",  "$ft2",  "$ft3",  "$ft4",  "$ft5",  "$ft6",  "$ft7",
+      "$ft8",  "$ft9",  "$ft10", "$ft11", "$ft12", "$ft13", "$ft14", "$ft15",
+      "$fs0",  "$fs1",  "$fs2",  "$fs3",  "$fs4",  "$fs5",  "$fs6",  "$fs7"
+   };
+   return reg_names[reg];
+}
+
+static const HChar* nameVReg( UInt reg )
+{
+   vassert(reg < 32);
+   static const HChar* reg_names[32] = {
+      "$vr0",  "$vr1",  "$vr2",  "$vr3",  "$vr4",  "$vr5",  "$vr6",  "$vr7",
+      "$vr8",  "$vr9",  "$vr10", "$vr11", "$vr12", "$vr13", "$vr14", "$vr15",
+      "$vr16", "$vr17", "$vr18", "$vr19", "$vr20", "$vr21", "$vr22", "$vr23",
+      "$vr24", "$vr25", "$vr26", "$vr27", "$vr28", "$vr29", "$vr30", "$vr31"
+   };
+   return reg_names[reg];
+}
+
+static const HChar* nameXReg( UInt reg )
+{
+   vassert(reg < 32);
+   static const HChar* reg_names[32] = {
+      "$xr0",  "$xr1",  "$xr2",  "$xr3",  "$xr4",  "$xr5",  "$xr6",  "$xr7",
+      "$xr8",  "$xr9",  "$xr10", "$xr11", "$xr12", "$xr13", "$xr14", "$xr15",
+      "$xr16", "$xr17", "$xr18", "$xr19", "$xr20", "$xr21", "$xr22", "$xr23",
+      "$xr24", "$xr25", "$xr26", "$xr27", "$xr28", "$xr29", "$xr30", "$xr31"
+   };
+   return reg_names[reg];
+}
+
+static const HChar* nameFCC( UInt reg )
+{
+   vassert(reg < 8);
+   static const HChar* reg_names[8] = {
+      "$fcc0", "$fcc1", "$fcc2", "$fcc3", "$fcc4", "$fcc5", "$fcc6", "$fcc7"
+   };
+   return reg_names[reg];
+}
+
+static const HChar* nameFCSR( UInt reg )
+{
+   vassert(reg < 4);
+   static const HChar* reg_names[4] = {
+      "$fcsr0", "$fcsr1", "$fcsr2", "$fcsr3"
+   };
+   return reg_names[reg];
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Helper bits and pieces for deconstructing the        ---*/
+/*--- loongarch64 insn stream.                             ---*/
+/*------------------------------------------------------------*/
+
+/* Get insn[max:min] */
+#define SLICE(insn, max, min) \
+   ((((UInt)(insn)) >> (min)) & (UInt)((1ULL << ((max) - (min) + 1)) - 1ULL))
+
+/* Do a little-endian load of a 32-bit word, regardless of the
+   endianness of the underlying host. */
+static inline UInt getUInt ( const UChar* p )
+{
+   UInt w = 0;
+   w = (w << 8) | p[3];
+   w = (w << 8) | p[2];
+   w = (w << 8) | p[1];
+   w = (w << 8) | p[0];
+   return w;
+}
+
+/* Sign extend to 32-bit */
+static inline UInt extend32 ( UInt imm, UInt size )
+{
+   UInt shift = 32 - size;
+   return (UInt)(((Int)imm << shift) >> shift);
+}
+
+/* Sign extend to 64-bit */
+static inline ULong extend64 ( ULong imm, UInt size )
+{
+   UInt shift = 64 - size;
+   return (ULong)(((Long)imm << shift) >> shift);
+}
+
+/* Get the suffix of the insn */
+static const HChar *mkInsSize ( UInt size ) {
+   const HChar *insSize[8]
+      = { "b",  "h",  "w",  "d", "bu", "hu", "wu", "du" };
+   vassert(size < 8);
+   return insSize[size];
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Helper bits and pieces for creating IR fragments.    ---*/
+/*------------------------------------------------------------*/
+
+static inline IRExpr* mkU64 ( ULong i )
+{
+   return IRExpr_Const(IRConst_U64(i));
+}
+
+static inline IRExpr* mkU32 ( UInt i )
+{
+   return IRExpr_Const(IRConst_U32(i));
+}
+
+static inline IRExpr* mkU16 ( UShort i )
+{
+   return IRExpr_Const(IRConst_U16(i));
+}
+
+static inline IRExpr* mkU8 ( UChar i )
+{
+   return IRExpr_Const(IRConst_U8(i));
+}
+
+static inline IRExpr* mkU1 ( Bool i )
+{
+   return IRExpr_Const(IRConst_U1(i));
+}
+
+static inline IRExpr* mkF64i ( ULong i )
+{
+   return IRExpr_Const(IRConst_F64i(i));
+}
+
+static inline IRExpr* mkF32i ( UInt i )
+{
+   return IRExpr_Const(IRConst_F32i(i));
+}
+
+static inline IRExpr* mkV128 ( UShort i )
+{
+   return IRExpr_Const(IRConst_V128(i));
+}
+
+static inline IRExpr* mkexpr ( IRTemp tmp )
+{
+   return IRExpr_RdTmp(tmp);
+}
+
+static inline IRExpr* unop ( IROp op, IRExpr* a )
+{
+   return IRExpr_Unop(op, a);
+}
+
+static inline IRExpr* binop ( IROp op, IRExpr* a1, IRExpr* a2 )
+{
+   return IRExpr_Binop(op, a1, a2);
+}
+
+static inline IRExpr* triop ( IROp op, IRExpr* a1, IRExpr* a2, IRExpr* a3 )
+{
+   return IRExpr_Triop(op, a1, a2, a3);
+}
+
+static inline IRExpr* qop ( IROp op, IRExpr* a1, IRExpr* a2,
+                            IRExpr* a3, IRExpr* a4 )
+{
+   return IRExpr_Qop(op, a1, a2, a3, a4);
+}
+
+static inline IRExpr* load ( IRType ty, IRExpr* addr )
+{
+   return IRExpr_Load(Iend_LE, ty, addr);
+}
+
+/* Add a statement to the list held by "irbb". */
+static inline void stmt ( IRStmt* st )
+{
+   addStmtToIRSB(irsb, st);
+}
+
+static inline void store ( IRExpr* addr, IRExpr* data )
+{
+   stmt(IRStmt_Store(Iend_LE, addr, data));
+}
+
+static inline void assign ( IRTemp dst, IRExpr* e )
+{
+   stmt(IRStmt_WrTmp(dst, e));
+}
+
+static inline void exit ( IRExpr* e, IRJumpKind jk, ULong offs )
+{
+   stmt(IRStmt_Exit(e, jk, IRConst_U64(guest_PC_curr_instr + offs),
+                    offsetof(VexGuestLOONGARCH64State, guest_PC)));
+}
+
+/* Generate an expression to check if addr is aligned. */
+static inline IRExpr* check_align ( IRExpr* addr, IRExpr* align )
+{
+   return binop(Iop_CmpNE64, binop(Iop_And64, addr, align),
+                IRExpr_Get(offsetof(VexGuestLOONGARCH64State, guest_R0),
+                           Ity_I64));
+}
+
+/* Generate a SIGSYS if the expression evaluates to true. */
+static inline void gen_SIGSYS ( IRExpr* cond )
+{
+   exit(cond, Ijk_SigSYS, 4);
+}
+
+/* Generate a SIGBUS if the expression evaluates to true. */
+static inline void gen_SIGBUS ( IRExpr* cond )
+{
+   exit(cond, Ijk_SigBUS, 4);
+}
+
+static inline void cas ( IRTemp old, IRExpr* addr, IRExpr* expd, IRExpr* new )
+{
+   IRCAS* c = mkIRCAS(IRTemp_INVALID, old, Iend_LE, addr,
+                      NULL, expd, NULL, new);
+   stmt(IRStmt_CAS(c));
+}
+
+/* Generate a new temporary of the given type. */
+static inline IRTemp newTemp ( IRType ty )
+{
+   vassert(isPlausibleIRType(ty));
+   return newIRTemp(irsb->tyenv, ty);
+}
+
+/* S-extend 8/16/32 bit int expr to 64. */
+static IRExpr* extendS ( IRType ty, IRExpr* e )
+{
+   switch (ty) {
+      case Ity_I1:  return unop(Iop_1Sto64, e);
+      case Ity_I8:  return unop(Iop_8Sto64, e);
+      case Ity_I16: return unop(Iop_16Sto64, e);
+      case Ity_I32: return unop(Iop_32Sto64, e);
+      default: vassert(0);
+   }
+}
+
+/* Z-extend 8/16/32 bit int expr to 64. */
+static IRExpr* extendU ( IRType ty, IRExpr* e )
+{
+   switch (ty) {
+      case Ity_I1:  return unop(Iop_1Uto64, e);
+      case Ity_I8:  return unop(Iop_8Uto64, e);
+      case Ity_I16: return unop(Iop_16Uto64, e);
+      case Ity_I32: return unop(Iop_32Uto64, e);
+      default: vassert(0);
+   }
+}
+
+/* Break a V128-bit value up into four 32-bit ints. */
+static void breakupV128to32s ( IRTemp t128,
+                               IRTemp* t3, IRTemp* t2,
+                               IRTemp* t1, IRTemp* t0 )
+{
+   IRTemp hi64 = newTemp(Ity_I64);
+   IRTemp lo64 = newTemp(Ity_I64);
+   assign(hi64, unop(Iop_V128HIto64, mkexpr(t128)));
+   assign(lo64, unop(Iop_V128to64,   mkexpr(t128)));
+
+   vassert(t0 && *t0 == IRTemp_INVALID);
+   vassert(t1 && *t1 == IRTemp_INVALID);
+   vassert(t2 && *t2 == IRTemp_INVALID);
+   vassert(t3 && *t3 == IRTemp_INVALID);
+   *t0 = newTemp(Ity_I32);
+   *t1 = newTemp(Ity_I32);
+   *t2 = newTemp(Ity_I32);
+   *t3 = newTemp(Ity_I32);
+   assign(*t0, unop(Iop_64to32,   mkexpr(lo64)));
+   assign(*t1, unop(Iop_64HIto32, mkexpr(lo64)));
+   assign(*t2, unop(Iop_64to32,   mkexpr(hi64)));
+   assign(*t3, unop(Iop_64HIto32, mkexpr(hi64)));
+}
+
+/* Break a V256-bit value up into four 64-bit ints. */
+static void breakupV256to64s ( IRTemp t256,
+                               IRTemp* t3, IRTemp* t2,
+                               IRTemp* t1, IRTemp* t0 )
+{
+   vassert(t0 && *t0 == IRTemp_INVALID);
+   vassert(t1 && *t1 == IRTemp_INVALID);
+   vassert(t2 && *t2 == IRTemp_INVALID);
+   vassert(t3 && *t3 == IRTemp_INVALID);
+   *t0 = newTemp(Ity_I64);
+   *t1 = newTemp(Ity_I64);
+   *t2 = newTemp(Ity_I64);
+   *t3 = newTemp(Ity_I64);
+   assign(*t0, unop(Iop_V256to64_0, mkexpr(t256)));
+   assign(*t1, unop(Iop_V256to64_1, mkexpr(t256)));
+   assign(*t2, unop(Iop_V256to64_2, mkexpr(t256)));
+   assign(*t3, unop(Iop_V256to64_3, mkexpr(t256)));
+}
+
+/* Break a V256-bit value up into two V128s. */
+static void breakupV256toV128s ( IRTemp t256,
+                                 IRTemp* hi, IRTemp* lo )
+{
+   vassert(hi && *hi == IRTemp_INVALID);
+   vassert(lo && *lo == IRTemp_INVALID);
+   *hi = newTemp(Ity_V128);
+   *lo = newTemp(Ity_V128);
+   assign(*hi, unop(Iop_V256toV128_1, mkexpr(t256)));
+   assign(*lo, unop(Iop_V256toV128_0, mkexpr(t256)));
+}
+
+/* Break a V256-bit value up into eight 32-bit ints.  */
+static void breakupV256to32s ( IRTemp t256,
+                               IRTemp* t7, IRTemp* t6,
+                               IRTemp* t5, IRTemp* t4,
+                               IRTemp* t3, IRTemp* t2,
+                               IRTemp* t1, IRTemp* t0 )
+{
+   IRTemp t128_1 = IRTemp_INVALID;
+   IRTemp t128_0 = IRTemp_INVALID;
+   breakupV256toV128s(t256, &t128_1, &t128_0);
+   breakupV128to32s(t128_1, t7, t6, t5, t4);
+   breakupV128to32s(t128_0, t3, t2, t1, t0);
+}
+
+/* Construct a V256-bit value from four 64-bit ints. */
+static IRExpr* mkV256from64s ( IRTemp t3, IRTemp t2,
+                               IRTemp t1, IRTemp t0 )
+{
+   return binop(Iop_V128HLtoV256,
+                binop(Iop_64HLtoV128, mkexpr(t3), mkexpr(t2)),
+                binop(Iop_64HLtoV128, mkexpr(t1), mkexpr(t0)));
+}
+
+/* Construct a V256-bit value from eight 32-bit ints. */
+static IRExpr* mkV256from32s ( IRTemp t7, IRTemp t6,
+                               IRTemp t5, IRTemp t4,
+                               IRTemp t3, IRTemp t2,
+                               IRTemp t1, IRTemp t0 )
+{
+   return binop(Iop_V128HLtoV256,
+                binop(Iop_64HLtoV128,
+                      binop(Iop_32HLto64, mkexpr(t7), mkexpr(t6)),
+                      binop(Iop_32HLto64, mkexpr(t5), mkexpr(t4))),
+                binop(Iop_64HLtoV128,
+                      binop(Iop_32HLto64, mkexpr(t3), mkexpr(t2)),
+                      binop(Iop_32HLto64, mkexpr(t1), mkexpr(t0)))
+   );
+}
+
+static IROp mkVecGetElem ( UInt size ) {
+   const IROp ops[4]
+      = { Iop_GetElem8x16, Iop_GetElem16x8,
+          Iop_GetElem32x4, Iop_GetElem64x2 };
+   vassert(size < 4);
+   return ops[size];
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Helpers for accessing guest registers.               ---*/
+/*------------------------------------------------------------*/
+
+/* ---------------- Integer registers ---------------- */
+
+static Int offsetIReg ( UInt iregNo )
+{
+   switch (iregNo) {
+      case 0:  return offsetof(VexGuestLOONGARCH64State, guest_R0);
+      case 1:  return offsetof(VexGuestLOONGARCH64State, guest_R1);
+      case 2:  return offsetof(VexGuestLOONGARCH64State, guest_R2);
+      case 3:  return offsetof(VexGuestLOONGARCH64State, guest_R3);
+      case 4:  return offsetof(VexGuestLOONGARCH64State, guest_R4);
+      case 5:  return offsetof(VexGuestLOONGARCH64State, guest_R5);
+      case 6:  return offsetof(VexGuestLOONGARCH64State, guest_R6);
+      case 7:  return offsetof(VexGuestLOONGARCH64State, guest_R7);
+      case 8:  return offsetof(VexGuestLOONGARCH64State, guest_R8);
+      case 9:  return offsetof(VexGuestLOONGARCH64State, guest_R9);
+      case 10: return offsetof(VexGuestLOONGARCH64State, guest_R10);
+      case 11: return offsetof(VexGuestLOONGARCH64State, guest_R11);
+      case 12: return offsetof(VexGuestLOONGARCH64State, guest_R12);
+      case 13: return offsetof(VexGuestLOONGARCH64State, guest_R13);
+      case 14: return offsetof(VexGuestLOONGARCH64State, guest_R14);
+      case 15: return offsetof(VexGuestLOONGARCH64State, guest_R15);
+      case 16: return offsetof(VexGuestLOONGARCH64State, guest_R16);
+      case 17: return offsetof(VexGuestLOONGARCH64State, guest_R17);
+      case 18: return offsetof(VexGuestLOONGARCH64State, guest_R18);
+      case 19: return offsetof(VexGuestLOONGARCH64State, guest_R19);
+      case 20: return offsetof(VexGuestLOONGARCH64State, guest_R20);
+      case 21: return offsetof(VexGuestLOONGARCH64State, guest_R21);
+      case 22: return offsetof(VexGuestLOONGARCH64State, guest_R22);
+      case 23: return offsetof(VexGuestLOONGARCH64State, guest_R23);
+      case 24: return offsetof(VexGuestLOONGARCH64State, guest_R24);
+      case 25: return offsetof(VexGuestLOONGARCH64State, guest_R25);
+      case 26: return offsetof(VexGuestLOONGARCH64State, guest_R26);
+      case 27: return offsetof(VexGuestLOONGARCH64State, guest_R27);
+      case 28: return offsetof(VexGuestLOONGARCH64State, guest_R28);
+      case 29: return offsetof(VexGuestLOONGARCH64State, guest_R29);
+      case 30: return offsetof(VexGuestLOONGARCH64State, guest_R30);
+      case 31: return offsetof(VexGuestLOONGARCH64State, guest_R31);
+      default: vassert(0);
+   }
+}
+
+static IRExpr* getIReg8 ( UInt iregNo )
+{
+   return IRExpr_Get(offsetIReg(iregNo), Ity_I8);
+}
+
+static IRExpr* getIReg16 ( UInt iregNo )
+{
+   return IRExpr_Get(offsetIReg(iregNo), Ity_I16);
+}
+
+static IRExpr* getIReg32 ( UInt iregNo )
+{
+   return IRExpr_Get(offsetIReg(iregNo), Ity_I32);
+}
+
+static IRExpr* getIReg64 ( UInt iregNo )
+{
+   return IRExpr_Get(offsetIReg(iregNo), Ity_I64);
+}
+
+static void putIReg ( UInt iregNo, IRExpr* e )
+{
+   vassert(typeOfIRExpr(irsb->tyenv, e) == Ity_I64);
+   if (iregNo != 0) /* $r0 - constant zero */
+      stmt(IRStmt_Put(offsetIReg(iregNo), e));
+}
+
+static void putPC ( IRExpr* e )
+{
+   vassert(typeOfIRExpr(irsb->tyenv, e) == Ity_I64);
+   stmt(IRStmt_Put(offsetof(VexGuestLOONGARCH64State, guest_PC), e));
+}
+
+/* ---------------- Floating point / vector registers ---------------- */
+
+static Int offsetXReg ( UInt iregNo )
+{
+   switch (iregNo) {
+      case 0:  return offsetof(VexGuestLOONGARCH64State, guest_X0);
+      case 1:  return offsetof(VexGuestLOONGARCH64State, guest_X1);
+      case 2:  return offsetof(VexGuestLOONGARCH64State, guest_X2);
+      case 3:  return offsetof(VexGuestLOONGARCH64State, guest_X3);
+      case 4:  return offsetof(VexGuestLOONGARCH64State, guest_X4);
+      case 5:  return offsetof(VexGuestLOONGARCH64State, guest_X5);
+      case 6:  return offsetof(VexGuestLOONGARCH64State, guest_X6);
+      case 7:  return offsetof(VexGuestLOONGARCH64State, guest_X7);
+      case 8:  return offsetof(VexGuestLOONGARCH64State, guest_X8);
+      case 9:  return offsetof(VexGuestLOONGARCH64State, guest_X9);
+      case 10: return offsetof(VexGuestLOONGARCH64State, guest_X10);
+      case 11: return offsetof(VexGuestLOONGARCH64State, guest_X11);
+      case 12: return offsetof(VexGuestLOONGARCH64State, guest_X12);
+      case 13: return offsetof(VexGuestLOONGARCH64State, guest_X13);
+      case 14: return offsetof(VexGuestLOONGARCH64State, guest_X14);
+      case 15: return offsetof(VexGuestLOONGARCH64State, guest_X15);
+      case 16: return offsetof(VexGuestLOONGARCH64State, guest_X16);
+      case 17: return offsetof(VexGuestLOONGARCH64State, guest_X17);
+      case 18: return offsetof(VexGuestLOONGARCH64State, guest_X18);
+      case 19: return offsetof(VexGuestLOONGARCH64State, guest_X19);
+      case 20: return offsetof(VexGuestLOONGARCH64State, guest_X20);
+      case 21: return offsetof(VexGuestLOONGARCH64State, guest_X21);
+      case 22: return offsetof(VexGuestLOONGARCH64State, guest_X22);
+      case 23: return offsetof(VexGuestLOONGARCH64State, guest_X23);
+      case 24: return offsetof(VexGuestLOONGARCH64State, guest_X24);
+      case 25: return offsetof(VexGuestLOONGARCH64State, guest_X25);
+      case 26: return offsetof(VexGuestLOONGARCH64State, guest_X26);
+      case 27: return offsetof(VexGuestLOONGARCH64State, guest_X27);
+      case 28: return offsetof(VexGuestLOONGARCH64State, guest_X28);
+      case 29: return offsetof(VexGuestLOONGARCH64State, guest_X29);
+      case 30: return offsetof(VexGuestLOONGARCH64State, guest_X30);
+      case 31: return offsetof(VexGuestLOONGARCH64State, guest_X31);
+      default: vassert(0);
+   }
+}
+
+static Int offsetFCC ( UInt iregNo )
+{
+   switch (iregNo) {
+      case 0:  return offsetof(VexGuestLOONGARCH64State, guest_FCC0);
+      case 1:  return offsetof(VexGuestLOONGARCH64State, guest_FCC1);
+      case 2:  return offsetof(VexGuestLOONGARCH64State, guest_FCC2);
+      case 3:  return offsetof(VexGuestLOONGARCH64State, guest_FCC3);
+      case 4:  return offsetof(VexGuestLOONGARCH64State, guest_FCC4);
+      case 5:  return offsetof(VexGuestLOONGARCH64State, guest_FCC5);
+      case 6:  return offsetof(VexGuestLOONGARCH64State, guest_FCC6);
+      case 7:  return offsetof(VexGuestLOONGARCH64State, guest_FCC7);
+      default: vassert(0);
+   }
+}
+
+/* Find the offset of the laneNo'th lane of type laneTy in the given
+   Xreg.  Since the host is little-endian, the least significant lane
+   has the lowest offset. */
+static Int offsetXRegLane ( UInt xregNo, IRType laneTy, UInt laneNo )
+{
+   vassert(host_endness == VexEndnessLE);
+   Int laneSzB;
+   /* Since the host is little-endian, the least significant lane
+      will be at the lowest address. */
+   switch (laneTy) {
+      case Ity_F32:  laneSzB = 4;  break;
+      case Ity_F64:  laneSzB = 8;  break;
+      case Ity_V128: laneSzB = 16; break;
+      case Ity_V256: laneSzB = 32; break;
+      default:       vassert(0);   break;
+   }
+   return offsetXReg(xregNo) + laneNo * laneSzB;
+}
+
+static IRExpr* getXReg ( UInt xregNo )
+{
+   return IRExpr_Get(offsetXRegLane(xregNo, Ity_V256, 0), Ity_V256);
+}
+
+static IRExpr* getVReg ( UInt vregNo )
+{
+   return IRExpr_Get(offsetXRegLane(vregNo, Ity_V128, 0), Ity_V128);
+}
+
+static IRExpr* getFReg64 ( UInt fregNo )
+{
+   return IRExpr_Get(offsetXRegLane(fregNo, Ity_F64, 0), Ity_F64);
+}
+
+static IRExpr* getFReg32 ( UInt fregNo )
+{
+   /* Get FReg32 from FReg64.
+      We could probably use IRExpr_Get(offsetXRegLane(fregNo, Ity_F32, 0), Ity_F32),
+      but that would cause Memcheck to report some errors.
+    */
+   IRExpr* i = unop(Iop_ReinterpF64asI64, getFReg64(fregNo));
+   return unop(Iop_ReinterpI32asF32, unop(Iop_64to32, i));
+}
+
+static IRExpr* getFCC ( UInt iregNo )
+{
+   return IRExpr_Get(offsetFCC(iregNo), Ity_I8);
+}
+
+static IRExpr* getFCSR ( UInt iregNo )
+{
+   /*
+      bits  | name
+      ---------------
+      4:0   | Enables
+      7:5   | 0
+      9:8   | RM
+      15:10 | 0
+      20:16 | Flags
+      23:21 | 0
+      28:24 | Cause
+      31:29 | 0
+    */
+   Int offs = offsetof(VexGuestLOONGARCH64State, guest_FCSR);
+   IRExpr* fcsr0 = IRExpr_Get(offs, Ity_I32);
+   switch (iregNo) {
+      case 0:
+         return fcsr0;
+      case 1:
+         /* FCSR1 is Enables of FCSR0.  It seems that the hardware
+            implementation is that the 7th bit belongs to FCSR1. */
+         return binop(Iop_And32, fcsr0, mkU32(0x0000009f));
+      case 2:
+         /* FCSR2 is Cause and Flags of FCSR0. */
+         return binop(Iop_And32, fcsr0, mkU32(0x1f1f0000));
+      case 3:
+         /* FCSR3 is RM of FCSR0. */
+         return binop(Iop_And32, fcsr0, mkU32(0x00000300));
+      default:
+         vassert(0);
+   }
+}
+
+static void putFReg32 ( UInt iregNo, IRExpr* e )
+{
+   vassert(typeOfIRExpr(irsb->tyenv, e) == Ity_F32);
+   stmt(IRStmt_Put(offsetXReg(iregNo), e));
+}
+
+static void putFReg64 ( UInt iregNo, IRExpr* e )
+{
+   vassert(typeOfIRExpr(irsb->tyenv, e) == Ity_F64);
+   stmt(IRStmt_Put(offsetXReg(iregNo), e));
+}
+
+static void putVReg ( UInt iregNo, IRExpr* e )
+{
+   vassert(typeOfIRExpr(irsb->tyenv, e) == Ity_V128);
+   stmt(IRStmt_Put(offsetXReg(iregNo), e));
+}
+
+static void putXReg ( UInt iregNo, IRExpr* e )
+{
+   vassert(typeOfIRExpr(irsb->tyenv, e) == Ity_V256);
+   stmt(IRStmt_Put(offsetXReg(iregNo), e));
+}
+
+static void putFCC ( UInt iregNo, IRExpr* e )
+{
+   vassert(typeOfIRExpr(irsb->tyenv, e) == Ity_I8);
+   stmt(IRStmt_Put(offsetFCC(iregNo), e));
+}
+
+static void putFCSR ( UInt iregNo, IRExpr* e )
+{
+   vassert(typeOfIRExpr(irsb->tyenv, e) == Ity_I32);
+   IRExpr* fcsr0 = getFCSR(0);
+   IRExpr* and1;
+   IRExpr* and2;
+   switch (iregNo) {
+      case 0:
+         /* It seems that the hardware implementation allows the 6th
+            bit and the 7th bit to be non-zero. */
+         and1 = getIReg32(0);
+         and2 = binop(Iop_And32, e, mkU32(0x1f1f03df));
+         break;
+      case 1:
+         /* FCSR1 is Enables of FCSR0.  It seems that the hardware
+            implementation is that the 7th bit belongs to FCSR1. */
+         and1 = binop(Iop_And32, fcsr0, mkU32(0xffffff60));
+         and2 = binop(Iop_And32, e, mkU32(0x0000009f));
+         break;
+      case 2:
+         /* FCSR2 is Cause and Flags of FCSR0. */
+         and1 = binop(Iop_And32, fcsr0, mkU32(0xe0e0ffff));
+         and2 = binop(Iop_And32, e, mkU32(0x1f1f0000));
+         break;
+      case 3:
+         /* FCSR3 is RM of FCSR0. */
+         and1 = binop(Iop_And32, fcsr0, mkU32(0xfffffcff));
+         and2 = binop(Iop_And32, e, mkU32(0x00000300));
+         break;
+      default:
+         vassert(0);
+   }
+   Int offs = offsetof(VexGuestLOONGARCH64State, guest_FCSR);
+   stmt(IRStmt_Put(offs, binop(Iop_Or32, and1, and2)));
+}
+
+static IRExpr* get_rounding_mode ( void )
+{
+   /*
+      rounding mode | LOONGARCH | IR
+      ------------------------------
+      to nearest    | 00        | 00
+      to zero       | 01        | 11
+      to +infinity  | 10        | 10
+      to -infinity  | 11        | 01
+   */
+
+   /* Bits 8 to 9 in FCSR are rounding mode. */
+   IRExpr* fcsr = getFCSR(0);
+   IRExpr* shr = binop(Iop_Shr32, fcsr, mkU8(8));
+   IRTemp rm = newTemp(Ity_I32);
+   assign(rm, binop(Iop_And32, shr, mkU32(0x3)));
+
+   /* rm = XOR(rm, (rm << 1) & 2) */
+   IRExpr* shl = binop(Iop_Shl32, mkexpr(rm), mkU8(1));
+   IRExpr* and = binop(Iop_And32, shl, mkU32(2));
+   return binop(Iop_Xor32, mkexpr(rm), and);
+}
+
+static void calculateFCSR ( enum fpop op, UInt nargs,
+                            UInt src1, UInt src2, UInt src3 )
+{
+   IRExpr* s1 = NULL;
+   IRExpr* s2 = NULL;
+   IRExpr* s3 = NULL;
+   switch (nargs) {
+      case 3: s3 = unop(Iop_ReinterpF64asI64, getFReg64(src3)); /* fallthrough */
+      case 2: s2 = unop(Iop_ReinterpF64asI64, getFReg64(src2)); /* fallthrough */
+      case 1: s1 = unop(Iop_ReinterpF64asI64, getFReg64(src1)); break;
+      default: vassert(0);
+   }
+   IRExpr** arg = mkIRExprVec_4(mkU64(op), s1, s2, s3);
+   IRExpr* call = mkIRExprCCall(Ity_I64, 0/*regparms*/,
+                                "loongarch64_calculate_FCSR",
+                                &loongarch64_calculate_FCSR,
+                                arg);
+   IRTemp fcsr2 = newTemp(Ity_I32);
+   assign(fcsr2, unop(Iop_64to32, call));
+   putFCSR(2, mkexpr(fcsr2));
+}
+
+static IRExpr* gen_round_to_nearest ( void )
+{
+   return mkU32(0x0);
+}
+
+static IRExpr* gen_round_down ( void )
+{
+   return mkU32(0x1);
+}
+
+static IRExpr* gen_round_up ( void )
+{
+   return mkU32(0x2);
+}
+
+static IRExpr* gen_round_to_zero ( void )
+{
+   return mkU32(0x3);
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Helpers for fixed point arithmetic insns             ---*/
+/*------------------------------------------------------------*/
+
+static Bool gen_add_w ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("add.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* add = binop(Iop_Add32, getIReg32(rj), getIReg32(rk));
+   putIReg(rd, extendS(Ity_I32, add));
+
+   return True;
+}
+
+static Bool gen_add_d ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("add.d %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   putIReg(rd, binop(Iop_Add64, getIReg64(rj), getIReg64(rk)));
+
+   return True;
+}
+
+static Bool gen_sub_w ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("sub.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* sub = binop(Iop_Sub32, getIReg32(rj), getIReg32(rk));
+   putIReg(rd, extendS(Ity_I32, sub));
+
+   return True;
+}
+
+static Bool gen_sub_d ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("sub.d %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   putIReg(rd, binop(Iop_Sub64, getIReg64(rj), getIReg64(rk)));
+
+   return True;
+}
+
+static Bool gen_slt ( DisResult* dres, UInt insn,
+                      const VexArchInfo* archinfo,
+                      const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("slt %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* cond = binop(Iop_CmpLT64S, getIReg64(rj), getIReg64(rk));
+   putIReg(rd, extendU(Ity_I1, cond));
+
+   return True;
+}
+
+static Bool gen_sltu ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("sltu %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* cond = binop(Iop_CmpLT64U, getIReg64(rj), getIReg64(rk));
+   putIReg(rd, extendU(Ity_I1, cond));
+
+   return True;
+}
+
+static Bool gen_slti ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt rj   = SLICE(insn, 9, 5);
+   UInt rd   = SLICE(insn, 4, 0);
+
+   DIP("slti %s, %s, %d\n", nameIReg(rd), nameIReg(rj),
+                           (Int)extend32(si12, 12));
+
+   IRExpr* cond = binop(Iop_CmpLT64S, getIReg64(rj),
+                        mkU64(extend64(si12, 12)));
+   putIReg(rd, extendU(Ity_I1, cond));
+
+   return True;
+}
+
+static Bool gen_sltui ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt rj   = SLICE(insn, 9, 5);
+   UInt rd   = SLICE(insn, 4, 0);
+
+   DIP("sltui %s, %s, %d\n", nameIReg(rd), nameIReg(rj),
+                             (Int)extend32(si12, 12));
+
+   IRExpr* cond = binop(Iop_CmpLT64U, getIReg64(rj),
+                        mkU64(extend64(si12, 12)));
+   putIReg(rd, extendU(Ity_I1, cond));
+
+   return True;
+}
+
+static Bool gen_nor ( DisResult* dres, UInt insn,
+                      const VexArchInfo* archinfo,
+                      const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("nor %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* or = binop(Iop_Or64, getIReg64(rj), getIReg64(rk));
+   putIReg(rd, unop(Iop_Not64, or));
+
+   return True;
+}
+
+static Bool gen_and ( DisResult* dres, UInt insn,
+                      const VexArchInfo* archinfo,
+                      const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("and %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   putIReg(rd, binop(Iop_And64, getIReg64(rj), getIReg64(rk)));
+
+   return True;
+}
+
+static Bool gen_or ( DisResult* dres, UInt insn,
+                     const VexArchInfo* archinfo,
+                     const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("or %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   putIReg(rd, binop(Iop_Or64, getIReg64(rj), getIReg64(rk)));
+
+   return True;
+}
+
+static Bool gen_xor ( DisResult* dres, UInt insn,
+                      const VexArchInfo* archinfo,
+                      const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("xor %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   putIReg(rd, binop(Iop_Xor64, getIReg64(rj), getIReg64(rk)));
+
+   return True;
+}
+
+static Bool gen_orn ( DisResult* dres, UInt insn,
+                      const VexArchInfo* archinfo,
+                      const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("orn %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* not = unop(Iop_Not64, getIReg64(rk));
+   putIReg(rd, binop(Iop_Or64, getIReg64(rj), not));
+
+   return True;
+}
+
+static Bool gen_andn ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("andn %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* not = unop(Iop_Not64, getIReg64(rk));
+   putIReg(rd, binop(Iop_And64, getIReg64(rj), not));
+
+   return True;
+}
+
+static Bool gen_mul_w ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("mul.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* mul = binop(Iop_MullS32, getIReg32(rj), getIReg32(rk));
+   putIReg(rd, extendS(Ity_I32, unop(Iop_64to32, mul)));
+
+   return True;
+}
+
+static Bool gen_mulh_w ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("mulh.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* mul = binop(Iop_MullS32, getIReg32(rj), getIReg32(rk));
+   putIReg(rd, extendS(Ity_I32, unop(Iop_64HIto32, mul)));
+
+   return True;
+}
+
+static Bool gen_mulh_wu ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("mulh.wu %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* mul = binop(Iop_MullU32, getIReg32(rj), getIReg32(rk));
+   putIReg(rd, extendS(Ity_I32, unop(Iop_64HIto32, mul)));
+
+   return True;
+}
+
+static Bool gen_mul_d ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("mul.d %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* mul = binop(Iop_MullS64, getIReg64(rj), getIReg64(rk));
+   putIReg(rd, unop(Iop_128to64, mul));
+
+   return True;
+}
+
+static Bool gen_mulh_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("mulh.d %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* mul = binop(Iop_MullS64, getIReg64(rj), getIReg64(rk));
+   putIReg(rd, unop(Iop_128HIto64, mul));
+
+   return True;
+}
+
+static Bool gen_mulh_du ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("mulh.du %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* mul = binop(Iop_MullU64, getIReg64(rj), getIReg64(rk));
+   putIReg(rd, unop(Iop_128HIto64, mul));
+
+   return True;
+}
+
+static Bool gen_mulw_d_w ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("mulw.d.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   putIReg(rd, binop(Iop_MullS32, getIReg32(rj), getIReg32(rk)));
+
+   return True;
+}
+
+static Bool gen_mulw_d_wu ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("mulw.d.wu %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   putIReg(rd, binop(Iop_MullU32, getIReg32(rj), getIReg32(rk)));
+
+   return True;
+}
+
+static Bool gen_div_w ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("div.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* div = binop(Iop_DivS32, getIReg32(rj), getIReg32(rk));
+   putIReg(rd, extendS(Ity_I32, div));
+
+   return True;
+}
+
+static Bool gen_mod_w ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("mod.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* mod = binop(Iop_DivModS32to32, getIReg32(rj), getIReg32(rk));
+   putIReg(rd, extendS(Ity_I32, unop(Iop_64HIto32, mod)));
+
+   return True;
+}
+
+static Bool gen_div_wu ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("div.wu %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* div = binop(Iop_DivU32, getIReg32(rj), getIReg32(rk));
+   putIReg(rd, extendS(Ity_I32, div));
+
+   return True;
+}
+
+static Bool gen_mod_wu ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("mod.wu %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* mod = binop(Iop_DivModU32to32, getIReg32(rj), getIReg32(rk));
+   putIReg(rd, extendS(Ity_I32, unop(Iop_64HIto32, mod)));
+
+   return True;
+}
+
+static Bool gen_div_d ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("div.d %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   putIReg(rd, binop(Iop_DivS64, getIReg64(rj), getIReg64(rk)));
+
+   return True;
+}
+
+static Bool gen_mod_d ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("mod.d %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* mod = binop(Iop_DivModS64to64, getIReg64(rj), getIReg64(rk));
+   putIReg(rd, unop(Iop_128HIto64, mod));
+
+   return True;
+}
+
+static Bool gen_div_du ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("div.du %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   putIReg(rd, binop(Iop_DivU64, getIReg64(rj), getIReg64(rk)));
+
+   return True;
+}
+
+static Bool gen_mod_du ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("mod.du %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* mod = binop(Iop_DivModU64to64, getIReg64(rj), getIReg64(rk));
+   putIReg(rd, unop(Iop_128HIto64, mod));
+
+   return True;
+}
+
+static Bool gen_alsl_w ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt sa2 = SLICE(insn, 16, 15);
+   UInt  rk = SLICE(insn, 14, 10);
+   UInt  rj = SLICE(insn, 9, 5);
+   UInt  rd = SLICE(insn, 4, 0);
+
+   DIP("alsl.w %s, %s, %s, %u\n", nameIReg(rd), nameIReg(rj),
+                                  nameIReg(rk), sa2);
+
+   IRExpr* shl = binop(Iop_Shl32, getIReg32(rj), mkU8(sa2 + 1));
+   IRExpr* add = binop(Iop_Add32, shl, getIReg32(rk));
+   putIReg(rd, extendS(Ity_I32, add));
+
+   return True;
+}
+
+static Bool gen_alsl_wu ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt sa2 = SLICE(insn, 16, 15);
+   UInt  rk = SLICE(insn, 14, 10);
+   UInt  rj = SLICE(insn, 9, 5);
+   UInt  rd = SLICE(insn, 4, 0);
+
+   DIP("alsl.wu %s, %s, %s, %u\n", nameIReg(rd), nameIReg(rj),
+                                   nameIReg(rk), sa2);
+
+   IRExpr* shl = binop(Iop_Shl32, getIReg32(rj), mkU8(sa2 + 1));
+   IRExpr* add = binop(Iop_Add32, shl, getIReg32(rk));
+   putIReg(rd, extendU(Ity_I32, add));
+
+   return True;
+}
+
+static Bool gen_alsl_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt sa2 = SLICE(insn, 16, 15);
+   UInt  rk = SLICE(insn, 14, 10);
+   UInt  rj = SLICE(insn, 9, 5);
+   UInt  rd = SLICE(insn, 4, 0);
+
+   DIP("alsl.d %s, %s, %s, %u\n", nameIReg(rd), nameIReg(rj),
+                                  nameIReg(rk), sa2);
+
+   IRExpr* shl = binop(Iop_Shl64, getIReg64(rj), mkU8(sa2 + 1));
+   putIReg(rd, binop(Iop_Add64, shl, getIReg64(rk)));
+
+   return True;
+}
+
+static Bool gen_lu12i_w ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt si20 = SLICE(insn, 24, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("lu12i.w %s, %d\n", nameIReg(rd), (Int)extend32(si20, 20));
+
+   IRExpr* imm = mkU32(si20 << 12);
+   putIReg(rd, extendS(Ity_I32, imm));
+
+   return True;
+}
+
+static Bool gen_lu32i_d ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt si20 = SLICE(insn, 24, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("lu32i.d %s, %d\n", nameIReg(rd), (Int)extend32(si20, 20));
+
+   IRExpr* imm = mkU64((ULong)extend32(si20, 20) << 32);
+   IRExpr* shl = binop(Iop_Shl64, getIReg64(rd), mkU8(32));
+   IRExpr* shr = binop(Iop_Shr64, shl, mkU8(32));
+   putIReg(rd, binop(Iop_Or64, imm, shr));
+
+   return True;
+}
+
+static Bool gen_lu52i_d ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("lu52i.d %s, %s, %d\n", nameIReg(rd), nameIReg(rj),
+                               (Int)extend32(si12, 12));
+
+   IRExpr* imm = mkU64((ULong)si12 << 52);
+   IRExpr* shl = binop(Iop_Shl64, getIReg64(rj), mkU8(12));
+   IRExpr* shr = binop(Iop_Shr64, shl, mkU8(12));
+   putIReg(rd, binop(Iop_Or64, imm, shr));
+
+   return True;
+}
+
+static Bool gen_pcaddi ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt si20 = SLICE(insn, 24, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("pcaddi %s, %d\n", nameIReg(rd), (Int)extend32(si20, 20));
+
+   putIReg(rd, mkU64(guest_PC_curr_instr + extend64(si20 << 2, 22)));
+
+   return True;
+}
+
+static Bool gen_pcalau12i ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt si20 = SLICE(insn, 24, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("pcalau12i %s, %d\n", nameIReg(rd), (Int)extend32(si20, 20));
+
+   IRExpr* imm = mkU64(guest_PC_curr_instr + extend64(si20 << 12, 32));
+   IRExpr* shr = binop(Iop_Shr64, imm, mkU8(12));
+   putIReg(rd, binop(Iop_Shl64, shr, mkU8(12)));
+
+   return True;
+}
+
+static Bool gen_pcaddu12i ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt si20 = SLICE(insn, 24, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("pcaddu12i %s, %d\n", nameIReg(rd), (Int)extend32(si20, 20));
+
+   putIReg(rd, mkU64(guest_PC_curr_instr + extend64(si20 << 12, 32)));
+
+   return True;
+}
+
+static Bool gen_pcaddu18i ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt si20 = SLICE(insn, 24, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("pcaddu18i %s, %d\n", nameIReg(rd), (Int)extend32(si20, 20));
+
+   putIReg(rd, mkU64(guest_PC_curr_instr + extend64((ULong)si20 << 18, 38)));
+
+   return True;
+}
+
+static Bool gen_addi_w ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("addi.w %s, %s, %d\n", nameIReg(rd), nameIReg(rj),
+                              (Int)extend32(si12, 12));
+
+   IRExpr* imm = mkU32(extend32(si12, 12));
+   IRExpr* add = binop(Iop_Add32, getIReg32(rj), imm);
+   putIReg(rd, extendS(Ity_I32, add));
+
+   return True;
+}
+
+static Bool gen_addi_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("addi.d %s, %s, %d\n", nameIReg(rd), nameIReg(rj),
+                              (Int)extend32(si12, 12));
+
+   IRExpr* imm = mkU64(extend64(si12, 12));
+   putIReg(rd, binop(Iop_Add64, getIReg64(rj), imm));
+
+   return True;
+}
+
+static Bool gen_addu16i_d ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt si16 = SLICE(insn, 25, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("addu16i.d %s, %s, %d\n", nameIReg(rd), nameIReg(rj),
+                                 (Int)extend32(si16, 16));
+
+   IRExpr* imm = mkU64(extend64(si16 << 16, 32));
+   putIReg(rd, binop(Iop_Add64, getIReg64(rj), imm));
+
+   return True;
+}
+
+static Bool gen_andi ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt ui12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("andi %s, %s, %u\n", nameIReg(rd), nameIReg(rj), ui12);
+
+   IRExpr* imm = mkU64((ULong)ui12);
+   putIReg(rd, binop(Iop_And64, getIReg64(rj), imm));
+
+   return True;
+}
+
+static Bool gen_ori ( DisResult* dres, UInt insn,
+                      const VexArchInfo* archinfo,
+                      const VexAbiInfo* abiinfo )
+{
+   UInt ui12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("ori %s, %s, %u\n", nameIReg(rd), nameIReg(rj), ui12);
+
+   IRExpr* imm = mkU64((ULong)ui12);
+   putIReg(rd, binop(Iop_Or64, getIReg64(rj), imm));
+
+   return True;
+}
+
+static Bool gen_xori ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt ui12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("xori %s, %s, %u\n", nameIReg(rd), nameIReg(rj), ui12);
+
+   IRExpr* imm = mkU64((ULong)ui12);
+   putIReg(rd, binop(Iop_Xor64, getIReg64(rj), imm));
+
+   return True;
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Helpers for fixed point shift insns                  ---*/
+/*------------------------------------------------------------*/
+
+static Bool gen_sll_w ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("sll.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* shl = binop(Iop_Shl32, getIReg32(rj), getIReg8(rk));
+   putIReg(rd, extendS(Ity_I32, shl));
+
+   return True;
+}
+
+static Bool gen_srl_w ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("srl.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* shr = binop(Iop_Shr32, getIReg32(rj), getIReg8(rk));
+   putIReg(rd, extendS(Ity_I32, shr));
+
+   return True;
+}
+
+static Bool gen_sra_w ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("sra.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* sar = binop(Iop_Sar32, getIReg32(rj), getIReg8(rk));
+   putIReg(rd, extendS(Ity_I32, sar));
+
+   return True;
+}
+
+static Bool gen_sll_d ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("sll.d %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   putIReg(rd, binop(Iop_Shl64, getIReg64(rj), getIReg8(rk)));
+
+   return True;
+}
+
+static Bool gen_srl_d ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("srl.d %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   putIReg(rd, binop(Iop_Shr64, getIReg64(rj), getIReg8(rk)));
+
+   return True;
+}
+
+static Bool gen_sra_d ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("sra.d %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   putIReg(rd, binop(Iop_Sar64, getIReg64(rj), getIReg8(rk)));
+
+   return True;
+}
+
+static Bool gen_rotr_w ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("rotr.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRTemp tmp1 = newTemp(Ity_I32);
+   assign(tmp1, getIReg32(rj));
+   IRTemp tmp2 = newTemp(Ity_I8);
+   assign(tmp2, getIReg8(rk));
+   IRExpr* shr = binop(Iop_Shr32, mkexpr(tmp1), mkexpr(tmp2));
+   IRExpr* imm = unop(Iop_8Uto32, mkexpr(tmp2));
+   IRExpr* sub = binop(Iop_Sub32, mkU32(32), imm);
+   IRExpr* imm2 = unop(Iop_32to8, sub);
+   IRExpr* shl = binop(Iop_Shl32, mkexpr(tmp1), imm2);
+   IRExpr* or = binop(Iop_Or32, shr, shl);
+   putIReg(rd, extendS(Ity_I32, or));
+
+   return True;
+}
+
+static Bool gen_rotr_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("rotr.d %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRTemp tmp1 = newTemp(Ity_I64);
+   assign(tmp1, getIReg64(rj));
+   IRTemp tmp2 = newTemp(Ity_I8);
+   assign(tmp2, getIReg8(rk));
+   IRExpr* shr = binop(Iop_Shr64, mkexpr(tmp1), mkexpr(tmp2));
+   IRExpr* imm = unop(Iop_8Uto64, mkexpr(tmp2));
+   IRExpr* sub = binop(Iop_Sub64, mkU64(64), imm);
+   IRExpr* imm2 = unop(Iop_64to8, sub);
+   IRExpr* shl = binop(Iop_Shl64, mkexpr(tmp1), imm2);
+   putIReg(rd, binop(Iop_Or64, shr, shl));
+
+   return True;
+}
+
+static Bool gen_slli_w ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt ui5 = SLICE(insn, 14, 10);
+   UInt  rj = SLICE(insn, 9, 5);
+   UInt  rd = SLICE(insn, 4, 0);
+
+   DIP("slli.w %s, %s, %u\n", nameIReg(rd), nameIReg(rj), ui5);
+
+   IRExpr* shl = binop(Iop_Shl32, getIReg32(rj), mkU8(ui5));
+   putIReg(rd, extendS(Ity_I32, shl));
+
+   return True;
+}
+
+static Bool gen_slli_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt ui6 = SLICE(insn, 15, 10);
+   UInt  rj = SLICE(insn, 9, 5);
+   UInt  rd = SLICE(insn, 4, 0);
+
+   DIP("slli.d %s, %s, %u\n", nameIReg(rd), nameIReg(rj), ui6);
+
+   putIReg(rd, binop(Iop_Shl64, getIReg64(rj), mkU8(ui6)));
+
+   return True;
+}
+
+static Bool gen_srli_w ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt ui5 = SLICE(insn, 14, 10);
+   UInt  rj = SLICE(insn, 9, 5);
+   UInt  rd = SLICE(insn, 4, 0);
+
+   DIP("srli.w %s, %s, %u\n", nameIReg(rd), nameIReg(rj), ui5);
+
+   IRExpr* shr = binop(Iop_Shr32, getIReg32(rj), mkU8(ui5));
+   putIReg(rd, extendS(Ity_I32, shr));
+
+   return True;
+}
+
+static Bool gen_srli_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt ui6 = SLICE(insn, 15, 10);
+   UInt  rj = SLICE(insn, 9, 5);
+   UInt  rd = SLICE(insn, 4, 0);
+
+   DIP("srli.d %s, %s, %u\n", nameIReg(rd), nameIReg(rj), ui6);
+
+   putIReg(rd, binop(Iop_Shr64, getIReg64(rj), mkU8(ui6)));
+
+   return True;
+}
+
+static Bool gen_srai_w ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt ui5 = SLICE(insn, 14, 10);
+   UInt  rj = SLICE(insn, 9, 5);
+   UInt  rd = SLICE(insn, 4, 0);
+
+   DIP("srai.w %s, %s, %u\n", nameIReg(rd), nameIReg(rj), ui5);
+
+   IRExpr* sar = binop(Iop_Sar32, getIReg32(rj), mkU8(ui5));
+   putIReg(rd, extendS(Ity_I32, sar));
+
+   return True;
+}
+
+static Bool gen_srai_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt ui6 = SLICE(insn, 15, 10);
+   UInt  rj = SLICE(insn, 9, 5);
+   UInt  rd = SLICE(insn, 4, 0);
+
+   DIP("srai.d %s, %s, %u\n", nameIReg(rd), nameIReg(rj), ui6);
+
+   putIReg(rd, binop(Iop_Sar64, getIReg64(rj), mkU8(ui6)));
+
+   return True;
+}
+
+static Bool gen_rotri_w ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt ui5 = SLICE(insn, 14, 10);
+   UInt  rj = SLICE(insn, 9, 5);
+   UInt  rd = SLICE(insn, 4, 0);
+
+   DIP("rotri.w %s, %s, %u\n", nameIReg(rd), nameIReg(rj), ui5);
+
+   IRTemp tmp = newTemp(Ity_I32);
+   assign(tmp, getIReg32(rj));
+   IRExpr* shr = binop(Iop_Shr32, mkexpr(tmp), mkU8(ui5));
+   IRExpr* shl = binop(Iop_Shl32, mkexpr(tmp), mkU8(32 - ui5));
+   if (32 - ui5 == 32)
+      shl = mkU32(0);
+   IRExpr* or = binop(Iop_Or32, shr, shl);
+   putIReg(rd, extendS(Ity_I32, or));
+
+   return True;
+}
+
+static Bool gen_rotri_d ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt ui6 = SLICE(insn, 15, 10);
+   UInt  rj = SLICE(insn, 9, 5);
+   UInt  rd = SLICE(insn, 4, 0);
+
+   DIP("rotri.d %s, %s, %u\n", nameIReg(rd), nameIReg(rj), ui6);
+
+   IRTemp tmp = newTemp(Ity_I64);
+   assign(tmp, getIReg64(rj));
+   IRExpr* shr = binop(Iop_Shr64, mkexpr(tmp), mkU8(ui6));
+   IRExpr* shl = binop(Iop_Shl64, mkexpr(tmp), mkU8(64 - ui6));
+   if (64 - ui6 == 64)
+      shl = mkU64(0);
+   putIReg(rd, binop(Iop_Or64, shr, shl));
+
+   return True;
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Helpers for fixed point bit insns                    ---*/
+/*------------------------------------------------------------*/
+
+static Bool gen_ext_w_h ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ext.w.h %s, %s\n", nameIReg(rd), nameIReg(rj));
+
+   putIReg(rd, extendS(Ity_I16, getIReg16(rj)));
+
+   return True;
+}
+
+static Bool gen_ext_w_b ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ext.w.b %s, %s\n", nameIReg(rd), nameIReg(rj));
+
+   putIReg(rd, extendS(Ity_I8, getIReg8(rj)));
+
+   return True;
+}
+
+static Bool gen_clo_w ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("clo.w %s, %s\n", nameIReg(rd), nameIReg(rj));
+
+   IRExpr* not = unop(Iop_Not32, getIReg32(rj));
+   IRExpr* clz = unop(Iop_Clz32, not);
+   putIReg(rd, extendU(Ity_I32, clz));
+
+   return True;
+}
+
+static Bool gen_clz_w ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("clz.w %s, %s\n", nameIReg(rd), nameIReg(rj));
+
+   IRExpr* clz = unop(Iop_Clz32, getIReg32(rj));
+   putIReg(rd, extendU(Ity_I32, clz));
+
+   return True;
+}
+
+static Bool gen_cto_w ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("cto.w %s, %s\n", nameIReg(rd), nameIReg(rj));
+
+   IRExpr* not = unop(Iop_Not32, getIReg32(rj));
+   IRExpr* clz = unop(Iop_Ctz32, not);
+   putIReg(rd, extendU(Ity_I32, clz));
+
+   return True;
+}
+
+static Bool gen_ctz_w ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ctz.w %s, %s\n", nameIReg(rd), nameIReg(rj));
+
+   IRExpr* clz = unop(Iop_Ctz32, getIReg32(rj));
+   putIReg(rd, extendU(Ity_I32, clz));
+
+   return True;
+}
+
+static Bool gen_clo_d ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("clo.d %s, %s\n", nameIReg(rd), nameIReg(rj));
+
+   IRExpr* not = unop(Iop_Not64, getIReg64(rj));
+   putIReg(rd, unop(Iop_Clz64, not));
+
+   return True;
+}
+
+static Bool gen_clz_d ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("clz.d %s, %s\n", nameIReg(rd), nameIReg(rj));
+
+   putIReg(rd, unop(Iop_Clz64, getIReg64(rj)));
+
+   return True;
+}
+
+static Bool gen_cto_d ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("cto.d %s, %s\n", nameIReg(rd), nameIReg(rj));
+
+   IRExpr* not = unop(Iop_Not64, getIReg64(rj));
+   putIReg(rd, unop(Iop_Ctz64, not));
+
+   return True;
+}
+
+static Bool gen_ctz_d ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ctz.d %s, %s\n", nameIReg(rd), nameIReg(rj));
+
+   putIReg(rd, unop(Iop_Ctz64, getIReg64(rj)));
+
+   return True;
+}
+
+static Bool gen_revb_2h ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("revb.2h %s, %s\n", nameIReg(rd), nameIReg(rj));
+
+   IRExpr** arg = mkIRExprVec_1(getIReg64(rj));
+   IRExpr* call = mkIRExprCCall(Ity_I64, 0/*regparms*/,
+                                "loongarch64_calculate_revb_2h",
+                                &loongarch64_calculate_revb_2h,
+                                arg);
+   putIReg(rd, call);
+
+   return True;
+}
+
+static Bool gen_revb_4h ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("revb.4h %s, %s\n", nameIReg(rd), nameIReg(rj));
+
+   IRExpr** arg = mkIRExprVec_1(getIReg64(rj));
+   IRExpr* call = mkIRExprCCall(Ity_I64, 0/*regparms*/,
+                                "loongarch64_calculate_revb_4h",
+                                &loongarch64_calculate_revb_4h,
+                                arg);
+   putIReg(rd, call);
+
+   return True;
+}
+
+static Bool gen_revb_2w ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("revb.2w %s, %s\n", nameIReg(rd), nameIReg(rj));
+
+   IRExpr** arg = mkIRExprVec_1(getIReg64(rj));
+   IRExpr* call = mkIRExprCCall(Ity_I64, 0/*regparms*/,
+                                "loongarch64_calculate_revb_2w",
+                                &loongarch64_calculate_revb_2w,
+                                arg);
+   putIReg(rd, call);
+
+   return True;
+}
+
+static Bool gen_revb_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("revb.d %s, %s\n", nameIReg(rd), nameIReg(rj));
+
+   IRExpr** arg = mkIRExprVec_1(getIReg64(rj));
+   IRExpr* call = mkIRExprCCall(Ity_I64, 0/*regparms*/,
+                                "loongarch64_calculate_revb_d",
+                                &loongarch64_calculate_revb_d,
+                                arg);
+   putIReg(rd, call);
+
+   return True;
+}
+
+static Bool gen_revh_2w ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("revh.2w %s, %s\n", nameIReg(rd), nameIReg(rj));
+
+   IRExpr** arg = mkIRExprVec_1(getIReg64(rj));
+   IRExpr* call = mkIRExprCCall(Ity_I64, 0/*regparms*/,
+                                "loongarch64_calculate_revh_2w",
+                                &loongarch64_calculate_revh_2w,
+                                arg);
+   putIReg(rd, call);
+
+   return True;
+}
+
+static Bool gen_revh_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("revh.d %s, %s\n", nameIReg(rd), nameIReg(rj));
+
+   IRExpr** arg = mkIRExprVec_1(getIReg64(rj));
+   IRExpr* call = mkIRExprCCall(Ity_I64, 0/*regparms*/,
+                                "loongarch64_calculate_revh_d",
+                                &loongarch64_calculate_revh_d,
+                                arg);
+   putIReg(rd, call);
+
+   return True;
+}
+
+static Bool gen_bitrev_4b ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("bitrev.4b %s, %s\n", nameIReg(rd), nameIReg(rj));
+
+   IRExpr** arg = mkIRExprVec_1(getIReg64(rj));
+   IRExpr* call = mkIRExprCCall(Ity_I64, 0/*regparms*/,
+                                "loongarch64_calculate_bitrev_4b",
+                                &loongarch64_calculate_bitrev_4b,
+                                arg);
+   putIReg(rd, call);
+
+   return True;
+}
+
+static Bool gen_bitrev_8b ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("bitrev.8b %s, %s\n", nameIReg(rd), nameIReg(rj));
+
+   IRExpr** arg = mkIRExprVec_1(getIReg64(rj));
+   IRExpr* call = mkIRExprCCall(Ity_I64, 0/*regparms*/,
+                                "loongarch64_calculate_bitrev_8b",
+                                &loongarch64_calculate_bitrev_8b,
+                                arg);
+   putIReg(rd, call);
+
+   return True;
+}
+
+static Bool gen_bitrev_w ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("bitrev.w %s, %s\n", nameIReg(rd), nameIReg(rj));
+
+   IRExpr** arg = mkIRExprVec_1(getIReg64(rj));
+   IRExpr* call = mkIRExprCCall(Ity_I64, 0/*regparms*/,
+                                "loongarch64_calculate_bitrev_w",
+                                &loongarch64_calculate_bitrev_w,
+                                arg);
+   putIReg(rd, call);
+
+   return True;
+}
+
+static Bool gen_bitrev_d ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("bitrev.d %s, %s\n", nameIReg(rd), nameIReg(rj));
+
+   IRExpr** arg = mkIRExprVec_1(getIReg64(rj));
+   IRExpr* call = mkIRExprCCall(Ity_I64, 0/*regparms*/,
+                                "loongarch64_calculate_bitrev_d",
+                                &loongarch64_calculate_bitrev_d,
+                                arg);
+   putIReg(rd, call);
+
+   return True;
+}
+
+static Bool gen_bytepick_w ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt sa2 = SLICE(insn, 16, 15);
+   UInt  rk = SLICE(insn, 14, 10);
+   UInt  rj = SLICE(insn, 9, 5);
+   UInt  rd = SLICE(insn, 4, 0);
+
+   DIP("bytepick.w %s, %s, %s, %u\n", nameIReg(rd), nameIReg(rj),
+                                      nameIReg(rk), sa2);
+
+   UInt shift = 8 * (4 - sa2);
+   IRExpr* shl = binop(Iop_Shl32, getIReg32(rk), mkU8(32 - shift));
+   if (32 - shift == 32)
+      shl = mkU32(0);
+   IRExpr* shr = binop(Iop_Shr32, getIReg32(rj), mkU8(shift));
+   if (shift == 32)
+      shr = mkU32(0);
+   IRExpr* or = binop(Iop_Or32, shl, shr);
+   putIReg(rd, extendS(Ity_I32, or));
+
+   return True;
+}
+
+static Bool gen_bytepick_d ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt sa3 = SLICE(insn, 17, 15);
+   UInt  rk = SLICE(insn, 14, 10);
+   UInt  rj = SLICE(insn, 9, 5);
+   UInt  rd = SLICE(insn, 4, 0);
+
+   DIP("bytepick.d %s, %s, %s, %u\n", nameIReg(rd), nameIReg(rj),
+                                      nameIReg(rk), sa3);
+
+   UInt shift = 8 * (8 - sa3);
+   IRExpr* shl = binop(Iop_Shl64, getIReg64(rk), mkU8(64 - shift));
+   if (64 - shift == 64)
+      shl = mkU64(0);
+   IRExpr* shr = binop(Iop_Shr64, getIReg64(rj), mkU8(shift));
+   if (shift == 64)
+      shr = mkU64(0);
+   putIReg(rd, binop(Iop_Or64, shl, shr));
+
+   return True;
+}
+
+static Bool gen_maskeqz ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("maskeqz %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* cond = binop(Iop_CmpNE64, getIReg64(rk), mkU64(0));
+   putIReg(rd, binop(Iop_And64, extendS(Ity_I1, cond), getIReg64(rj)));
+
+   return True;
+}
+
+static Bool gen_masknez ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("masknez %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* cond = binop(Iop_CmpEQ64, getIReg64(rk), mkU64(0));
+   putIReg(rd, binop(Iop_And64, extendS(Ity_I1, cond), getIReg64(rj)));
+
+   return True;
+}
+
+static Bool gen_bstrins_w ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt msb = SLICE(insn, 20, 16);
+   UInt lsb = SLICE(insn, 14, 10);
+   UInt  rj = SLICE(insn, 9, 5);
+   UInt  rd = SLICE(insn, 4, 0);
+
+   DIP("bstrins.w %s, %s, %u, %u\n", nameIReg(rd), nameIReg(rj), msb, lsb);
+
+   IRTemp tmp = newTemp(Ity_I32);
+   assign(tmp, getIReg32(rd));
+   IRExpr* shl1;
+   if (msb == 31) {
+      shl1 = mkU32(0);
+   } else {
+      IRExpr* shr1 = binop(Iop_Shr32, mkexpr(tmp), mkU8(msb + 1));
+      shl1 = binop(Iop_Shl32, shr1, mkU8(msb + 1));
+   }
+   IRExpr* shl2 = binop(Iop_Shl32, getIReg32(rj), mkU8(31 - msb + lsb));
+   IRExpr* shr2 = binop(Iop_Shr32, shl2, mkU8(31 - msb));
+   IRExpr* shr3;
+   if (lsb == 0) {
+      shr3 = mkU32(0);
+   } else {
+      IRExpr* shl3 = binop(Iop_Shl32, mkexpr(tmp), mkU8(32 - lsb));
+      shr3 = binop(Iop_Shr32, shl3, mkU8(32 - lsb));
+   }
+   IRExpr* or1 = binop(Iop_Or32, shl1, shr2);
+   IRExpr* or2 = binop(Iop_Or32, or1, shr3);
+   putIReg(rd, extendS(Ity_I32, or2));
+
+   return True;
+}
+
+static Bool gen_bstrpick_w ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt msb = SLICE(insn, 20, 16);
+   UInt lsb = SLICE(insn, 14, 10);
+   UInt  rj = SLICE(insn, 9, 5);
+   UInt  rd = SLICE(insn, 4, 0);
+
+   DIP("bstrpick.w %s, %s, %u, %u\n", nameIReg(rd), nameIReg(rj), msb, lsb);
+
+   IRExpr* shl = binop(Iop_Shl32, getIReg32(rj), mkU8(31 - msb));
+   IRExpr* shr = binop(Iop_Shr32, shl, mkU8(31 - msb + lsb));
+   putIReg(rd, extendS(Ity_I32, shr));
+
+   return True;
+}
+
+static Bool gen_bstrins_d ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt msb = SLICE(insn, 21, 16);
+   UInt lsb = SLICE(insn, 15, 10);
+   UInt  rj = SLICE(insn, 9, 5);
+   UInt  rd = SLICE(insn, 4, 0);
+
+   DIP("bstrins.d %s, %s, %u, %u\n", nameIReg(rd), nameIReg(rj), msb, lsb);
+
+   IRTemp tmp = newTemp(Ity_I64);
+   assign(tmp, getIReg64(rd));
+   IRExpr* shl1;
+   if (msb == 63) {
+      shl1 = mkU64(0);
+   } else {
+      IRExpr* shr1 = binop(Iop_Shr64, mkexpr(tmp), mkU8(msb + 1));
+      shl1 = binop(Iop_Shl64, shr1, mkU8(msb + 1));
+   }
+   IRExpr* shl2 = binop(Iop_Shl64, getIReg64(rj), mkU8(63 - msb + lsb));
+   IRExpr* shr2 = binop(Iop_Shr64, shl2, mkU8(63 - msb));
+   IRExpr* shr3;
+   if (lsb == 0) {
+      shr3 = mkU64(0);
+   } else {
+      IRExpr* shl3 = binop(Iop_Shl64, mkexpr(tmp), mkU8(64 - lsb));
+      shr3 = binop(Iop_Shr64, shl3, mkU8(64 - lsb));
+   }
+   IRExpr* or = binop(Iop_Or64, shl1, shr2);
+   putIReg(rd, binop(Iop_Or64, or, shr3));
+
+   return True;
+}
+
+static Bool gen_bstrpick_d ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt msb = SLICE(insn, 21, 16);
+   UInt lsb = SLICE(insn, 15, 10);
+   UInt  rj = SLICE(insn, 9, 5);
+   UInt  rd = SLICE(insn, 4, 0);
+
+   DIP("bstrpick.d %s, %s, %u, %u\n", nameIReg(rd), nameIReg(rj), msb, lsb);
+
+   IRExpr* shl = binop(Iop_Shl64, getIReg64(rj), mkU8(63 - msb));
+   putIReg(rd, binop(Iop_Shr64, shl, mkU8(63 - msb + lsb)));
+
+   return True;
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Helpers for fixed point load/store insns             ---*/
+/*------------------------------------------------------------*/
+
+static Bool gen_ld_b ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("ld.b %s, %s, %d\n", nameIReg(rd), nameIReg(rj),
+                            (Int)extend32(si12, 12));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), mkU64(extend64(si12, 12)));
+   putIReg(rd, extendS(Ity_I8, load(Ity_I8, addr)));
+
+   return True;
+}
+
+static Bool gen_ld_h ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("ld.h %s, %s, %d\n", nameIReg(rd), nameIReg(rj),
+                            (Int)extend32(si12, 12));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), mkU64(extend64(si12, 12)));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x1)));
+   putIReg(rd, extendS(Ity_I16, load(Ity_I16, addr)));
+
+   return True;
+}
+
+static Bool gen_ld_w ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("ld.w %s, %s, %d\n", nameIReg(rd), nameIReg(rj),
+                            (Int)extend32(si12, 12));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), mkU64(extend64(si12, 12)));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x3)));
+   putIReg(rd, extendS(Ity_I32, load(Ity_I32, addr)));
+
+   return True;
+}
+
+static Bool gen_ld_d ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("ld.d %s, %s, %d\n", nameIReg(rd), nameIReg(rj),
+                            (Int)extend32(si12, 12));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), mkU64(extend64(si12, 12)));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x7)));
+   putIReg(rd, load(Ity_I64, addr));
+
+   return True;
+}
+
+static Bool gen_st_b ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("st.b %s, %s, %d\n", nameIReg(rd), nameIReg(rj),
+                            (Int)extend32(si12, 12));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), mkU64(extend64(si12, 12)));
+   store(addr, getIReg8(rd));
+
+   return True;
+}
+
+static Bool gen_st_h ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("st.h %s, %s, %d\n", nameIReg(rd), nameIReg(rj),
+                            (Int)extend32(si12, 12));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), mkU64(extend64(si12, 12)));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x1)));
+   store(addr, getIReg16(rd));
+
+   return True;
+}
+
+static Bool gen_st_w ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("st.w %s, %s, %d\n", nameIReg(rd), nameIReg(rj),
+                            (Int)extend32(si12, 12));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), mkU64(extend64(si12, 12)));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x3)));
+   store(addr, getIReg32(rd));
+
+   return True;
+}
+
+static Bool gen_st_d ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("st.d %s, %s, %d\n", nameIReg(rd), nameIReg(rj),
+                            (Int)extend32(si12, 12));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), mkU64(extend64(si12, 12)));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x7)));
+   store(addr, getIReg64(rd));
+
+   return True;
+}
+
+static Bool gen_ld_bu ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("ld.bu %s, %s, %d\n", nameIReg(rd), nameIReg(rj),
+                             (Int)extend32(si12, 12));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), mkU64(extend64(si12, 12)));
+   putIReg(rd, extendU(Ity_I8, load(Ity_I8, addr)));
+
+   return True;
+}
+
+static Bool gen_ld_hu ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("ld.hu %s, %s, %d\n", nameIReg(rd), nameIReg(rj),
+                             (Int)extend32(si12, 12));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), mkU64(extend64(si12, 12)));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x1)));
+   putIReg(rd, extendU(Ity_I16, load(Ity_I16, addr)));
+
+   return True;
+}
+
+static Bool gen_ld_wu ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("ld.wu %s, %s, %d\n", nameIReg(rd), nameIReg(rj),
+                             (Int)extend32(si12, 12));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), mkU64(extend64(si12, 12)));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x3)));
+   putIReg(rd, extendU(Ity_I32, load(Ity_I32, addr)));
+
+   return True;
+}
+
+static Bool gen_ldx_b ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ldx.b %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), getIReg64(rk));
+   putIReg(rd, extendS(Ity_I8, load(Ity_I8, addr)));
+
+   return True;
+}
+
+static Bool gen_ldx_h ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ldx.h %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), getIReg64(rk));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x1)));
+   putIReg(rd, extendS(Ity_I16, load(Ity_I16, addr)));
+
+   return True;
+}
+
+static Bool gen_ldx_w ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ldx.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), getIReg64(rk));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x3)));
+   putIReg(rd, extendS(Ity_I32, load(Ity_I32, addr)));
+
+   return True;
+}
+
+static Bool gen_ldx_d ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ldx.d %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), getIReg64(rk));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x7)));
+   putIReg(rd, load(Ity_I64, addr));
+
+   return True;
+}
+
+static Bool gen_stx_b ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("stx.b %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), getIReg64(rk));
+   store(addr, getIReg8(rd));
+
+   return True;
+}
+
+static Bool gen_stx_h ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("stx.h %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), getIReg64(rk));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x1)));
+   store(addr, getIReg16(rd));
+
+   return True;
+}
+
+static Bool gen_stx_w ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("stx.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), getIReg64(rk));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x3)));
+   store(addr, getIReg32(rd));
+
+   return True;
+}
+
+static Bool gen_stx_d ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("stx.d %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), getIReg64(rk));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x7)));
+   store(addr, getIReg64(rd));
+
+   return True;
+}
+
+static Bool gen_ldx_bu ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ldx.bu %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), getIReg64(rk));
+   putIReg(rd, extendU(Ity_I8, load(Ity_I8, addr)));
+
+   return True;
+}
+
+static Bool gen_ldx_hu ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ldx.hu %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), getIReg64(rk));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x1)));
+   putIReg(rd, extendU(Ity_I16, load(Ity_I16, addr)));
+
+   return True;
+}
+
+static Bool gen_ldx_wu ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ldx.wu %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), getIReg64(rk));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x3)));
+   putIReg(rd, extendU(Ity_I32, load(Ity_I32, addr)));
+
+   return True;
+}
+
+static Bool gen_preld ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt rj   = SLICE(insn, 9, 5);
+   UInt hint = SLICE(insn, 4, 0);
+
+   DIP("preld %u, %s, %d\n", hint, nameIReg(rj), (Int)extend32(si12, 12));
+
+   return True;
+}
+
+static Bool gen_preldx ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt rj   = SLICE(insn, 9, 5);
+   UInt hint = SLICE(insn, 4, 0);
+
+   DIP("preldx %u, %s, %d\n", hint, nameIReg(rj), (Int)extend32(si12, 12));
+
+   return True;
+}
+
+static Bool gen_dbar ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt hint = SLICE(insn, 14, 0);
+
+   DIP("dbar %u\n", hint);
+
+   stmt(IRStmt_MBE(Imbe_Fence));
+
+   return True;
+}
+
+static Bool gen_ibar ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt hint = SLICE(insn, 14, 0);
+
+   DIP("ibar %u\n", hint);
+
+   stmt(IRStmt_MBE(Imbe_InsnFence));
+
+   return True;
+}
+
+static Bool gen_ldptr_w ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt si14 = SLICE(insn, 23, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("ldptr.w %s, %s, %d\n", nameIReg(rd), nameIReg(rj),
+                               (Int)extend32(si14, 14));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj),
+                        mkU64(extend64(si14 << 2, 16)));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x3)));
+   putIReg(rd, extendS(Ity_I32, load(Ity_I32, addr)));
+
+   return True;
+}
+
+static Bool gen_stptr_w ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt si14 = SLICE(insn, 23, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("stptr.w %s, %s, %d\n", nameIReg(rd), nameIReg(rj),
+                               (Int)extend32(si14, 14));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj),
+                        mkU64(extend64(si14 << 2, 16)));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x3)));
+   store(addr, getIReg32(rd));
+
+   return True;
+}
+
+static Bool gen_ldptr_d ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt si14 = SLICE(insn, 23, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("ldptr.d %s, %s, %d\n", nameIReg(rd), nameIReg(rj),
+                               (Int)extend32(si14, 14));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj),
+                        mkU64(extend64(si14 << 2, 16)));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x7)));
+   putIReg(rd, load(Ity_I64, addr));
+
+   return True;
+}
+
+static Bool gen_stptr_d ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt si14 = SLICE(insn, 23, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("stptr.d %s, %s, %d\n", nameIReg(rd), nameIReg(rj),
+                               (Int)extend32(si14, 14));
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj),
+                        mkU64(extend64(si14 << 2, 16)));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x7)));
+   store(addr, getIReg64(rd));
+
+   return True;
+}
+
+static Bool gen_ldgt_b ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ldgt.b %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGSYS(binop(Iop_CmpLE64U, mkexpr(addr), getIReg64(rk)));
+   putIReg(rd, extendS(Ity_I8, load(Ity_I8, mkexpr(addr))));
+
+   return True;
+}
+
+static Bool gen_ldgt_h ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ldgt.h %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x1)));
+   gen_SIGSYS(binop(Iop_CmpLE64U, mkexpr(addr), getIReg64(rk)));
+   putIReg(rd, extendS(Ity_I16, load(Ity_I16, mkexpr(addr))));
+
+   return True;
+}
+
+static Bool gen_ldgt_w ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ldgt.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x3)));
+   gen_SIGSYS(binop(Iop_CmpLE64U, mkexpr(addr), getIReg64(rk)));
+   putIReg(rd, extendS(Ity_I32, load(Ity_I32, mkexpr(addr))));
+
+   return True;
+}
+
+static Bool gen_ldgt_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ldgt.d %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x7)));
+   gen_SIGSYS(binop(Iop_CmpLE64U, mkexpr(addr), getIReg64(rk)));
+   putIReg(rd, load(Ity_I64, mkexpr(addr)));
+
+   return True;
+}
+
+static Bool gen_ldle_b ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ldle.b %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGSYS(binop(Iop_CmpLT64U, getIReg64(rk), mkexpr(addr)));
+   putIReg(rd, extendS(Ity_I8, load(Ity_I8, mkexpr(addr))));
+
+   return True;
+}
+
+static Bool gen_ldle_h ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ldle.h %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x1)));
+   gen_SIGSYS(binop(Iop_CmpLT64U, getIReg64(rk), mkexpr(addr)));
+   putIReg(rd, extendS(Ity_I16, load(Ity_I16, mkexpr(addr))));
+
+   return True;
+}
+
+static Bool gen_ldle_w ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ldle.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x3)));
+   gen_SIGSYS(binop(Iop_CmpLT64U, getIReg64(rk), mkexpr(addr)));
+   putIReg(rd, extendS(Ity_I32, load(Ity_I32, mkexpr(addr))));
+
+   return True;
+}
+
+static Bool gen_ldle_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ldle.d %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x7)));
+   gen_SIGSYS(binop(Iop_CmpLT64U, getIReg64(rk), mkexpr(addr)));
+   putIReg(rd, load(Ity_I64, mkexpr(addr)));
+
+   return True;
+}
+
+static Bool gen_stgt_b ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("stgt.b %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGSYS(binop(Iop_CmpLE64U, mkexpr(addr), getIReg64(rk)));
+   store(mkexpr(addr), getIReg8(rd));
+
+   return True;
+}
+
+static Bool gen_stgt_h ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("stgt.h %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x1)));
+   gen_SIGSYS(binop(Iop_CmpLE64U, mkexpr(addr), getIReg64(rk)));
+   store(mkexpr(addr), getIReg16(rd));
+
+   return True;
+}
+
+static Bool gen_stgt_w ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("stgt.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x3)));
+   gen_SIGSYS(binop(Iop_CmpLE64U, mkexpr(addr), getIReg64(rk)));
+   store(mkexpr(addr), getIReg32(rd));
+
+   return True;
+}
+
+static Bool gen_stgt_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("stgt.d %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x7)));
+   gen_SIGSYS(binop(Iop_CmpLE64U, mkexpr(addr), getIReg64(rk)));
+   store(mkexpr(addr), getIReg64(rd));
+
+   return True;
+}
+
+static Bool gen_stle_b ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("stle.b %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGSYS(binop(Iop_CmpLT64U, getIReg64(rk), mkexpr(addr)));
+   store(mkexpr(addr), getIReg8(rd));
+
+   return True;
+}
+
+static Bool gen_stle_h ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("stle.h %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x1)));
+   gen_SIGSYS(binop(Iop_CmpLT64U, getIReg64(rk), mkexpr(addr)));
+   store(mkexpr(addr), getIReg16(rd));
+
+   return True;
+}
+
+static Bool gen_stle_w ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("stle.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x3)));
+   gen_SIGSYS(binop(Iop_CmpLT64U, getIReg64(rk), mkexpr(addr)));
+   store(mkexpr(addr), getIReg32(rd));
+
+   return True;
+}
+
+static Bool gen_stle_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("stle.d %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x7)));
+   gen_SIGSYS(binop(Iop_CmpLT64U, getIReg64(rk), mkexpr(addr)));
+   store(mkexpr(addr), getIReg64(rd));
+
+   return True;
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Helpers for fixed point atomic insns                 ---*/
+/*------------------------------------------------------------*/
+
+static Bool gen_ll_helper ( UInt rd, UInt rj, UInt si14, Bool size64 )
+{
+   Int offs_size = offsetof(VexGuestLOONGARCH64State, guest_LLSC_SIZE);
+   Int offs_addr = offsetof(VexGuestLOONGARCH64State, guest_LLSC_ADDR);
+   Int offs_data = offsetof(VexGuestLOONGARCH64State, guest_LLSC_DATA);
+
+   /* Get address of the load. */
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, binop(Iop_Add64, getIReg64(rj),
+                      mkU64(extend64(si14 << 2, 16))));
+   if (size64)
+      gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x7)));
+   else
+      gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x3)));
+
+   /* Load the value. */
+   IRTemp res = newTemp(Ity_I64);
+   if (size64)
+      assign(res, load(Ity_I64, mkexpr(addr)));
+   else
+      assign(res, extendS(Ity_I32, load(Ity_I32, mkexpr(addr))));
+
+   /* Set up the LLSC fallback data. */
+   if (size64)
+      stmt(IRStmt_Put(offs_size, mkU64(8)));
+   else
+      stmt(IRStmt_Put(offs_size, mkU64(4)));
+   stmt(IRStmt_Put(offs_addr, mkexpr(addr)));
+   stmt(IRStmt_Put(offs_data, mkexpr(res)));
+
+   /* Write the result to the destination register. */
+   putIReg(rd, mkexpr(res));
+
+   return True;
+}
+
+static Bool gen_sc_helper ( UInt rd, UInt rj, UInt si14, Bool size64 )
+{
+   Int offs_size = offsetof(VexGuestLOONGARCH64State, guest_LLSC_SIZE);
+   Int offs_addr = offsetof(VexGuestLOONGARCH64State, guest_LLSC_ADDR);
+   Int offs_data = offsetof(VexGuestLOONGARCH64State, guest_LLSC_DATA);
+
+   /* Get address of the load. */
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, binop(Iop_Add64, getIReg64(rj),
+                      mkU64(extend64(si14 << 2, 16))));
+   if (size64)
+      gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x7)));
+   else
+      gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x3)));
+
+   /* Get new value. */
+   IRTemp new;
+   if (size64) {
+      new = newTemp(Ity_I64);
+      assign(new, getIReg64(rd));
+   } else {
+      new = newTemp(Ity_I32);
+      assign(new, getIReg32(rd));
+   }
+
+   /* Mark the SC initially as failed. */
+   putIReg(rd, mkU64(0));
+
+   /* Set that no transaction is in progress. */
+   IRTemp size = newTemp(Ity_I64);
+   assign(size, IRExpr_Get(offs_size, Ity_I64));
+   stmt(IRStmt_Put(offs_size, mkU64(0) /* "no transaction" */));
+
+   /* Fail if no or wrong-size transaction. */
+   if (size64)
+      exit(binop(Iop_CmpNE64, mkexpr(size), mkU64(8)), Ijk_Boring, 4);
+   else
+      exit(binop(Iop_CmpNE64, mkexpr(size), mkU64(4)), Ijk_Boring, 4);
+
+   /* Fail if the address doesn't match the LL address. */
+   exit(binop(Iop_CmpNE64, mkexpr(addr), IRExpr_Get(offs_addr, Ity_I64)),
+        Ijk_Boring, 4);
+
+   /* Fail if the data doesn't match the LL data. */
+   IRTemp data;
+   if (size64) {
+      data = newTemp(Ity_I64);
+      assign(data, IRExpr_Get(offs_data, Ity_I64));
+      IRExpr* d = load(Ity_I64, mkexpr(addr));
+      exit(binop(Iop_CmpNE64, d, mkexpr(data)), Ijk_Boring, 4);
+   } else {
+      data = newTemp(Ity_I32);
+      IRTemp tmp = newTemp(Ity_I64);
+      assign(tmp, IRExpr_Get(offs_data, Ity_I64));
+      assign(data, unop(Iop_64to32, mkexpr(tmp)));
+      IRExpr* d = extendS(Ity_I32, load(Ity_I32, mkexpr(addr)));
+      exit(binop(Iop_CmpNE64, d, mkexpr(tmp)), Ijk_Boring, 4);
+   }
+
+   /* Try to CAS the new value in. */
+   IRTemp old;
+   if (size64) {
+      old = newTemp(Ity_I64);
+      cas(old, mkexpr(addr), mkexpr(data), mkexpr(new));
+   } else {
+      old = newTemp(Ity_I32);
+      cas(old, mkexpr(addr), mkexpr(data), mkexpr(new));
+   }
+
+   /* Fail if the CAS failed (old != expd). */
+   if (size64)
+      exit(binop(Iop_CasCmpNE64, mkexpr(old), mkexpr(data)), Ijk_Boring, 4);
+   else
+      exit(binop(Iop_CasCmpNE32, mkexpr(old), mkexpr(data)), Ijk_Boring, 4);
+
+   /* Otherwise mark the operation as successful. */
+   putIReg(rd, mkU64(1));
+
+   return True;
+}
+
+static Bool gen_ll_w ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt si14 = SLICE(insn, 23, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("ll.w %s, %s, %d%s\n", nameIReg(rd), nameIReg(rj),
+                              (Int)extend32(si14, 14),
+                              abiinfo->guest__use_fallback_LLSC ?
+                              " (fallback implementation)" : "");
+
+   if (abiinfo->guest__use_fallback_LLSC) {
+      return gen_ll_helper(rd, rj, si14, False);
+   } else {
+      IRTemp  res = newTemp(Ity_I32);
+      IRTemp addr = newTemp(Ity_I64);
+      assign(addr, binop(Iop_Add64, getIReg64(rj),
+                         mkU64(extend64(si14 << 2, 16))));
+      gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x3)));
+      stmt(IRStmt_LLSC(Iend_LE, res, mkexpr(addr), NULL/*LL*/));
+      putIReg(rd, extendS(Ity_I32, mkexpr(res)));
+      return True;
+   }
+}
+
+static Bool gen_sc_w ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt si14 = SLICE(insn, 23, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("sc.w %s, %s, %d%s\n", nameIReg(rd), nameIReg(rj),
+                              (Int)extend32(si14, 14),
+                              abiinfo->guest__use_fallback_LLSC ?
+                              " (fallback implementation)" : "");
+
+   if (abiinfo->guest__use_fallback_LLSC) {
+      return gen_sc_helper(rd, rj, si14, False);
+   } else {
+      IRTemp  res = newTemp(Ity_I1);
+      IRTemp addr = newTemp(Ity_I64);
+      assign(addr, binop(Iop_Add64, getIReg64(rj),
+                         mkU64(extend64(si14 << 2, 16))));
+      gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x3)));
+      stmt(IRStmt_LLSC(Iend_LE, res, mkexpr(addr), getIReg32(rd)));
+      return True;
+   }
+}
+
+static Bool gen_ll_d ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt si14 = SLICE(insn, 23, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("ll.d %s, %s, %d%s\n", nameIReg(rd), nameIReg(rj),
+                              (Int)extend32(si14, 14),
+                              abiinfo->guest__use_fallback_LLSC ?
+                              " (fallback implementation)" : "");
+
+   if (abiinfo->guest__use_fallback_LLSC) {
+      return gen_ll_helper(rd, rj, si14, True);
+   } else {
+      IRTemp  res = newTemp(Ity_I64);
+      IRTemp addr = newTemp(Ity_I64);
+      assign(addr, binop(Iop_Add64, getIReg64(rj),
+                         mkU64(extend64(si14 << 2, 16))));
+      gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x7)));
+      stmt(IRStmt_LLSC(Iend_LE, res, mkexpr(addr), NULL/*LL*/));
+      putIReg(rd, mkexpr(res));
+      return True;
+   }
+}
+
+static Bool gen_sc_d ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt si14 = SLICE(insn, 23, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("sc.d %s, %s, %d%s\n", nameIReg(rd), nameIReg(rj),
+                              (Int)extend32(si14, 14),
+                              abiinfo->guest__use_fallback_LLSC ?
+                              " (fallback implementation)" : "");
+
+   if (abiinfo->guest__use_fallback_LLSC) {
+      return gen_sc_helper(rd, rj, si14, True);
+   } else {
+      IRTemp  res = newTemp(Ity_I1);
+      IRTemp addr = newTemp(Ity_I64);
+      assign(addr, binop(Iop_Add64, getIReg64(rj),
+                         mkU64(extend64(si14 << 2, 16))));
+      gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x7)));
+      stmt(IRStmt_LLSC(Iend_LE, res, mkexpr(addr), getIReg64(rd)));
+      return True;
+   }
+}
+
+enum amop {
+   AMSWAP, AMADD, AMAND, AMOR, AMXOR, AMMAX, AMMIN, AMMAX_U, AMMIN_U
+};
+
+static Bool gen_am_w_helper ( enum amop op, Bool fence,
+                              UInt rd, UInt rj, UInt rk )
+{
+   if (fence)
+      stmt(IRStmt_MBE(Imbe_Fence));
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x3)));
+
+   IRTemp o = newTemp(Ity_I32);
+   assign(o, load(Ity_I32, mkexpr(addr)));
+   IRTemp n = newTemp(Ity_I32);
+   assign(n, getIReg32(rk));
+   IRExpr* e;
+   switch (op) {
+      case AMSWAP:
+         e = mkexpr(n);
+         break;
+      case AMADD:
+         e = binop(Iop_Add32, mkexpr(o), mkexpr(n));
+         break;
+      case AMAND:
+         e = binop(Iop_And32, mkexpr(o), mkexpr(n));
+         break;
+      case AMOR:
+         e = binop(Iop_Or32, mkexpr(o), mkexpr(n));
+         break;
+      case AMXOR:
+         e = binop(Iop_Xor32, mkexpr(o), mkexpr(n));
+         break;
+      case AMMAX: {
+         IRExpr* cond = binop(Iop_CmpLT32S, mkexpr(n), mkexpr(o));
+         e = IRExpr_ITE(cond, mkexpr(o), mkexpr(n));
+         break;
+      }
+      case AMMIN: {
+         IRExpr* cond = binop(Iop_CmpLT32S, mkexpr(o), mkexpr(n));
+         e = IRExpr_ITE(cond, mkexpr(o), mkexpr(n));
+         break;
+      }
+      case AMMAX_U: {
+         IRExpr* cond = binop(Iop_CmpLT32U, mkexpr(n), mkexpr(o));
+         e = IRExpr_ITE(cond, mkexpr(o), mkexpr(n));
+         break;
+      }
+      case AMMIN_U: {
+         IRExpr* cond = binop(Iop_CmpLT32U, mkexpr(o), mkexpr(n));
+         e = IRExpr_ITE(cond, mkexpr(o), mkexpr(n));
+         break;
+      }
+      default:
+         return False;
+   }
+
+   IRTemp old = newTemp(Ity_I32);
+   cas(old, mkexpr(addr), mkexpr(o), e);
+   IRExpr* cond = binop(Iop_CasCmpNE32, mkexpr(old), mkexpr(o));
+   exit(cond, Ijk_Boring, 0); /* Loop if failed */
+   putIReg(rd, extendS(Ity_I32, mkexpr(o)));
+
+   if (fence)
+      stmt(IRStmt_MBE(Imbe_Fence));
+
+   return True;
+}
+
+static Bool gen_am_d_helper ( enum amop op, Bool fence,
+                              UInt rd, UInt rj, UInt rk )
+{
+   if (fence)
+      stmt(IRStmt_MBE(Imbe_Fence));
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x7)));
+
+   IRTemp o = newTemp(Ity_I64);
+   assign(o, load(Ity_I64, mkexpr(addr)));
+   IRTemp n = newTemp(Ity_I64);
+   assign(n, getIReg64(rk));
+   IRExpr* e;
+   switch (op) {
+      case AMSWAP:
+         e = mkexpr(n);
+         break;
+      case AMADD:
+         e = binop(Iop_Add64, mkexpr(o), mkexpr(n));
+         break;
+      case AMAND:
+         e = binop(Iop_And64, mkexpr(o), mkexpr(n));
+         break;
+      case AMOR:
+         e = binop(Iop_Or64, mkexpr(o), mkexpr(n));
+         break;
+      case AMXOR:
+         e = binop(Iop_Xor64, mkexpr(o), mkexpr(n));
+         break;
+      case AMMAX: {
+         IRExpr* cond = binop(Iop_CmpLT64S, mkexpr(n), mkexpr(o));
+         e = IRExpr_ITE(cond, mkexpr(o), mkexpr(n));
+         break;
+      }
+      case AMMIN: {
+         IRExpr* cond = binop(Iop_CmpLT64S, mkexpr(o), mkexpr(n));
+         e = IRExpr_ITE(cond, mkexpr(o), mkexpr(n));
+         break;
+      }
+      case AMMAX_U: {
+         IRExpr* cond = binop(Iop_CmpLT64U, mkexpr(n), mkexpr(o));
+         e = IRExpr_ITE(cond, mkexpr(o), mkexpr(n));
+         break;
+      }
+      case AMMIN_U: {
+         IRExpr* cond = binop(Iop_CmpLT64U, mkexpr(o), mkexpr(n));
+         e = IRExpr_ITE(cond, mkexpr(o), mkexpr(n));
+         break;
+      }
+      default:
+         return False;
+   }
+
+   IRTemp old = newTemp(Ity_I64);
+   cas(old, mkexpr(addr), mkexpr(o), e);
+   IRExpr* cond = binop(Iop_CasCmpNE64, mkexpr(old), mkexpr(o));
+   exit(cond, Ijk_Boring, 0); /* Loop if failed */
+   putIReg(rd, mkexpr(o));
+
+   if (fence)
+      stmt(IRStmt_MBE(Imbe_Fence));
+
+   return True;
+}
+
+static Bool gen_amswap_w ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("amswap.w %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_w_helper(AMSWAP, False, rd, rj, rk);
+}
+
+static Bool gen_amswap_d ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("amswap.d %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_d_helper(AMSWAP, False, rd, rj, rk);
+}
+
+static Bool gen_amadd_w ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("amadd.w %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_w_helper(AMADD, False, rd, rj, rk);
+}
+
+static Bool gen_amadd_d ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("amadd.d %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_d_helper(AMADD, False, rd, rj, rk);
+}
+
+static Bool gen_amand_w ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("amand.w %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_w_helper(AMAND, False, rd, rj, rk);
+}
+
+static Bool gen_amand_d ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("amand.d %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_d_helper(AMAND, False, rd, rj, rk);
+}
+
+static Bool gen_amor_w ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("amor.w %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_w_helper(AMOR, False, rd, rj, rk);
+}
+
+static Bool gen_amor_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("amor.d %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_d_helper(AMOR, False, rd, rj, rk);
+}
+
+static Bool gen_amxor_w ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("amxor.w %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_w_helper(AMXOR, False, rd, rj, rk);
+}
+
+static Bool gen_amxor_d ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("amxor.d %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_d_helper(AMXOR, False, rd, rj, rk);
+}
+
+static Bool gen_ammax_w ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ammax.w %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_w_helper(AMMAX, False, rd, rj, rk);
+}
+
+static Bool gen_ammax_d ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ammax.d %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_d_helper(AMMAX, False, rd, rj, rk);
+}
+
+static Bool gen_ammin_w ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ammin.w %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_w_helper(AMMIN, False, rd, rj, rk);
+}
+
+static Bool gen_ammin_d ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ammin.d %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_d_helper(AMMIN, False, rd, rj, rk);
+}
+
+static Bool gen_ammax_wu ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ammax.wu %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_w_helper(AMMAX_U, False, rd, rj, rk);
+}
+
+static Bool gen_ammax_du ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ammax.du %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_d_helper(AMMAX_U, False, rd, rj, rk);
+}
+
+static Bool gen_ammin_wu ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ammin.wu %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_w_helper(AMMIN_U, False, rd, rj, rk);
+}
+
+static Bool gen_ammin_du ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ammin.du %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_d_helper(AMMIN_U, False, rd, rj, rk);
+}
+
+static Bool gen_amswap_db_w ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("amswap_db.w %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_w_helper(AMSWAP, True, rd, rj, rk);
+}
+
+static Bool gen_amswap_db_d ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("amswap_db.d %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_d_helper(AMSWAP, True, rd, rj, rk);
+}
+
+static Bool gen_amadd_db_w ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("amadd_db.w %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_w_helper(AMADD, True, rd, rj, rk);
+}
+
+static Bool gen_amadd_db_d ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("amadd_db.d %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_d_helper(AMADD, True, rd, rj, rk);
+}
+
+static Bool gen_amand_db_w ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("amand_db.w %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_w_helper(AMAND, True, rd, rj, rk);
+}
+
+static Bool gen_amand_db_d ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("amand_db.d %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_d_helper(AMAND, True, rd, rj, rk);
+}
+
+static Bool gen_amor_db_w ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("amor_db.w %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_w_helper(AMOR, True, rd, rj, rk);
+}
+
+static Bool gen_amor_db_d ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("amor_db.d %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_d_helper(AMOR, True, rd, rj, rk);
+}
+
+static Bool gen_amxor_db_w ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("amxor_db.w %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_w_helper(AMXOR, True, rd, rj, rk);
+}
+
+static Bool gen_amxor_db_d ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("amxor_db.d %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_d_helper(AMXOR, True, rd, rj, rk);
+}
+
+static Bool gen_ammax_db_w ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ammax_db.w %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_w_helper(AMMAX, True, rd, rj, rk);
+}
+
+static Bool gen_ammax_db_d ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ammax_db.d %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_d_helper(AMMAX, True, rd, rj, rk);
+}
+
+static Bool gen_ammin_db_w ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ammin_db.w %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_w_helper(AMMIN, True, rd, rj, rk);
+}
+
+static Bool gen_ammin_db_d ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ammin_db.d %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_d_helper(AMMIN, True, rd, rj, rk);
+}
+
+static Bool gen_ammax_db_wu ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ammax_db.wu %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_w_helper(AMMAX_U, True, rd, rj, rk);
+}
+
+static Bool gen_ammax_db_du ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ammax_db.du %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_d_helper(AMMAX_U, True, rd, rj, rk);
+}
+
+static Bool gen_ammin_db_wu ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ammin_db.wu %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_w_helper(AMMIN_U, True, rd, rj, rk);
+}
+
+static Bool gen_ammin_db_du ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("ammin_db.du %s, %s, %s\n", nameIReg(rd), nameIReg(rk), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LAM)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_am_d_helper(AMMIN_U, True, rd, rj, rk);
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Helpers for fixed point extra insns                  ---*/
+/*------------------------------------------------------------*/
+
+static Bool gen_crc_w_b_w ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("crc.w.b.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr** arg = mkIRExprVec_3(getIReg64(rk), getIReg64(rj), mkU64(8));
+   IRExpr* call = mkIRExprCCall(Ity_I64, 0/*regparms*/,
+                                "loongarch64_calculate_crc",
+                                &loongarch64_calculate_crc,
+                                arg);
+   putIReg(rd, call);
+
+   return True;
+}
+
+static Bool gen_crc_w_h_w ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("crc.w.h.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr** arg = mkIRExprVec_3(getIReg64(rk), getIReg64(rj), mkU64(16));
+   IRExpr* call = mkIRExprCCall(Ity_I64, 0/*regparms*/,
+                                "loongarch64_calculate_crc",
+                                &loongarch64_calculate_crc,
+                                arg);
+   putIReg(rd, call);
+
+   return True;
+}
+
+static Bool gen_crc_w_w_w ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("crc.w.w.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr** arg = mkIRExprVec_3(getIReg64(rk), getIReg64(rj), mkU64(32));
+   IRExpr* call = mkIRExprCCall(Ity_I64, 0/*regparms*/,
+                                "loongarch64_calculate_crc",
+                                &loongarch64_calculate_crc,
+                                arg);
+   putIReg(rd, call);
+
+   return True;
+}
+
+static Bool gen_crc_w_d_w ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("crc.w.d.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr** arg = mkIRExprVec_3(getIReg64(rk), getIReg64(rj), mkU64(64));
+   IRExpr* call = mkIRExprCCall(Ity_I64, 0/*regparms*/,
+                                "loongarch64_calculate_crc",
+                                &loongarch64_calculate_crc,
+                                arg);
+   putIReg(rd, call);
+
+   return True;
+}
+
+static Bool gen_crcc_w_b_w ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("crcc.w.b.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr** arg = mkIRExprVec_3(getIReg64(rk), getIReg64(rj), mkU64(8));
+   IRExpr* call = mkIRExprCCall(Ity_I64, 0/*regparms*/,
+                                "loongarch64_calculate_crcc",
+                                &loongarch64_calculate_crcc,
+                                arg);
+   putIReg(rd, call);
+
+   return True;
+}
+
+static Bool gen_crcc_w_h_w ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("crcc.w.h.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr** arg = mkIRExprVec_3(getIReg64(rk), getIReg64(rj), mkU64(16));
+   IRExpr* call = mkIRExprCCall(Ity_I64, 0/*regparms*/,
+                                "loongarch64_calculate_crcc",
+                                &loongarch64_calculate_crcc,
+                                arg);
+   putIReg(rd, call);
+
+   return True;
+}
+
+static Bool gen_crcc_w_w_w ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("crcc.w.w.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr** arg = mkIRExprVec_3(getIReg64(rk), getIReg64(rj), mkU64(32));
+   IRExpr* call = mkIRExprCCall(Ity_I64, 0/*regparms*/,
+                                "loongarch64_calculate_crcc",
+                                &loongarch64_calculate_crcc,
+                                arg);
+   putIReg(rd, call);
+
+   return True;
+}
+
+static Bool gen_crcc_w_d_w ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("crcc.w.d.w %s, %s, %s\n", nameIReg(rd), nameIReg(rj), nameIReg(rk));
+
+   IRExpr** arg = mkIRExprVec_3(getIReg64(rk), getIReg64(rj), mkU64(64));
+   IRExpr* call = mkIRExprCCall(Ity_I64, 0/*regparms*/,
+                                "loongarch64_calculate_crcc",
+                                &loongarch64_calculate_crcc,
+                                arg);
+   putIReg(rd, call);
+
+   return True;
+}
+
+static Bool gen_break ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt code = SLICE(insn, 14, 0);
+
+   DIP("break %u\n", code);
+
+   putPC(mkU64(guest_PC_curr_instr + 4));
+
+   /* On LoongArch, most instructions do not raise exceptions;
+      instead, gcc notifies the kernel with a trap instruction.
+      We simulate the behavior of the linux kernel here.
+      See arch/loongarch/kernel/traps.c.
+    */
+   switch (code) {
+      case 6: /* BRK_OVERFLOW */
+         dres->jk_StopHere = Ijk_SigFPE_IntOvf;
+         break;
+      case 7: /* BRK_DIVZERO */
+         dres->jk_StopHere = Ijk_SigFPE_IntDiv;
+         break;
+      default:
+         dres->jk_StopHere = Ijk_SigTRAP;
+         break;
+   }
+   dres->whatNext    = Dis_StopHere;
+
+   return True;
+}
+
+static Bool gen_syscall ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt hint = SLICE(insn, 14, 0);
+
+   DIP("syscall %u\n", hint);
+
+   putPC(mkU64(guest_PC_curr_instr + 4));
+
+   dres->jk_StopHere = Ijk_Sys_syscall;
+   dres->whatNext    = Dis_StopHere;
+
+   return True;
+}
+
+static Bool gen_asrtle_d ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+
+   DIP("asrtle.d %s, %s\n", nameIReg(rj), nameIReg(rk));
+
+   gen_SIGSYS(binop(Iop_CmpLT64U, getIReg64(rk), getIReg64(rj)));
+
+   return True;
+}
+
+static Bool gen_asrtgt_d ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+
+   DIP("asrtgt.d %s, %s\n", nameIReg(rj), nameIReg(rk));
+
+   gen_SIGSYS(binop(Iop_CmpLE64U, getIReg64(rj), getIReg64(rk)));
+
+   return True;
+}
+
+static Bool gen_rdtimel_w ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("rdtimel.w %s, %s\n", nameIReg(rd), nameIReg(rj));
+
+   putIReg(rd, mkU64(0));
+
+   return True;
+}
+
+static Bool gen_rdtimeh_w ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("rdtimeh.w %s, %s\n", nameIReg(rd), nameIReg(rj));
+
+   putIReg(rd, mkU64(0));
+
+   return True;
+}
+
+static Bool gen_rdtime_d ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("rdtime.d %s, %s\n", nameIReg(rd), nameIReg(rj));
+
+   putIReg(rd, mkU64(0));
+
+   return True;
+}
+
+static Bool gen_cpucfg ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("cpucfg %s, %s\n", nameIReg(rd), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_CPUCFG)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr** arg = mkIRExprVec_1(getIReg64(rj));
+   IRExpr* call = mkIRExprCCall(Ity_I64, 0/*regparms*/,
+                                "loongarch64_calculate_cpucfg",
+                                &loongarch64_calculate_cpucfg,
+                                arg);
+   putIReg(rd, call);
+
+   return True;
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Helpers for floating point arithmetic insns          ---*/
+/*------------------------------------------------------------*/
+
+static Bool gen_fadd_s ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fadd.s %s, %s, %s\n", nameFReg(fd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FADD_S, 2, fj, fk, 0);
+   IRExpr* rm = get_rounding_mode();
+   putFReg32(fd, triop(Iop_AddF32, rm, getFReg32(fj), getFReg32(fk)));
+
+   return True;
+}
+
+static Bool gen_fadd_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fadd.d %s, %s, %s\n", nameFReg(fd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FADD_D, 2, fj, fk, 0);
+   IRExpr* rm = get_rounding_mode();
+   putFReg64(fd, triop(Iop_AddF64, rm, getFReg64(fj), getFReg64(fk)));
+
+   return True;
+}
+
+static Bool gen_fsub_s ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fsub.s %s, %s, %s\n", nameFReg(fd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FSUB_S, 2, fj, fk, 0);
+   IRExpr* rm = get_rounding_mode();
+   putFReg32(fd, triop(Iop_SubF32, rm, getFReg32(fj), getFReg32(fk)));
+
+   return True;
+}
+
+static Bool gen_fsub_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fsub.d %s, %s, %s\n", nameFReg(fd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FSUB_D, 2, fj, fk, 0);
+   IRExpr* rm = get_rounding_mode();
+   putFReg64(fd, triop(Iop_SubF64, rm, getFReg64(fj), getFReg64(fk)));
+
+   return True;
+}
+
+static Bool gen_fmul_s ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fmul.s %s, %s, %s\n", nameFReg(fd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FMUL_S, 2, fj, fk, 0);
+   IRExpr* rm = get_rounding_mode();
+   putFReg32(fd, triop(Iop_MulF32, rm, getFReg32(fj), getFReg32(fk)));
+
+   return True;
+}
+
+static Bool gen_fmul_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fmul.d %s, %s, %s\n", nameFReg(fd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FMUL_D, 2, fj, fk, 0);
+   IRExpr* rm = get_rounding_mode();
+   putFReg64(fd, triop(Iop_MulF64, rm, getFReg64(fj), getFReg64(fk)));
+
+   return True;
+}
+
+static Bool gen_fdiv_s ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fdiv.s %s, %s, %s\n", nameFReg(fd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FDIV_S, 2, fj, fk, 0);
+   IRExpr* rm = get_rounding_mode();
+   putFReg32(fd, triop(Iop_DivF32, rm, getFReg32(fj), getFReg32(fk)));
+
+   return True;
+}
+
+static Bool gen_fdiv_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fdiv.d %s, %s, %s\n", nameFReg(fd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FDIV_D, 2, fj, fk, 0);
+   IRExpr* rm = get_rounding_mode();
+   putFReg64(fd, triop(Iop_DivF64, rm, getFReg64(fj), getFReg64(fk)));
+
+   return True;
+}
+
+static Bool gen_fmadd_s ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt fa = SLICE(insn, 19, 15);
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fmadd.s %s, %s, %s, %s\n", nameFReg(fd), nameFReg(fj),
+                                   nameFReg(fk), nameFReg(fa));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FMADD_S, 3, fj, fk, fa);
+   IRExpr* rm = get_rounding_mode();
+   putFReg32(fd, qop(Iop_MAddF32, rm, getFReg32(fj),
+                     getFReg32(fk), getFReg32(fa)));
+
+   return True;
+}
+
+static Bool gen_fmadd_d ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt fa = SLICE(insn, 19, 15);
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fmadd.d %s, %s, %s, %s\n", nameFReg(fd), nameFReg(fj),
+                                   nameFReg(fk), nameFReg(fa));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FMADD_D, 3, fj, fk, fa);
+   IRExpr* rm = get_rounding_mode();
+   putFReg64(fd, qop(Iop_MAddF64, rm, getFReg64(fj),
+                     getFReg64(fk), getFReg64(fa)));
+
+   return True;
+}
+
+static Bool gen_fmsub_s ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt fa = SLICE(insn, 19, 15);
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fmsub.s %s, %s, %s, %s\n", nameFReg(fd), nameFReg(fj),
+                                   nameFReg(fk), nameFReg(fa));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FMSUB_S, 3, fj, fk, fa);
+   IRExpr* rm = get_rounding_mode();
+   putFReg32(fd, qop(Iop_MSubF32, rm, getFReg32(fj),
+                     getFReg32(fk), getFReg32(fa)));
+
+   return True;
+}
+
+static Bool gen_fmsub_d ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt fa = SLICE(insn, 19, 15);
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fmsub.d %s, %s, %s, %s\n", nameFReg(fd), nameFReg(fj),
+                                   nameFReg(fk), nameFReg(fa));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FMSUB_D, 3, fj, fk, fa);
+   IRExpr* rm = get_rounding_mode();
+   putFReg64(fd, qop(Iop_MSubF64, rm, getFReg64(fj),
+                     getFReg64(fk), getFReg64(fa)));
+
+   return True;
+}
+
+static Bool gen_fnmadd_s ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt fa = SLICE(insn, 19, 15);
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fnmadd.s %s, %s, %s, %s\n", nameFReg(fd), nameFReg(fj),
+                                    nameFReg(fk), nameFReg(fa));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FNMADD_S, 3, fj, fk, fa);
+   IRExpr* rm = get_rounding_mode();
+   IRExpr* madd = qop(Iop_MAddF32, rm, getFReg32(fj),
+                      getFReg32(fk), getFReg32(fa));
+   putFReg32(fd, unop(Iop_NegF32, madd));
+
+   return True;
+}
+
+static Bool gen_fnmadd_d ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt fa = SLICE(insn, 19, 15);
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fnmadd.d %s, %s, %s, %s\n", nameFReg(fd), nameFReg(fj),
+                                    nameFReg(fk), nameFReg(fa));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FNMADD_D, 3, fj, fk, fa);
+   IRExpr* rm = get_rounding_mode();
+   IRExpr* madd = qop(Iop_MAddF64, rm, getFReg64(fj),
+                      getFReg64(fk), getFReg64(fa));
+   putFReg64(fd, unop(Iop_NegF64, madd));
+
+   return True;
+}
+
+static Bool gen_fnmsub_s ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt fa = SLICE(insn, 19, 15);
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fnmsub.s %s, %s, %s, %s\n", nameFReg(fd), nameFReg(fj),
+                                    nameFReg(fk), nameFReg(fa));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FNMSUB_S, 3, fj, fk, fa);
+   IRExpr* rm = get_rounding_mode();
+   IRExpr* msub = qop(Iop_MSubF32, rm, getFReg32(fj),
+                      getFReg32(fk), getFReg32(fa));
+   putFReg32(fd, unop(Iop_NegF32, msub));
+
+   return True;
+}
+
+static Bool gen_fnmsub_d ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt fa = SLICE(insn, 19, 15);
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fnmsub.d %s, %s, %s, %s\n", nameFReg(fd), nameFReg(fj),
+                                    nameFReg(fk), nameFReg(fa));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FNMSUB_D, 3, fj, fk, fa);
+   IRExpr* rm = get_rounding_mode();
+   IRExpr* msub = qop(Iop_MSubF64, rm, getFReg64(fj),
+                      getFReg64(fk), getFReg64(fa));
+   putFReg64(fd, unop(Iop_NegF64, msub));
+
+   return True;
+}
+
+static Bool gen_fmax_s ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fmax.s %s, %s, %s\n", nameFReg(fd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FMAX_S, 2, fj, fk, 0);
+   putFReg32(fd, binop(Iop_MaxNumF32, getFReg32(fj), getFReg32(fk)));
+
+   return True;
+}
+
+static Bool gen_fmax_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fmax.d %s, %s, %s\n", nameFReg(fd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FMAX_D, 2, fj, fk, 0);
+   putFReg64(fd, binop(Iop_MaxNumF64, getFReg64(fj), getFReg64(fk)));
+
+   return True;
+}
+
+static Bool gen_fmin_s ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fmin.s %s, %s, %s\n", nameFReg(fd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FMIN_S, 2, fj, fk, 0);
+   putFReg32(fd, binop(Iop_MinNumF32, getFReg32(fj), getFReg32(fk)));
+
+   return True;
+}
+
+static Bool gen_fmin_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fmin.d %s, %s, %s\n", nameFReg(fd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FMIN_D, 2, fj, fk, 0);
+   putFReg64(fd, binop(Iop_MinNumF64, getFReg64(fj), getFReg64(fk)));
+
+   return True;
+}
+
+static Bool gen_fmaxa_s ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fmaxa.s %s, %s, %s\n", nameFReg(fd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FMAXA_S, 2, fj, fk, 0);
+   putFReg32(fd, binop(Iop_MaxNumAbsF32, getFReg32(fj), getFReg32(fk)));
+
+   return True;
+}
+
+static Bool gen_fmaxa_d ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fmaxa.d %s, %s, %s\n", nameFReg(fd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FMAXA_D, 2, fj, fk, 0);
+   putFReg64(fd, binop(Iop_MaxNumAbsF64, getFReg64(fj), getFReg64(fk)));
+
+   return True;
+}
+
+static Bool gen_fmina_s ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fmina.s %s, %s, %s\n", nameFReg(fd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FMINA_S, 2, fj, fk, 0);
+   putFReg32(fd, binop(Iop_MinNumAbsF32, getFReg32(fj), getFReg32(fk)));
+
+   return True;
+}
+
+static Bool gen_fmina_d ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fmina.d %s, %s, %s\n", nameFReg(fd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FMINA_D, 2, fj, fk, 0);
+   putFReg64(fd, binop(Iop_MinNumAbsF64, getFReg64(fj), getFReg64(fk)));
+
+   return True;
+}
+
+static Bool gen_fabs_s ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fabs.s %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FABS_S, 1, fj, 0, 0);
+   putFReg32(fd, unop(Iop_AbsF32, getFReg32(fj)));
+
+   return True;
+}
+
+static Bool gen_fabs_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fabs.d %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FABS_D, 1, fj, 0, 0);
+   putFReg64(fd, unop(Iop_AbsF64, getFReg64(fj)));
+
+   return True;
+}
+
+static Bool gen_fneg_s ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fneg.s %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FNEG_S, 1, fj, 0, 0);
+   putFReg32(fd, unop(Iop_NegF32, getFReg32(fj)));
+
+   return True;
+}
+
+static Bool gen_fneg_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fneg.d %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FNEG_D, 1, fj, 0, 0);
+   putFReg64(fd, unop(Iop_NegF64, getFReg64(fj)));
+
+   return True;
+}
+
+static Bool gen_fsqrt_s ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fsqrt.s %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FSQRT_S, 1, fj, 0, 0);
+   IRExpr* rm = get_rounding_mode();
+   putFReg32(fd, binop(Iop_SqrtF32, rm, getFReg32(fj)));
+
+   return True;
+}
+
+static Bool gen_fsqrt_d ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fsqrt.d %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FSQRT_D, 1, fj, 0, 0);
+   IRExpr* rm = get_rounding_mode();
+   putFReg64(fd, binop(Iop_SqrtF64, rm, getFReg64(fj)));
+
+   return True;
+}
+
+static Bool gen_frecip_s ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("frecip.s %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FRECIP_S, 1, fj, 0, 0);
+   IRExpr* rm = get_rounding_mode();
+   putFReg32(fd, triop(Iop_DivF32, rm, mkF32i(1), getFReg32(fj)));
+
+   return True;
+}
+
+static Bool gen_frecip_d ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("frecip.d %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FRECIP_D, 1, fj, 0, 0);
+   IRExpr* rm = get_rounding_mode();
+   putFReg64(fd, triop(Iop_DivF64, rm, mkF64i(1), getFReg64(fj)));
+
+   return True;
+}
+
+static Bool gen_frsqrt_s ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("frsqrt.s %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FRSQRT_S, 1, fj, 0, 0);
+   IRExpr* rm = get_rounding_mode();
+   putFReg32(fd, binop(Iop_RSqrtF32, rm, getFReg32(fj)));
+
+   return True;
+}
+
+static Bool gen_frsqrt_d ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("frsqrt.d %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FRSQRT_D, 1, fj, 0, 0);
+   IRExpr* rm = get_rounding_mode();
+   putFReg64(fd, binop(Iop_RSqrtF64, rm, getFReg64(fj)));
+
+   return True;
+}
+
+static Bool gen_fscaleb_s ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fscaleb.s %s, %s, %s\n", nameFReg(fd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FSCALEB_S, 2, fj, fk, 0);
+   IRExpr* rm = get_rounding_mode();
+   putFReg32(fd, triop(Iop_ScaleBF32, rm, getFReg32(fj), getFReg32(fk)));
+
+   return True;
+}
+
+static Bool gen_fscaleb_d ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fscaleb.d %s, %s, %s\n", nameFReg(fd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FSCALEB_D, 2, fj, fk, 0);
+   IRExpr* rm = get_rounding_mode();
+   putFReg64(fd, triop(Iop_ScaleBF64, rm, getFReg64(fj), getFReg64(fk)));
+
+   return True;
+}
+
+static Bool gen_flogb_s ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("flogb.s %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FLOGB_S, 1, fj, 0, 0);
+   IRExpr* rm = get_rounding_mode();
+   putFReg32(fd, binop(Iop_LogBF32, rm, getFReg32(fj)));
+
+   return True;
+}
+
+static Bool gen_flogb_d ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("flogb.d %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FLOGB_D, 1, fj, 0, 0);
+   IRExpr* rm = get_rounding_mode();
+   putFReg64(fd, binop(Iop_LogBF64, rm, getFReg64(fj)));
+
+   return True;
+}
+
+static Bool gen_fcopysign_s ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fcopysign.s %s, %s, %s\n", nameFReg(fd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr* i1 = unop(Iop_ReinterpF32asI32, getFReg32(fj));
+   IRExpr* shl1 = binop(Iop_Shl32, i1, mkU8(1));
+   IRExpr* shr1 = binop(Iop_Shr32, shl1, mkU8(1));
+   IRExpr* i2 = unop(Iop_ReinterpF32asI32, getFReg32(fk));
+   IRExpr* shr2 = binop(Iop_Shr32, i2, mkU8(31));
+   IRExpr* shl2 = binop(Iop_Shl32, shr2, mkU8(31));
+   IRExpr* or = binop(Iop_Or32, shr1, shl2);
+   putFReg32(fd, unop(Iop_ReinterpI32asF32, or));
+
+   return True;
+}
+
+static Bool gen_fcopysign_d ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fcopysign.d %s, %s, %s\n", nameFReg(fd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr* i1 = unop(Iop_ReinterpF64asI64, getFReg64(fj));
+   IRExpr* shl1 = binop(Iop_Shl64, i1, mkU8(1));
+   IRExpr* shr1 = binop(Iop_Shr64, shl1, mkU8(1));
+   IRExpr* i2 = unop(Iop_ReinterpF64asI64, getFReg64(fk));
+   IRExpr* shr2 = binop(Iop_Shr64, i2, mkU8(63));
+   IRExpr* shl2 = binop(Iop_Shl64, shr2, mkU8(63));
+   IRExpr* or = binop(Iop_Or64, shr1, shl2);
+   putFReg64(fd, unop(Iop_ReinterpI64asF64, or));
+
+   return True;
+}
+
+static Bool gen_fclass_s ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fclass.s %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr** arg = mkIRExprVec_1(unop(Iop_ReinterpF64asI64, getFReg64(fj)));
+   IRExpr* call = mkIRExprCCall(Ity_I64, 0/*regparms*/,
+                                "loongarch64_calculate_fclass_s",
+                                &loongarch64_calculate_fclass_s,
+                                arg);
+   putFReg32(fd, unop(Iop_ReinterpI32asF32, unop(Iop_64to32, call)));
+
+   return True;
+}
+
+static Bool gen_fclass_d ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fclass.d %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr** arg = mkIRExprVec_1(unop(Iop_ReinterpF64asI64, getFReg64(fj)));
+   IRExpr* call = mkIRExprCCall(Ity_I64, 0/*regparms*/,
+                                "loongarch64_calculate_fclass_d",
+                                &loongarch64_calculate_fclass_d,
+                                arg);
+   putFReg64(fd, unop(Iop_ReinterpI64asF64, call));
+
+   return True;
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Helpers for floating point comparison insns          ---*/
+/*------------------------------------------------------------*/
+
+static inline IRExpr* is_UN ( IRExpr* e )
+{
+   return binop(Iop_CmpEQ32, e, mkU32(0x45));
+}
+
+static inline IRExpr* is_LT ( IRExpr* e )
+{
+   return binop(Iop_CmpEQ32, e, mkU32(0x1));
+}
+
+static inline IRExpr* is_GT ( IRExpr* e )
+{
+   return binop(Iop_CmpEQ32, e, mkU32(0x0));
+}
+
+static inline IRExpr* is_EQ ( IRExpr* e )
+{
+   return binop(Iop_CmpEQ32, e, mkU32(0x40));
+}
+
+static Bool gen_fcmp_cond_helper ( enum fpop op, UInt cc,
+                                   UInt fj, UInt fk, Bool size64 )
+{
+   /* We have to convert 'irRes' from an IR-convention return result
+      (IRCmpF32Result / IRCmpF64Result) to a LOONGARCH-encoded group.
+
+      FP cmp result | IR
+      --------------------
+      UN            | 0x45
+      LT            | 0x01
+      GT            | 0x00
+      EQ            | 0x40
+    */
+   IRTemp result = newTemp(Ity_I32);
+   if (size64)
+      assign(result, binop(Iop_CmpF64, getFReg64(fj), getFReg64(fk)));
+   else
+      assign(result, binop(Iop_CmpF32, getFReg32(fj), getFReg32(fk)));
+
+   IRExpr* e;
+   switch (op) {
+      case FCMP_CAF_S: case FCMP_CAF_D: case FCMP_SAF_S: case FCMP_SAF_D:
+         e = mkU1(False);
+         break;
+      case FCMP_CLT_S: case FCMP_CLT_D: case FCMP_SLT_S: case FCMP_SLT_D:
+         e = is_LT(mkexpr(result));
+         break;
+      case FCMP_CEQ_S: case FCMP_CEQ_D: case FCMP_SEQ_S: case FCMP_SEQ_D:
+         e = is_EQ(mkexpr(result));
+         break;
+      case FCMP_CLE_S: case FCMP_CLE_D: case FCMP_SLE_S: case FCMP_SLE_D:
+         e = binop(Iop_Or1, is_LT(mkexpr(result)), is_EQ(mkexpr(result)));
+         break;
+      case FCMP_CUN_S: case FCMP_CUN_D: case FCMP_SUN_S: case FCMP_SUN_D:
+         e = is_UN(mkexpr(result));
+         break;
+      case FCMP_CULT_S: case FCMP_CULT_D: case FCMP_SULT_S: case FCMP_SULT_D:
+         e = binop(Iop_Or1, is_UN(mkexpr(result)), is_LT(mkexpr(result)));
+         break;
+      case FCMP_CUEQ_S: case FCMP_CUEQ_D: case FCMP_SUEQ_S: case FCMP_SUEQ_D:
+         e = binop(Iop_Or1, is_UN(mkexpr(result)), is_EQ(mkexpr(result)));
+         break;
+      case FCMP_CULE_S: case FCMP_CULE_D: case FCMP_SULE_S: case FCMP_SULE_D:
+         e = binop(Iop_Or1, is_UN(mkexpr(result)),
+                            binop(Iop_Or1, is_LT(mkexpr(result)),
+                                           is_EQ(mkexpr(result))));
+         break;
+      case FCMP_CNE_S: case FCMP_CNE_D: case FCMP_SNE_S: case FCMP_SNE_D:
+         e = binop(Iop_Or1, is_GT(mkexpr(result)), is_LT(mkexpr(result)));
+         break;
+      case FCMP_COR_S: case FCMP_COR_D: case FCMP_SOR_S: case FCMP_SOR_D:
+         e = binop(Iop_Or1, is_GT(mkexpr(result)),
+                            binop(Iop_Or1, is_LT(mkexpr(result)),
+                                           is_EQ(mkexpr(result))));
+         break;
+      case FCMP_CUNE_S: case FCMP_CUNE_D: case FCMP_SUNE_S: case FCMP_SUNE_D:
+         e = binop(Iop_Or1, is_UN(mkexpr(result)),
+                            binop(Iop_Or1, is_GT(mkexpr(result)),
+                                           is_LT(mkexpr(result))));
+         break;
+      default:
+         return False;
+   }
+
+   calculateFCSR(op, 2, fj, fk, 0);
+   putFCC(cc, unop(Iop_1Uto8, e));
+
+   return True;
+}
+
+static Bool gen_fcmp_caf_s ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.caf.s %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_CAF_S, cd, fj, fk, False);
+}
+
+static Bool gen_fcmp_caf_d ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.caf.d %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_CAF_D, cd, fj, fk, True);
+}
+
+static Bool gen_fcmp_saf_s ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.saf.s %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_SAF_S, cd, fj, fk, False);
+}
+
+static Bool gen_fcmp_saf_d ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.saf.d %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_SAF_D, cd, fj, fk, True);
+}
+
+static Bool gen_fcmp_clt_s ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.clt.s %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_CLT_S, cd, fj, fk, False);
+}
+
+static Bool gen_fcmp_clt_d ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.clt.d %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_CLT_D, cd, fj, fk, True);
+}
+
+static Bool gen_fcmp_slt_s ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.slt.s %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_SLT_S, cd, fj, fk, False);
+}
+
+static Bool gen_fcmp_slt_d ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.slt.d %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_SLT_D, cd, fj, fk, True);
+}
+
+static Bool gen_fcmp_ceq_s ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.ceq.s %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_CEQ_S, cd, fj, fk, False);
+}
+
+static Bool gen_fcmp_ceq_d ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.ceq.d %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_CEQ_D, cd, fj, fk, True);
+}
+
+static Bool gen_fcmp_seq_s ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.seq.s %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_SEQ_S, cd, fj, fk, False);
+}
+
+static Bool gen_fcmp_seq_d ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.seq.d %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_SEQ_D, cd, fj, fk, True);
+}
+
+static Bool gen_fcmp_cle_s ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.cle.s %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_CLE_S, cd, fj, fk, False);
+}
+
+static Bool gen_fcmp_cle_d ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.cle.d %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_CLE_D, cd, fj, fk, True);
+}
+
+static Bool gen_fcmp_sle_s ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.sle.s %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_SLE_S, cd, fj, fk, False);
+}
+
+static Bool gen_fcmp_sle_d ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.sle.d %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_SLE_D, cd, fj, fk, True);
+}
+
+static Bool gen_fcmp_cun_s ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.cun.s %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_CUN_S, cd, fj, fk, False);
+}
+
+static Bool gen_fcmp_cun_d ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.cun.d %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_CUN_D, cd, fj, fk, True);
+}
+
+static Bool gen_fcmp_sun_s ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.sun.s %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_SUN_S, cd, fj, fk, False);
+}
+
+static Bool gen_fcmp_sun_d ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.sun.d %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_SUN_D, cd, fj, fk, True);
+}
+
+static Bool gen_fcmp_cult_s ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.cult.s %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_CULT_S, cd, fj, fk, False);
+}
+
+static Bool gen_fcmp_cult_d ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.cult.d %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_CULT_D, cd, fj, fk, True);
+}
+
+static Bool gen_fcmp_sult_s ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.sult.s %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_SULT_S, cd, fj, fk, False);
+}
+
+static Bool gen_fcmp_sult_d ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.sult.d %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_SULT_D, cd, fj, fk, True);
+}
+
+static Bool gen_fcmp_cueq_s ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.cueq.s %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_CUEQ_S, cd, fj, fk, False);
+}
+
+static Bool gen_fcmp_cueq_d ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.cueq.d %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_CUEQ_D, cd, fj, fk, True);
+}
+
+static Bool gen_fcmp_sueq_s ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.sueq.s %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_SUEQ_S, cd, fj, fk, False);
+}
+
+static Bool gen_fcmp_sueq_d ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.sueq.d %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_SUEQ_D, cd, fj, fk, True);
+}
+
+static Bool gen_fcmp_cule_s ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.cule.s %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_CULE_S, cd, fj, fk, False);
+}
+
+static Bool gen_fcmp_cule_d ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.cule.d %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_CULE_D, cd, fj, fk, True);
+}
+
+static Bool gen_fcmp_sule_s ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.sule.s %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_SULE_S, cd, fj, fk, False);
+}
+
+static Bool gen_fcmp_sule_d ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.sule.d %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_SULE_D, cd, fj, fk, True);
+}
+
+static Bool gen_fcmp_cne_s ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.cne.s %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_CNE_S, cd, fj, fk, False);
+}
+
+static Bool gen_fcmp_cne_d ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.cne.d %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_CNE_D, cd, fj, fk, True);
+}
+
+static Bool gen_fcmp_sne_s ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.sne.s %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_SNE_S, cd, fj, fk, False);
+}
+
+static Bool gen_fcmp_sne_d ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.sne.d %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_SNE_D, cd, fj, fk, True);
+}
+
+static Bool gen_fcmp_cor_s ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.cor.s %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_COR_S, cd, fj, fk, False);
+}
+
+static Bool gen_fcmp_cor_d ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.cor.d %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_COR_D, cd, fj, fk, True);
+}
+
+static Bool gen_fcmp_sor_s ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.sor.s %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_SOR_S, cd, fj, fk, False);
+}
+
+static Bool gen_fcmp_sor_d ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.sor.d %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_SOR_D, cd, fj, fk, True);
+}
+
+static Bool gen_fcmp_cune_s ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.cune.s %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_CUNE_S, cd, fj, fk, False);
+}
+
+static Bool gen_fcmp_cune_d ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.cune.d %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_CUNE_D, cd, fj, fk, True);
+}
+
+static Bool gen_fcmp_sune_s ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.sune.s %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_SUNE_S, cd, fj, fk, False);
+}
+
+static Bool gen_fcmp_sune_d ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("fcmp.sune.d %s, %s, %s\n", nameFCC(cd), nameFReg(fj), nameFReg(fk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_fcmp_cond_helper(FCMP_SUNE_D, cd, fj, fk, True);
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Helpers for floating point conversion insns          ---*/
+/*------------------------------------------------------------*/
+
+static IRExpr* is_Invalid_Overflow ( void )
+{
+   /* Bits 16 to 20 in FCSR are flags.
+      Bit 18 - overflow
+      Bit 20 - invalid
+    */
+   IRExpr* fcsr = getFCSR(0);
+   IRExpr* shr = binop(Iop_Shr32, fcsr, mkU8(16));
+   IRExpr* and = binop(Iop_And32, shr, mkU32(0x14));
+   return binop(Iop_CmpNE32, and, getIReg32(0));
+}
+
+static Bool gen_convert_s_helper ( enum fpop op, UInt fd, UInt fj )
+{
+   IRExpr* e;
+   IRExpr* rm;
+   switch (op) {
+      case FTINTRM_W_S:
+         rm = gen_round_down();
+         e = binop(Iop_F32toI32S, rm, getFReg32(fj));
+         break;
+      case FTINTRM_W_D:
+         rm = gen_round_down();
+         e = binop(Iop_F64toI32S, rm, getFReg64(fj));
+         break;
+      case FTINTRP_W_S:
+         rm = gen_round_up();
+         e = binop(Iop_F32toI32S, rm, getFReg32(fj));
+         break;
+      case FTINTRP_W_D:
+         rm = gen_round_up();
+         e = binop(Iop_F64toI32S, rm, getFReg64(fj));
+         break;
+      case FTINTRZ_W_S:
+         rm = gen_round_to_zero();
+         e = binop(Iop_F32toI32S, rm, getFReg32(fj));
+         break;
+      case FTINTRZ_W_D:
+         rm = gen_round_to_zero();
+         e = binop(Iop_F64toI32S, rm, getFReg64(fj));
+         break;
+      case FTINTRNE_W_S:
+         rm = gen_round_to_nearest();
+         e = binop(Iop_F32toI32S, rm, getFReg32(fj));
+         break;
+      case FTINTRNE_W_D:
+         rm = gen_round_to_nearest();
+         e = binop(Iop_F64toI32S, rm, getFReg64(fj));
+         break;
+      case FTINT_W_S:
+         rm = get_rounding_mode();
+         e = binop(Iop_F32toI32S, rm, getFReg32(fj));
+         break;
+      case FTINT_W_D:
+         rm = get_rounding_mode();
+         e = binop(Iop_F64toI32S, rm, getFReg64(fj));
+         break;
+      default:
+         return False;
+   }
+
+   calculateFCSR(op, 1, fj, 0, 0);
+   IRExpr* ite = IRExpr_ITE(is_Invalid_Overflow(), mkU32(0x7fffffff), e);
+   putFReg32(fd, unop(Iop_ReinterpI32asF32, ite));
+
+   return True;
+}
+
+static Bool gen_convert_d_helper ( enum fpop op, UInt fd, UInt fj )
+{
+   IRExpr* e;
+   IRExpr* rm;
+   switch (op) {
+      case FTINTRM_L_S:
+         rm = gen_round_down();
+         e = binop(Iop_F32toI64S, rm, getFReg32(fj));
+         break;
+      case FTINTRM_L_D:
+         rm = gen_round_down();
+         e = binop(Iop_F64toI64S, rm, getFReg64(fj));
+         break;
+      case FTINTRP_L_S:
+         rm = gen_round_up();
+         e = binop(Iop_F32toI64S, rm, getFReg32(fj));
+         break;
+      case FTINTRP_L_D:
+         rm = gen_round_up();
+         e = binop(Iop_F64toI64S, rm, getFReg64(fj));
+         break;
+      case FTINTRZ_L_S:
+         rm = gen_round_to_zero();
+         e = binop(Iop_F32toI64S, rm, getFReg32(fj));
+         break;
+      case FTINTRZ_L_D:
+         rm = gen_round_to_zero();
+         e = binop(Iop_F64toI64S, rm, getFReg64(fj));
+         break;
+      case FTINTRNE_L_S:
+         rm = gen_round_to_nearest();
+         e = binop(Iop_F32toI64S, rm, getFReg32(fj));
+         break;
+      case FTINTRNE_L_D:
+         rm = gen_round_to_nearest();
+         e = binop(Iop_F64toI64S, rm, getFReg64(fj));
+         break;
+      case FTINT_L_S:
+         rm = get_rounding_mode();
+         e = binop(Iop_F32toI64S, rm, getFReg32(fj));
+         break;
+      case FTINT_L_D:
+         rm = get_rounding_mode();
+         e = binop(Iop_F64toI64S, rm, getFReg64(fj));
+         break;
+      default:
+         return False;
+   }
+
+   calculateFCSR(op, 1, fj, 0, 0);
+   IRExpr* ite = IRExpr_ITE(is_Invalid_Overflow(),
+                            mkU64(0x7fffffffffffffffULL), e);
+   putFReg64(fd, unop(Iop_ReinterpI64asF64, ite));
+
+   return True;
+}
+
+static Bool gen_fcvt_s_d ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fcvt.s.d %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FCVT_S_D, 1, fj, 0, 0);
+   IRExpr* rm = get_rounding_mode();
+   putFReg32(fd, binop(Iop_F64toF32, rm, getFReg64(fj)));
+
+   return True;
+}
+
+static Bool gen_fcvt_d_s ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fcvt.d.s %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FCVT_D_S, 1, fj, 0, 0);
+   putFReg64(fd, unop(Iop_F32toF64, getFReg32(fj)));
+
+   return True;
+}
+
+static Bool gen_ftintrm_w_s ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("ftintrm.w.s %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_convert_s_helper(FTINTRM_W_S, fd, fj);
+}
+
+static Bool gen_ftintrm_w_d ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("ftintrm.w.d %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_convert_s_helper(FTINTRM_W_D, fd, fj);
+}
+
+static Bool gen_ftintrm_l_s ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("ftintrm.l.s %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_convert_d_helper(FTINTRM_L_S, fd, fj);
+}
+
+static Bool gen_ftintrm_l_d ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("ftintrm.l.d %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_convert_d_helper(FTINTRM_L_D, fd, fj);
+}
+
+static Bool gen_ftintrp_w_s ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("ftintrp.w.s %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_convert_s_helper(FTINTRP_W_S, fd, fj);
+}
+
+static Bool gen_ftintrp_w_d ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("ftintrp.w.d %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_convert_s_helper(FTINTRP_W_D, fd, fj);
+}
+
+static Bool gen_ftintrp_l_s ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("ftintrp.l.s %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_convert_d_helper(FTINTRP_L_S, fd, fj);
+}
+
+static Bool gen_ftintrp_l_d ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("ftintrp.l.d %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_convert_d_helper(FTINTRP_L_D, fd, fj);
+}
+
+static Bool gen_ftintrz_w_s ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("ftintrz.w.s %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_convert_s_helper(FTINTRZ_W_S, fd, fj);
+}
+
+static Bool gen_ftintrz_w_d ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("ftintrz.w.d %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_convert_s_helper(FTINTRZ_W_D, fd, fj);
+}
+
+static Bool gen_ftintrz_l_s ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("ftintrz.l.s %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_convert_d_helper(FTINTRZ_L_S, fd, fj);
+}
+
+static Bool gen_ftintrz_l_d ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("ftintrz.l.d %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_convert_d_helper(FTINTRZ_L_D, fd, fj);
+}
+
+static Bool gen_ftintrne_w_s ( DisResult* dres, UInt insn,
+                               const VexArchInfo* archinfo,
+                               const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("ftintrne.w.s %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_convert_s_helper(FTINTRNE_W_S, fd, fj);
+}
+
+static Bool gen_ftintrne_w_d ( DisResult* dres, UInt insn,
+                               const VexArchInfo* archinfo,
+                               const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("ftintrne.w.d %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_convert_s_helper(FTINTRNE_W_D, fd, fj);
+}
+
+static Bool gen_ftintrne_l_s ( DisResult* dres, UInt insn,
+                               const VexArchInfo* archinfo,
+                               const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("ftintrne.l.s %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_convert_d_helper(FTINTRNE_L_S, fd, fj);
+}
+
+static Bool gen_ftintrne_l_d ( DisResult* dres, UInt insn,
+                               const VexArchInfo* archinfo,
+                               const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("ftintrne.l.d %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_convert_d_helper(FTINTRNE_L_D, fd, fj);
+}
+
+static Bool gen_ftint_w_s ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("ftint.w.s %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_convert_s_helper(FTINT_W_S, fd, fj);
+}
+
+static Bool gen_ftint_w_d ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("ftint.w.d %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_convert_s_helper(FTINT_W_D, fd, fj);
+}
+
+static Bool gen_ftint_l_s ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("ftint.l.s %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_convert_d_helper(FTINT_L_S, fd, fj);
+}
+
+static Bool gen_ftint_l_d ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("ftint.l.d %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   return gen_convert_d_helper(FTINT_L_D, fd, fj);
+}
+
+static Bool gen_ffint_s_w ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("ffint.s.w %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FFINT_S_W, 1, fj, 0, 0);
+   IRExpr* rm = get_rounding_mode();
+   IRExpr* f = unop(Iop_ReinterpF32asI32, getFReg32(fj));
+   putFReg32(fd, binop(Iop_I32StoF32, rm, f));
+
+   return True;
+}
+
+static Bool gen_ffint_s_l ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("ffint.s.l %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FFINT_S_L, 1, fj, 0, 0);
+   IRExpr* rm = get_rounding_mode();
+   IRExpr* f = unop(Iop_ReinterpF64asI64, getFReg64(fj));
+   putFReg32(fd, binop(Iop_I64StoF32, rm, f));
+
+   return True;
+}
+
+static Bool gen_ffint_d_w ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("ffint.d.w %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FFINT_D_W, 1, fj, 0, 0);
+   IRExpr* f = unop(Iop_ReinterpF32asI32, getFReg32(fj));
+   putFReg64(fd, unop(Iop_I32StoF64, f));
+
+   return True;
+}
+
+static Bool gen_ffint_d_l ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("ffint.d.l %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FFINT_D_L, 1, fj, 0, 0);
+   IRExpr* rm = get_rounding_mode();
+   IRExpr* f = unop(Iop_ReinterpF64asI64, getFReg64(fj));
+   putFReg64(fd, binop(Iop_I64StoF64, rm, f));
+
+   return True;
+}
+
+static Bool gen_frint_s ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("frint.s %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FRINT_S, 1, fj, 0, 0);
+   IRExpr* rm = get_rounding_mode();
+   putFReg32(fd, binop(Iop_RoundF32toInt, rm, getFReg32(fj)));
+
+   return True;
+}
+
+static Bool gen_frint_d ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("frint.d %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   calculateFCSR(FRINT_D, 1, fj, 0, 0);
+   IRExpr* rm = get_rounding_mode();
+   putFReg64(fd, binop(Iop_RoundF64toInt, rm, getFReg64(fj)));
+
+   return True;
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Helpers for floating point move insns                ---*/
+/*------------------------------------------------------------*/
+
+static Bool gen_fmov_s ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fmov.s %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putFReg32(fd, getFReg32(fj));
+
+   return True;
+}
+
+static Bool gen_fmov_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fmov.d %s, %s\n", nameFReg(fd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putFReg64(fd, getFReg64(fj));
+
+   return True;
+}
+
+static Bool gen_fsel ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt ca = SLICE(insn, 17, 15);
+   UInt fk = SLICE(insn, 14, 10);
+   UInt fj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fsel %s, %s, %s, %s\n", nameFReg(fd), nameFReg(fj),
+                                nameFReg(fk), nameFCC(ca));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr* cc = unop(Iop_8Uto64, getFCC(ca));
+   IRExpr* cond = binop(Iop_CmpEQ64, cc, mkU64(0));
+   putFReg64(fd, IRExpr_ITE(cond, getFReg64(fj), getFReg64(fk)));
+
+   return True;
+}
+
+static Bool gen_movgr2fr_w ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("movgr2fr.w %s, %s\n", nameFReg(fd), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   /* The high bits might be undefined, now the hardware implementation
+      of this instruction is that it is equivalent to movgr2fr.d. */
+   putFReg64(fd, unop(Iop_ReinterpI64asF64, getIReg64(rj)));
+
+   return True;
+}
+
+static Bool gen_movgr2fr_d ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("movgr2fr.d %s, %s\n", nameFReg(fd), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putFReg64(fd, unop(Iop_ReinterpI64asF64, getIReg64(rj)));
+
+   return True;
+}
+
+static Bool gen_movgr2frh_w ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("movgr2frh.w %s, %s\n", nameFReg(fd), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr* shl1 = binop(Iop_Shl64, getIReg64(rj), mkU8(32));
+   IRExpr* i = unop(Iop_ReinterpF64asI64, getFReg64(fd));
+   IRExpr* shl2 = binop(Iop_Shl64, i, mkU8(32));
+   IRExpr* shr = binop(Iop_Shr64, shl2, mkU8(32));
+   IRExpr* or = binop(Iop_Or64, shl1, shr);
+   putFReg64(fd, unop(Iop_ReinterpI64asF64, or));
+
+   return True;
+}
+
+static Bool gen_movfr2gr_s ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("movfr2gr.s %s, %s\n", nameIReg(rd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr* i = unop(Iop_ReinterpF32asI32, getFReg32(fj));
+   putIReg(rd, extendS(Ity_I32, i));
+
+   return True;
+}
+
+static Bool gen_movfr2gr_d ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("movfr2gr.d %s, %s\n", nameIReg(rd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putIReg(rd, unop(Iop_ReinterpF64asI64, getFReg64(fj)));
+
+   return True;
+}
+
+static Bool gen_movfrh2gr_s ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("movfrh2gr.s %s, %s\n", nameIReg(rd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr* i = unop(Iop_ReinterpF64asI64, getFReg64(fj));
+   IRExpr* shr = binop(Iop_Shr64, i, mkU8(32));
+   putIReg(rd, extendS(Ity_I32, unop(Iop_64to32, shr)));
+
+   return True;
+}
+
+static Bool gen_movgr2fcsr ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt fcsr = SLICE(insn, 4, 0);
+
+   DIP("movgr2fcsr %s, %s\n", nameFCSR(fcsr), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putFCSR(fcsr, getIReg32(rj));
+
+   return True;
+}
+
+static Bool gen_movfcsr2gr ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt fcsr = SLICE(insn, 9, 5);
+   UInt   rd = SLICE(insn, 4, 0);
+
+   DIP("movfcsr2gr %s, %s\n", nameIReg(rd), nameFCSR(fcsr));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putIReg(rd, extendS(Ity_I32, getFCSR(fcsr)));
+
+   return True;
+}
+
+static Bool gen_movfr2cf ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt fj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("movfr2cf %s, %s\n", nameFCC(cd), nameFReg(fj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr* i = unop(Iop_ReinterpF64asI64, getFReg64(fj));
+   IRExpr* and = binop(Iop_And64, i, mkU64(0x1));
+   putFCC(cd, unop(Iop_64to8, and));
+
+   return True;
+}
+
+static Bool gen_movcf2fr ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt cj = SLICE(insn, 7, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("movcf2fr %s, %s\n", nameFReg(fd), nameFCC(cj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   /* The hardware implementation of this instruction
+      does clear the high bits. */
+   IRExpr* cc = unop(Iop_8Uto64, getFCC(cj));
+   putFReg64(fd, unop(Iop_ReinterpI64asF64, cc));
+
+   return True;
+}
+
+static Bool gen_movgr2cf ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt rj = SLICE(insn, 9, 5);
+   UInt cd = SLICE(insn, 2, 0);
+
+   DIP("movgr2cf %s, %s\n", nameFCC(cd), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr* and = binop(Iop_And64, getIReg64(rj), mkU64(0x1));
+   putFCC(cd, unop(Iop_64to8, and));
+
+   return True;
+}
+
+static Bool gen_movcf2gr ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo* abiinfo )
+{
+   UInt cj = SLICE(insn, 7, 5);
+   UInt rd = SLICE(insn, 4, 0);
+
+   DIP("movcf2gr %s, %s\n", nameIReg(rd), nameFCC(cj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   /* The hardware implementation of this instruction
+      does clear the high bits. */
+   putIReg(rd, unop(Iop_8Uto64, getFCC(cj)));
+
+   return True;
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Helpers for floating point load/store insns          ---*/
+/*------------------------------------------------------------*/
+
+static Bool gen_fld_s ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   fd = SLICE(insn, 4, 0);
+
+   DIP("fld.s %s, %s, %d\n", nameFReg(fd), nameIReg(rj),
+                             (Int)extend32(si12, 12));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), mkU64(extend64(si12, 12)));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x3)));
+   putFReg32(fd, load(Ity_F32, addr));
+
+   return True;
+}
+
+static Bool gen_fst_s ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   fd = SLICE(insn, 4, 0);
+
+   DIP("fst.s %s, %s, %d\n", nameFReg(fd), nameIReg(rj),
+                             (Int)extend32(si12, 12));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), mkU64(extend64(si12, 12)));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x3)));
+   store(addr, getFReg32(fd));
+
+   return True;
+}
+
+static Bool gen_fld_d ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   fd = SLICE(insn, 4, 0);
+
+   DIP("fld.d %s, %s, %d\n", nameFReg(fd), nameIReg(rj),
+                             (Int)extend32(si12, 12));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), mkU64(extend64(si12, 12)));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x7)));
+   putFReg64(fd, load(Ity_F64, addr));
+
+   return True;
+}
+
+static Bool gen_fst_d ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   fd = SLICE(insn, 4, 0);
+
+   DIP("fst.d %s, %s, %d\n", nameFReg(fd), nameIReg(rj),
+                             (Int)extend32(si12, 12));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), mkU64(extend64(si12, 12)));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x7)));
+   store(addr, getFReg64(fd));
+
+   return True;
+}
+
+static Bool gen_fldx_s ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fldx.s %s, %s, %s\n", nameFReg(fd), nameIReg(rj), nameIReg(rk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), getIReg64(rk));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x3)));
+   putFReg32(fd, load(Ity_F32, addr));
+
+   return True;
+}
+
+static Bool gen_fldx_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fldx.d %s, %s, %s\n", nameFReg(fd), nameIReg(rj), nameIReg(rk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), getIReg64(rk));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x7)));
+   putFReg64(fd, load(Ity_F64, addr));
+
+   return True;
+}
+
+static Bool gen_fstx_s ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fstx.s %s, %s, %s\n", nameFReg(fd), nameIReg(rj), nameIReg(rk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), getIReg64(rk));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x3)));
+   store(addr, getFReg32(fd));
+
+   return True;
+}
+
+static Bool gen_fstx_d ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fstx.d %s, %s, %s\n", nameFReg(fd), nameIReg(rj), nameIReg(rk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), getIReg64(rk));
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_UAL))
+      gen_SIGBUS(check_align(addr, mkU64(0x7)));
+   store(addr, getFReg64(fd));
+
+   return True;
+}
+
+static Bool gen_fldgt_s ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fldgt.s %s, %s, %s\n", nameFReg(fd), nameIReg(rj), nameIReg(rk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x3)));
+   gen_SIGSYS(binop(Iop_CmpLE64U, mkexpr(addr), getIReg64(rk)));
+   putFReg32(fd, load(Ity_F32, mkexpr(addr)));
+
+   return True;
+}
+
+static Bool gen_fldgt_d ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fldgt.d %s, %s, %s\n", nameFReg(fd), nameIReg(rj), nameIReg(rk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x7)));
+   gen_SIGSYS(binop(Iop_CmpLE64U, mkexpr(addr), getIReg64(rk)));
+   putFReg64(fd, load(Ity_F64, mkexpr(addr)));
+
+   return True;
+}
+
+static Bool gen_fldle_s ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fldle.s %s, %s, %s\n", nameFReg(fd), nameIReg(rj), nameIReg(rk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x3)));
+   gen_SIGSYS(binop(Iop_CmpLT64U, getIReg64(rk), mkexpr(addr)));
+   putFReg32(fd, load(Ity_F32, mkexpr(addr)));
+
+   return True;
+}
+
+static Bool gen_fldle_d ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fldle.d %s, %s, %s\n", nameFReg(fd), nameIReg(rj), nameIReg(rk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x7)));
+   gen_SIGSYS(binop(Iop_CmpLT64U, getIReg64(rk), mkexpr(addr)));
+   putFReg64(fd, load(Ity_F64, mkexpr(addr)));
+
+   return True;
+}
+
+static Bool gen_fstgt_s ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fstgt.s %s, %s, %s\n", nameFReg(fd), nameIReg(rj), nameIReg(rk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x3)));
+   gen_SIGSYS(binop(Iop_CmpLE64U, mkexpr(addr), getIReg64(rk)));
+   store(mkexpr(addr), getFReg32(fd));
+
+   return True;
+}
+
+static Bool gen_fstgt_d ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fstgt.d %s, %s, %s\n", nameFReg(fd), nameIReg(rj), nameIReg(rk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x7)));
+   gen_SIGSYS(binop(Iop_CmpLE64U, mkexpr(addr), getIReg64(rk)));
+   store(mkexpr(addr), getFReg64(fd));
+
+   return True;
+}
+
+static Bool gen_fstle_s ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fstle.s %s, %s, %s\n", nameFReg(fd), nameIReg(rj), nameIReg(rk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x3)));
+   gen_SIGSYS(binop(Iop_CmpLT64U, getIReg64(rk), mkexpr(addr)));
+   store(mkexpr(addr), getFReg32(fd));
+
+   return True;
+}
+
+static Bool gen_fstle_d ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo* abiinfo )
+{
+   UInt rk = SLICE(insn, 14, 10);
+   UInt rj = SLICE(insn, 9, 5);
+   UInt fd = SLICE(insn, 4, 0);
+
+   DIP("fstle.d %s, %s, %s\n", nameFReg(fd), nameIReg(rj), nameIReg(rk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRTemp addr = newTemp(Ity_I64);
+   assign(addr, getIReg64(rj));
+   gen_SIGBUS(check_align(mkexpr(addr), mkU64(0x7)));
+   gen_SIGSYS(binop(Iop_CmpLT64U, getIReg64(rk), mkexpr(addr)));
+   store(mkexpr(addr), getFReg64(fd));
+
+   return True;
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Helpers for branch insns                             ---*/
+/*------------------------------------------------------------*/
+
+static Bool gen_beqz ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt offs21 = (SLICE(insn, 4, 0) << 16) | SLICE(insn, 25, 10);
+   UInt     rj = SLICE(insn, 9, 5);
+
+   DIP("beqz %s, %d\n", nameIReg(rj), (Int)extend32(offs21, 21));
+
+   IRExpr* cond = binop(Iop_CmpEQ64, getIReg64(rj), mkU64(0));
+   exit(cond, Ijk_Boring, extend64(offs21 << 2, 23));
+
+   return True;
+}
+
+static Bool gen_bnez ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt offs21 = (SLICE(insn, 4, 0) << 16) | SLICE(insn, 25, 10);
+   UInt     rj = SLICE(insn, 9, 5);
+
+   DIP("bnez %s, %d\n", nameIReg(rj), (Int)extend32(offs21, 21));
+
+   IRExpr* cond = binop(Iop_CmpNE64, getIReg64(rj), mkU64(0));
+   exit(cond, Ijk_Boring, extend64(offs21 << 2, 23));
+
+   return True;
+}
+
+static Bool gen_bceqz ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt offs21 = (SLICE(insn, 4, 0) << 16) | SLICE(insn, 25, 10);
+   UInt cj     = SLICE(insn, 7, 5);
+
+   DIP("bceqz %s, %d\n", nameFCC(cj), (Int)extend32(offs21, 21));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr* cc = unop(Iop_8Uto64, getFCC(cj));
+   IRExpr* cond = binop(Iop_CmpEQ64, cc, mkU64(0));
+   exit(cond, Ijk_Boring, extend64(offs21 << 2, 23));
+
+   return True;
+}
+
+static Bool gen_bcnez ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo* abiinfo )
+{
+   UInt offs21 = (SLICE(insn, 4, 0) << 16) | SLICE(insn, 25, 10);
+   UInt cj     = SLICE(insn, 7, 5);
+
+   DIP("bcnez %s, %d\n", nameFCC(cj), (Int)extend32(offs21, 21));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_FP)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr* cc = unop(Iop_8Uto64, getFCC(cj));
+   IRExpr* cond = binop(Iop_CmpNE64, cc, mkU64(0));
+   exit(cond, Ijk_Boring, extend64(offs21 << 2, 23));
+
+   return True;
+}
+
+static Bool gen_jirl ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt offs16 = SLICE(insn, 25, 10);
+   UInt     rj = SLICE(insn, 9, 5);
+   UInt     rd = SLICE(insn, 4, 0);
+
+   DIP("jirl %s, %s, %d\n", nameIReg(rd), nameIReg(rj),
+                            (Int)extend32(offs16, 16));
+
+   IRTemp tmp = newTemp(Ity_I64);
+   assign(tmp, getIReg64(rj)); /* This is necessary when rd == rj */
+   putIReg(rd, mkU64(guest_PC_curr_instr + 4));
+   IRExpr* imm = mkU64(extend64(offs16 << 2, 18));
+   putPC(binop(Iop_Add64, mkexpr(tmp), imm));
+
+   dres->whatNext = Dis_StopHere;
+   dres->jk_StopHere = Ijk_Boring;
+
+   return True;
+}
+
+static Bool gen_b ( DisResult* dres, UInt insn,
+                    const VexArchInfo* archinfo,
+                    const VexAbiInfo* abiinfo )
+{
+   UInt offs26 = (SLICE(insn, 9, 0) << 16) | SLICE(insn, 25, 10);
+
+   DIP("b %d\n", (Int)extend32(offs26, 26));
+
+   putPC(mkU64(guest_PC_curr_instr + extend64(offs26 << 2, 28)));
+
+   dres->whatNext = Dis_StopHere;
+   dres->jk_StopHere = Ijk_Boring;
+
+   return True;
+}
+
+static Bool gen_bl ( DisResult* dres, UInt insn,
+                     const VexArchInfo* archinfo,
+                     const VexAbiInfo* abiinfo )
+{
+   UInt offs26 = (SLICE(insn, 9, 0) << 16) | SLICE(insn, 25, 10);
+
+   DIP("bl %d\n", (Int)extend32(offs26, 26));
+
+   putIReg(1, mkU64(guest_PC_curr_instr + 4));
+   putPC(mkU64(guest_PC_curr_instr + extend64(offs26 << 2, 28)));
+
+   dres->whatNext = Dis_StopHere;
+   dres->jk_StopHere = Ijk_Boring;
+
+   return True;
+}
+
+static Bool gen_beq ( DisResult* dres, UInt insn,
+                      const VexArchInfo* archinfo,
+                      const VexAbiInfo* abiinfo )
+{
+   UInt offs16 = SLICE(insn, 25, 10);
+   UInt     rj = SLICE(insn, 9, 5);
+   UInt     rd = SLICE(insn, 4, 0);
+
+   DIP("beq %s, %s, %d\n", nameIReg(rj), nameIReg(rd),
+                           (Int)extend32(offs16, 16));
+
+   IRExpr* cond = binop(Iop_CmpEQ64, getIReg64(rj), getIReg64(rd));
+   exit(cond, Ijk_Boring, extend64(offs16 << 2, 18));
+
+   return True;
+}
+
+static Bool gen_bne ( DisResult* dres, UInt insn,
+                      const VexArchInfo* archinfo,
+                      const VexAbiInfo* abiinfo )
+{
+   UInt offs16 = SLICE(insn, 25, 10);
+   UInt     rj = SLICE(insn, 9, 5);
+   UInt     rd = SLICE(insn, 4, 0);
+
+   DIP("bne %s, %s, %d\n", nameIReg(rj), nameIReg(rd),
+                           (Int)extend32(offs16, 16));
+
+   IRExpr* cond = binop(Iop_CmpNE64, getIReg64(rj), getIReg64(rd));
+   exit(cond, Ijk_Boring, extend64(offs16 << 2, 18));
+
+   return True;
+}
+
+static Bool gen_blt ( DisResult* dres, UInt insn,
+                      const VexArchInfo* archinfo,
+                      const VexAbiInfo* abiinfo )
+{
+   UInt offs16 = SLICE(insn, 25, 10);
+   UInt     rj = SLICE(insn, 9, 5);
+   UInt     rd = SLICE(insn, 4, 0);
+
+   DIP("blt %s, %s, %d\n", nameIReg(rj), nameIReg(rd),
+                           (Int)extend32(offs16, 16));
+
+   IRExpr* cond = binop(Iop_CmpLT64S, getIReg64(rj), getIReg64(rd));
+   exit(cond, Ijk_Boring, extend64(offs16 << 2, 18));
+
+   return True;
+}
+
+static Bool gen_bge ( DisResult* dres, UInt insn,
+                      const VexArchInfo* archinfo,
+                      const VexAbiInfo* abiinfo )
+{
+   UInt offs16 = SLICE(insn, 25, 10);
+   UInt     rj = SLICE(insn, 9, 5);
+   UInt     rd = SLICE(insn, 4, 0);
+
+   DIP("bge %s, %s, %d\n", nameIReg(rj), nameIReg(rd),
+                           (Int)extend32(offs16, 16));
+
+   IRExpr* cond = binop(Iop_CmpLE64S, getIReg64(rd), getIReg64(rj));
+   exit(cond, Ijk_Boring, extend64(offs16 << 2, 18));
+
+   return True;
+}
+
+static Bool gen_bltu ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt offs16 = SLICE(insn, 25, 10);
+   UInt     rj = SLICE(insn, 9, 5);
+   UInt     rd = SLICE(insn, 4, 0);
+
+   DIP("bltu %s, %s, %d\n", nameIReg(rj), nameIReg(rd),
+                            (Int)extend32(offs16, 16));
+
+   IRExpr* cond = binop(Iop_CmpLT64U, getIReg64(rj), getIReg64(rd));
+   exit(cond, Ijk_Boring, extend64(offs16 << 2, 18));
+
+   return True;
+}
+
+static Bool gen_bgeu ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt offs16 = SLICE(insn, 25, 10);
+   UInt     rj = SLICE(insn, 9, 5);
+   UInt     rd = SLICE(insn, 4, 0);
+
+   DIP("bgeu %s, %s, %d\n", nameIReg(rj), nameIReg(rd),
+                            (Int)extend32(offs16, 16));
+
+   IRExpr* cond = binop(Iop_CmpLE64U, getIReg64(rd), getIReg64(rj));
+   exit(cond, Ijk_Boring, extend64(offs16 << 2, 18));
+
+   return True;
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Helpers for vector integer arithmetic insns          ---*/
+/*------------------------------------------------------------*/
+
+static IROp mkVecADD ( UInt size ) {
+   const IROp ops[5]
+      = { Iop_Add8x16, Iop_Add16x8, Iop_Add32x4, Iop_Add64x2, Iop_Add128x1 };
+   vassert(size < 5);
+   return ops[size];
+}
+
+static IROp mkVecSUB ( UInt size ) {
+   const IROp ops[5]
+      = { Iop_Sub8x16, Iop_Sub16x8, Iop_Sub32x4, Iop_Sub64x2, Iop_Sub128x1 };
+   vassert(size < 5);
+   return ops[size];
+}
+
+static IROp mkVecMAXU ( UInt size ) {
+   const IROp ops[4]
+      = { Iop_Max8Ux16, Iop_Max16Ux8, Iop_Max32Ux4, Iop_Max64Ux2 };
+   vassert(size < 4);
+   return ops[size];
+}
+
+static IROp mkVecMAXS ( UInt size ) {
+   const IROp ops[4]
+      = { Iop_Max8Sx16, Iop_Max16Sx8, Iop_Max32Sx4, Iop_Max64Sx2 };
+   vassert(size < 4);
+   return ops[size];
+}
+
+static IROp mkVecMINU ( UInt size ) {
+   const IROp ops[4]
+      = { Iop_Min8Ux16, Iop_Min16Ux8, Iop_Min32Ux4, Iop_Min64Ux2 };
+   vassert(size < 4);
+   return ops[size];
+}
+
+static IROp mkVecMINS ( UInt size ) {
+   const IROp ops[4]
+      = { Iop_Min8Sx16, Iop_Min16Sx8, Iop_Min32Sx4, Iop_Min64Sx2 };
+   vassert(size < 4);
+   return ops[size];
+}
+
+static IROp mkV256MAXU ( UInt size ) {
+   const IROp ops[4]
+      = { Iop_Max8Ux32, Iop_Max16Ux16, Iop_Max32Ux8, Iop_Max64Ux4 };
+   vassert(size < 4);
+   return ops[size];
+}
+
+static IROp mkV256MAXS ( UInt size ) {
+   const IROp ops[4]
+      = { Iop_Max8Sx32, Iop_Max16Sx16, Iop_Max32Sx8, Iop_Max64Sx4 };
+   vassert(size < 4);
+   return ops[size];
+}
+
+static IROp mkV256MINU ( UInt size ) {
+   const IROp ops[4]
+      = { Iop_Min8Ux32, Iop_Min16Ux16, Iop_Min32Ux8, Iop_Min64Ux4 };
+   vassert(size < 4);
+   return ops[size];
+}
+
+static IROp mkV256MINS ( UInt size ) {
+   const IROp ops[4]
+      = { Iop_Min8Sx32, Iop_Min16Sx16, Iop_Min32Sx8, Iop_Min64Sx4 };
+   vassert(size < 4);
+   return ops[size];
+}
+
+static IROp mkVecCMPGTS ( UInt size ) {
+   const IROp ops[4]
+      = { Iop_CmpGT8Sx16, Iop_CmpGT16Sx8, Iop_CmpGT32Sx4, Iop_CmpGT64Sx2 };
+   vassert(size < 4);
+   return ops[size];
+}
+
+static IROp mkVecSHLN ( UInt size ) {
+   const IROp ops[4]
+      = { Iop_ShlN8x16, Iop_ShlN16x8, Iop_ShlN32x4, Iop_ShlN64x2 };
+   vassert(size < 4);
+   return ops[size];
+}
+
+static IROp mkVecSHRN ( UInt size ) {
+   const IROp ops[4]
+      = { Iop_ShrN8x16, Iop_ShrN16x8, Iop_ShrN32x4, Iop_ShrN64x2 };
+   vassert(size < 4);
+   return ops[size];
+}
+
+static Bool gen_vadd_vsub ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt vd    = SLICE(insn, 4, 0);
+   UInt vj    = SLICE(insn, 9, 5);
+   UInt vk    = SLICE(insn, 14, 10);
+   UInt insSz = SLICE(insn, 16, 15);
+   UInt isAdd = SLICE(insn, 17, 17);
+
+   const HChar *nm[2] = { "vsub", "vadd" };
+   IROp mathOp = isAdd ? mkVecADD(insSz): mkVecSUB(insSz);
+
+   DIP("%s.%s %s, %s, %s\n", nm[isAdd], mkInsSize(insSz),
+                             nameVReg(vd), nameVReg(vj), nameVReg(vk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putVReg(vd, binop(mathOp, getVReg(vj), getVReg(vk)));
+
+   return True;
+}
+
+static Bool gen_vaddi_vsubi ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt vd     = SLICE(insn, 4, 0);
+   UInt vj     = SLICE(insn, 9, 5);
+   UInt ui5    = SLICE(insn, 14, 10);
+   UInt insSz  = SLICE(insn, 16, 15);
+   UInt isAdd  = SLICE(insn, 17, 17);
+
+   IRTemp res = newTemp(Ity_V128);
+   IROp mathOp = isAdd ? mkVecADD(insSz) : mkVecSUB(insSz);
+
+   switch (insSz) {
+      case 0b00: assign(res, unop(Iop_Dup8x16, mkU8(ui5)));                  break;
+      case 0b01: assign(res, unop(Iop_Dup16x8, mkU16(ui5)));                 break;
+      case 0b10: assign(res, unop(Iop_Dup32x4, mkU32(ui5)));                 break;
+      case 0b11: assign(res, binop(Iop_64HLtoV128, mkU64(ui5), mkU64(ui5))); break;
+      default:   vassert(0);                                                 break;
+   }
+
+   const HChar *nm[2] = { "vsubi", "vaddi" };
+
+   DIP("%s.%s %s, %s, %u\n", nm[isAdd], mkInsSize(insSz + 4),
+                             nameVReg(vd), nameVReg(vj), ui5);
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putVReg(vd, binop(mathOp, getVReg(vj), mkexpr(res)));
+
+   return True;
+}
+
+static Bool gen_vmax_vmin ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt vd    = SLICE(insn, 4, 0);
+   UInt vj    = SLICE(insn, 9, 5);
+   UInt vk    = SLICE(insn, 14, 10);
+   UInt insSz = SLICE(insn, 16, 15);
+   UInt isMin = SLICE(insn, 17, 17);
+   UInt isU   = SLICE(insn, 18, 18);
+
+   IROp op = isMin ? isU ? mkVecMINU(insSz) : mkVecMINS(insSz) :
+                     isU ? mkVecMAXU(insSz) : mkVecMAXS(insSz);
+   UInt id = isU ? (insSz + 4) : insSz;
+   const HChar *nm[2] = { "vmax", "vmin" };
+
+   DIP("%s.%s %s, %s, %s\n", nm[isMin], mkInsSize(id),
+                             nameVReg(vd), nameVReg(vj), nameVReg(vk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putVReg(vd, binop(op, getVReg(vj), getVReg(vk)));
+
+   return True;
+}
+
+static Bool gen_xvmax_xvmin ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt xd    = SLICE(insn, 4, 0);
+   UInt xj    = SLICE(insn, 9, 5);
+   UInt xk    = SLICE(insn, 14, 10);
+   UInt insSz = SLICE(insn, 16, 15);
+   UInt isMin = SLICE(insn, 17, 17);
+   UInt isU   = SLICE(insn, 18, 18);
+
+   IROp op = isMin ? isU ? mkV256MINU(insSz) : mkV256MINS(insSz) :
+                     isU ? mkV256MAXU(insSz) : mkV256MAXS(insSz);
+   UInt id = isU ? (insSz + 4) : insSz;
+   const HChar *nm[2] = { "xvmax", "xvmin" };
+
+   DIP("%s.%s %s, %s, %s\n", nm[isMin], mkInsSize(id),
+                             nameXReg(xd), nameXReg(xj), nameXReg(xk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LASX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putXReg(xd, binop(op, getXReg(xj), getXReg(xk)));
+
+   return True;
+}
+
+static IRTemp gen_vmsk_b ( IRTemp shr )
+{
+   UInt i;
+   IRTemp tmp[16];
+   IRTemp tOr = newTemp(Ity_I32);
+   IRTemp res = newTemp(Ity_V128);
+
+   for (i = 0; i < 16; i++) {
+      tmp[i] = newTemp(Ity_I32);
+      assign(tmp[i], binop(Iop_Shl32,
+                           unop(Iop_8Uto32,
+                                binop(Iop_GetElem8x16,
+                                      mkexpr(shr), mkU8(i))),
+                           mkU8(i)));
+   }
+
+   assign(tOr, binop(Iop_Or32,
+                     binop(Iop_Or32,
+                           binop(Iop_Or32,
+                                 binop(Iop_Or32, mkexpr(tmp[15]), mkexpr(tmp[14])),
+                                 binop(Iop_Or32, mkexpr(tmp[13]), mkexpr(tmp[12]))),
+                           binop(Iop_Or32,
+                                 binop(Iop_Or32, mkexpr(tmp[11]), mkexpr(tmp[10])),
+                                 binop(Iop_Or32, mkexpr(tmp[9]), mkexpr(tmp[8])))),
+                     binop(Iop_Or32,
+                           binop(Iop_Or32,
+                                 binop(Iop_Or32, mkexpr(tmp[7]), mkexpr(tmp[6])),
+                                 binop(Iop_Or32, mkexpr(tmp[5]), mkexpr(tmp[4]))),
+                           binop(Iop_Or32,
+                                 binop(Iop_Or32, mkexpr(tmp[3]), mkexpr(tmp[2])),
+                                 binop(Iop_Or32, mkexpr(tmp[1]), mkexpr(tmp[0]))))));
+   assign(res, unop(Iop_64UtoV128, extendU(Ity_I32, mkexpr(tOr))));
+
+   return res;
+}
+
+static IRTemp gen_vmsk_h ( IRTemp shr )
+{
+   UInt i;
+   IRTemp tmp[8];
+   IRTemp tOr = newTemp(Ity_I32);
+   IRTemp res = newTemp(Ity_V128);
+
+   for (i = 0; i < 8; i++) {
+      tmp[i] = newTemp(Ity_I32);
+      assign(tmp[i], binop(Iop_Shl32,
+                           unop(Iop_16Uto32,
+                                binop(Iop_GetElem16x8,
+                                      mkexpr(shr), mkU8(i))),
+                           mkU8(i)));
+   }
+
+   assign(tOr, binop(Iop_Or32,
+                     binop(Iop_Or32,
+                           binop(Iop_Or32, mkexpr(tmp[7]), mkexpr(tmp[6])),
+                           binop(Iop_Or32, mkexpr(tmp[5]), mkexpr(tmp[4]))),
+                     binop(Iop_Or32,
+                           binop(Iop_Or32, mkexpr(tmp[3]), mkexpr(tmp[2])),
+                           binop(Iop_Or32, mkexpr(tmp[1]), mkexpr(tmp[0])))));
+   assign(res, unop(Iop_64UtoV128, extendU(Ity_I32, mkexpr(tOr))));
+
+   return res;
+}
+
+static IRTemp gen_vmsk_w ( IRTemp shr )
+{
+   UInt i;
+   IRTemp tmp[4];
+   IRTemp tOr = newTemp(Ity_I32);
+   IRTemp res = newTemp(Ity_V128);
+
+   for (i = 0; i < 4; i++) {
+      tmp[i] = newTemp(Ity_I32);
+      assign(tmp[i], binop(Iop_Shl32,
+                           binop(Iop_GetElem32x4,
+                                 mkexpr(shr), mkU8(i)),
+                           mkU8(i)));
+   }
+   assign(tOr, binop(Iop_Or32,
+                     binop(Iop_Or32, mkexpr(tmp[3]), mkexpr(tmp[2])),
+                     binop(Iop_Or32, mkexpr(tmp[1]), mkexpr(tmp[0]))));
+
+   assign(res, unop(Iop_64UtoV128, extendU(Ity_I32, mkexpr(tOr))));
+
+   return res;
+}
+
+static IRTemp gen_vmsk_d ( IRTemp shr )
+{
+   UInt i;
+   IRTemp tmp[2];
+   IRTemp res = newTemp(Ity_V128);
+
+   for (i = 0; i < 2; i++) {
+      tmp[i] = newTemp(Ity_I64);
+      assign(tmp[i], binop(Iop_Shl64,
+                           binop(Iop_GetElem64x2,
+                                 mkexpr(shr), mkU8(i)),
+                           mkU8(i)));
+   }
+   assign(res, unop(Iop_64UtoV128,
+                    binop(Iop_Or64,mkexpr(tmp[1]), mkexpr(tmp[0]))));
+
+   return res;
+}
+
+static Bool gen_vmsk ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo*  abiinfo )
+{
+   UInt vd    = SLICE(insn, 4, 0);
+   UInt vj    = SLICE(insn, 9, 5);
+   UInt insSz = SLICE(insn, 11, 10);
+   UInt insTy = SLICE(insn, 13, 12);
+
+   IRTemp shr = newTemp(Ity_V128);
+   IRTemp cmp = newTemp(Ity_V128);
+   IRTemp res = newTemp(Ity_V128);
+   IRTemp src = newTemp(Ity_V128);
+   assign(src, getVReg(vj));
+
+   switch (insTy) {
+      case 0b00: {
+         UInt shrNum[4] = {7, 15, 31, 63};
+
+         DIP("vmskltz.%s %s, %s\n", mkInsSize(insSz), nameVReg(vd), nameVReg(vj));
+
+         if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+            dres->jk_StopHere = Ijk_SigILL;
+            dres->whatNext    = Dis_StopHere;
+            return True;
+         }
+
+         assign(cmp, binop(mkVecCMPGTS(insSz), mkV128(0x0000), mkexpr(src)));
+         assign(shr, binop(mkVecSHRN(insSz), mkexpr(cmp), mkU8(shrNum[insSz])));
+
+         switch(insSz) {
+            case 0b00: res = gen_vmsk_b(shr); break;
+            case 0b01: res = gen_vmsk_h(shr); break;
+            case 0b10: res = gen_vmsk_w(shr); break;
+            case 0b11: res = gen_vmsk_d(shr); break;
+            default:   vassert(0);            break;
+         }
+         break;
+      }
+
+      case 0b01: {
+         DIP("vmskgez.b %s, %s\n", nameVReg(vd), nameVReg(vj));
+
+         if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+            dres->jk_StopHere = Ijk_SigILL;
+            dres->whatNext    = Dis_StopHere;
+            return True;
+         }
+
+         assign(cmp, binop(Iop_OrV128,
+                           binop(Iop_CmpGT8Sx16, mkexpr(src), mkV128(0x0000)),
+                           binop(Iop_CmpEQ8x16, mkV128(0x0000), mkexpr(src))));
+         assign(shr, binop(Iop_ShrN8x16, mkexpr(cmp), mkU8(7)));
+         res = gen_vmsk_b(shr);
+         break;
+      }
+
+      case 0b10: {
+         DIP("vmsknz.b %s, %s\n", nameVReg(vd), nameVReg(vj));
+
+         if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+            dres->jk_StopHere = Ijk_SigILL;
+            dres->whatNext    = Dis_StopHere;
+            return True;
+         }
+         assign(cmp, unop(Iop_NotV128,
+                          binop(Iop_CmpEQ8x16, mkV128(0x0000), mkexpr(src))));
+         assign(shr, binop(Iop_ShrN8x16, mkexpr(cmp), mkU8(7)));
+         res = gen_vmsk_b(shr);
+         break;
+      }
+
+      default:
+         return False;
+   }
+
+   putVReg(vd, mkexpr(res));
+
+   return True;
+}
+
+static Bool gen_xvmsk ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo*  abiinfo )
+{
+   UInt xd    = SLICE(insn, 4, 0);
+   UInt xj    = SLICE(insn, 9, 5);
+   UInt insTy = SLICE(insn, 13, 12);
+
+   IRTemp shrHi = newTemp(Ity_V128);
+   IRTemp shrLo = newTemp(Ity_V128);
+   IRTemp cmpHi = newTemp(Ity_V128);
+   IRTemp cmpLo = newTemp(Ity_V128);
+   IRTemp res = newTemp(Ity_V256);
+   IRTemp src = newTemp(Ity_V256);
+   assign(src, getXReg(xj));
+
+   switch (insTy) {
+      case 0b10: {
+         DIP("xvmsknz.b %s, %s\n", nameXReg(xd), nameXReg(xj));
+
+         if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+            dres->jk_StopHere = Ijk_SigILL;
+            dres->whatNext    = Dis_StopHere;
+            return True;
+         }
+
+         IRTemp hi, lo;
+         hi = lo = IRTemp_INVALID;
+         breakupV256toV128s(src, &hi, &lo);
+         assign(cmpHi, unop(Iop_NotV128,
+                          binop(Iop_CmpEQ8x16, mkV128(0x0000), mkexpr(hi))));
+         assign(shrHi, binop(Iop_ShrN8x16, mkexpr(cmpHi), mkU8(7)));
+         assign(cmpLo, unop(Iop_NotV128,
+                          binop(Iop_CmpEQ8x16, mkV128(0x0000), mkexpr(lo))));
+         assign(shrLo, binop(Iop_ShrN8x16, mkexpr(cmpLo), mkU8(7)));
+         assign(res, binop(Iop_V128HLtoV256, mkexpr(gen_vmsk_b(shrHi)), mkexpr(gen_vmsk_b(shrLo))));
+         break;
+      }
+
+      default:
+         return False;
+   }
+
+   putXReg(xd, mkexpr(res));
+
+   return True;
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Helpers for vector bit operation insns               ---*/
+/*------------------------------------------------------------*/
+
+static Bool gen_logical_v ( DisResult* dres, UInt insn,
+                            const VexArchInfo* archinfo,
+                            const VexAbiInfo* abiinfo )
+{
+   UInt vd    = SLICE(insn, 4, 0);
+   UInt vj    = SLICE(insn, 9, 5);
+   UInt vk    = SLICE(insn, 14, 10);
+   UInt insTy = SLICE(insn, 17, 15);
+
+   IRTemp res  = newTemp(Ity_V128);
+   IRTemp srcL = newTemp(Ity_V128);
+   IRTemp srcR = newTemp(Ity_V128);
+   assign(srcL, getVReg(vj));
+   assign(srcR, getVReg(vk));
+
+   switch (insTy) {
+      case 0b100:
+         assign(res, binop(Iop_AndV128, mkexpr(srcL), mkexpr(srcR)));
+         break;
+      case 0b101:
+         assign(res, binop(Iop_OrV128,  mkexpr(srcL), mkexpr(srcR)));
+         break;
+      case 0b110:
+         assign(res, binop(Iop_XorV128, mkexpr(srcL), mkexpr(srcR)));
+         break;
+      case 0b111:
+         assign(res, unop(Iop_NotV128, binop(Iop_OrV128,
+                                             mkexpr(srcL), mkexpr(srcR))));
+         break;
+      case 0b000:
+         assign(res, binop(Iop_AndV128,
+                           unop(Iop_NotV128, mkexpr(srcL)),
+                           mkexpr(srcR)));
+         break;
+      case 0b001:
+         assign(res, binop(Iop_OrV128,
+                           mkexpr(srcL),
+                           unop(Iop_NotV128, mkexpr(srcR))));
+         break;
+      default:
+         return False;
+   }
+
+   const HChar *nm[8] = { "vandn.v", "vorn.v", "", "",
+                          "vand.v",  "vor.v", "vxor.v", "vnor.v" };
+
+   DIP("%s %s, %s, %s\n", nm[insTy], nameVReg(vd), nameVReg(vj), nameVReg(vk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putVReg(vd, mkexpr(res));
+
+   return True;
+}
+
+static Bool gen_logical_xv ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt xd    = SLICE(insn, 4, 0);
+   UInt xj    = SLICE(insn, 9, 5);
+   UInt xk    = SLICE(insn, 14, 10);
+   UInt insTy = SLICE(insn, 17, 15);
+
+   IRTemp res = newTemp(Ity_V256);
+   IRTemp sL  = newTemp(Ity_V256);
+   IRTemp sR  = newTemp(Ity_V256);
+   assign(sL, getXReg(xj));
+   assign(sR, getXReg(xk));
+
+   switch (insTy) {
+      case 0b110:
+         assign(res, binop(Iop_XorV256, mkexpr(sL), mkexpr(sR)));
+         break;
+      default:
+         return False;
+   }
+
+   const HChar *nm[8] = { "xvandn.v", "xvorn.v", "", "",
+                          "xvand.v",  "xvor.v", "xvxor.v", "xvnor.v" };
+
+   DIP("%s %s, %s, %s\n", nm[insTy], nameXReg(xd), nameXReg(xj), nameXReg(xk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LASX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putXReg(xd, mkexpr(res));
+
+   return True;
+}
+
+static Bool gen_vlogical_u8 ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo* abiinfo )
+{
+   UInt vd    = SLICE(insn, 4, 0);
+   UInt vj    = SLICE(insn, 9, 5);
+   UInt ui8   = SLICE(insn, 17, 10);
+   UInt insTy = SLICE(insn, 19, 18);
+
+   IRTemp res  = newTemp(Ity_V128);
+   switch (insTy) {
+      case 0b00:
+         assign(res, binop(Iop_AndV128,
+                           getVReg(vj),
+                           unop(Iop_Dup8x16, mkU8(ui8))));
+         break;
+      case 0b01:
+         assign(res, binop(Iop_OrV128,
+                           getVReg(vj),
+                           unop(Iop_Dup8x16, mkU8(ui8))));
+         break;
+      case 0b10:
+         assign(res, binop(Iop_XorV128,
+                           getVReg(vj),
+                           unop(Iop_Dup8x16, mkU8(ui8))));
+         break;
+      case 0b11:
+         assign(res, unop(Iop_NotV128,
+                          binop(Iop_OrV128,
+                                getVReg(vj),
+                                unop(Iop_Dup8x16, mkU8(ui8)))));
+         break;
+      default:
+         vassert(0);
+         break;
+   }
+
+   const HChar *nm[4] = { "vandi.b", "vori.b", "vxori.b", "vnori.b" };
+
+   DIP("%s %s, %s, %u\n", nm[insTy], nameVReg(vd), nameVReg(vj), ui8);
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putVReg(vd, mkexpr(res));
+
+   return True;
+}
+
+static Bool gen_vbiti ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo*  abiinfo )
+{
+   UInt vd     = SLICE(insn, 4, 0);
+   UInt vj     = SLICE(insn, 9, 5);
+   UInt insImm = SLICE(insn, 17, 10);
+   UInt insTy  = SLICE(insn, 19, 18);
+
+   IRTemp c1   = newTemp(Ity_V128);
+   IRTemp argR = newTemp(Ity_V128);
+   IRTemp res  = newTemp(Ity_V128);
+   UInt insSz, uImm;
+
+   if ((insImm & 0xf8) == 0x8) {         // 00001mmm; b
+      uImm = insImm & 0x07;
+      insSz = 0;
+   } else if ((insImm & 0xf0) == 0x10) { // 0001mmmm; h
+      uImm = insImm & 0x0f;
+      insSz = 1;
+   } else if ((insImm & 0xe0) == 0x20) { // 001mmmmm; w
+      uImm = insImm & 0x1f;
+      insSz = 2;
+   } else if ((insImm & 0xc0) == 0x40) { // 01mmmmmm; d
+      uImm = insImm & 0x3f;
+      insSz = 3;
+   } else {
+      vassert(0);
+   }
+
+   switch (insSz) {
+      case 0b00:
+         assign(c1, unop(Iop_Dup8x16, mkU8(1)));
+         break;
+      case 0b01:
+         assign(c1, unop(Iop_Dup16x8, mkU16(1)));
+         break;
+      case 0b10:
+         assign(c1, unop(Iop_Dup32x4, mkU32(1)));
+         break;
+      case 0b11:
+         assign(c1, binop(Iop_64HLtoV128, mkU64(1), mkU64(1)));
+         break;
+      default:
+         vassert(0);
+         break;
+   }
+
+   assign(argR, binop(mkVecSHLN(insSz), mkexpr(c1), mkU8(uImm)));
+   switch (insTy) {
+      case 0b00:
+         assign(res, binop(Iop_AndV128,
+                           getVReg(vj), unop(Iop_NotV128, mkexpr(argR))));
+         break;
+      case 0b01:
+         assign(res, binop(Iop_OrV128, getVReg(vj), mkexpr(argR)));
+         break;
+      case 0b10:
+         assign(res, binop(Iop_XorV128, getVReg(vj), mkexpr(argR)));
+         break;
+      default:
+         vassert(0);
+         break;
+   }
+
+   const HChar *nm[3] = { "vbitrevi", "vbitclri", "vbitseti" };
+
+   DIP("%s.%s %s, %u\n", nm[insTy], mkInsSize(insSz), nameVReg(vd), uImm);
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putVReg(vd, mkexpr(res));
+
+   return True;
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Helpers for vector string processing insns           ---*/
+/*------------------------------------------------------------*/
+
+static Bool gen_vfrstpi ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo*  abiinfo )
+{
+   UInt vd    = SLICE(insn, 4, 0);
+   UInt vj    = SLICE(insn, 9, 5);
+   UInt ui5   = SLICE(insn, 14, 10);
+   UInt insSz = SLICE(insn, 16, 15);
+
+   UInt i;
+   IRTemp data[2];
+   IRTemp res = newTemp(Ity_V128);
+
+   for (i = 0; i < 2; i++) {
+      data[i] = newTemp(Ity_I64);
+      assign(data[i], binop(Iop_GetElem64x2, getVReg(vj), mkU8(i)));
+   }
+
+   IRExpr** arg = mkIRExprVec_3(mkU64(insSz), mkexpr(data[1]), mkexpr(data[0]));
+   IRExpr* call = mkIRExprCCall(Ity_I64, 0/*regparms*/,
+                                "loongarch64_calculate_negative_id",
+                                &loongarch64_calculate_negative_id,
+                                arg);
+
+   switch (insSz) {
+      case 0b00:
+         assign(res, triop(Iop_SetElem8x16,
+                           getVReg(vd),
+                           mkU8(ui5 % 16),
+                           unop(Iop_64to8, call)));
+         break;
+      case 0b01:
+         assign(res, triop(Iop_SetElem16x8,
+                           getVReg(vd),
+                           mkU8(ui5 % 8),
+                           unop(Iop_64to16, call)));
+         break;
+      default:
+         return False;
+   }
+
+   DIP("vfrstpi.%s %s, %s, %u\n", mkInsSize(insSz), nameVReg(vd), nameVReg(vj), ui5);
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putVReg(vd, mkexpr(res));
+
+   return True;
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Helpers for vector comparison and selection insns    ---*/
+/*------------------------------------------------------------*/
+
+static IROp mkVecCMPEQ ( UInt size ) {
+   const IROp ops[4]
+      = { Iop_CmpEQ8x16, Iop_CmpEQ16x8, Iop_CmpEQ32x4, Iop_CmpEQ64x2 };
+   vassert(size < 4);
+   return ops[size];
+}
+
+static IROp mkV256CMPEQ ( UInt size ) {
+   const IROp ops[4]
+      = { Iop_CmpEQ8x32, Iop_CmpEQ16x16, Iop_CmpEQ32x8, Iop_CmpEQ64x4 };
+   vassert(size < 4);
+   return ops[size];
+}
+
+static IROp mkVecCMPGTU ( UInt size ) {
+   const IROp ops[4]
+      = { Iop_CmpGT8Ux16, Iop_CmpGT16Ux8, Iop_CmpGT32Ux4, Iop_CmpGT64Ux2 };
+   vassert(size < 4);
+   return ops[size];
+}
+
+static Bool gen_vcmp_integer ( DisResult* dres, UInt insn,
+                               const VexArchInfo* archinfo,
+                               const VexAbiInfo* abiinfo )
+{
+   UInt vd    = SLICE(insn, 4, 0);
+   UInt vj    = SLICE(insn, 9, 5);
+   UInt vk    = SLICE(insn, 14, 10);
+   UInt insSz = SLICE(insn, 16, 15);
+   UInt insTy = SLICE(insn, 19, 17);
+
+   UInt szId   = insSz;
+   IRTemp res  = newTemp(Ity_V128);
+   IRTemp argL = newTemp(Ity_V128);
+   IRTemp argR = newTemp(Ity_V128);
+   assign(argL, getVReg(vj));
+   assign(argR, getVReg(vk));
+
+   switch (insTy) {
+      case 0b000:
+         assign(res, binop(mkVecCMPEQ(insSz), mkexpr(argL), mkexpr(argR)));
+         break;
+      case 0b001:
+         assign(res, binop(Iop_OrV128,
+                           binop(mkVecCMPGTS(insSz), mkexpr(argR), mkexpr(argL)),
+                           binop(mkVecCMPEQ(insSz), mkexpr(argL), mkexpr(argR))));
+         break;
+      case 0b010:
+         assign(res, binop(Iop_OrV128,
+                           binop(mkVecCMPGTU(insSz), mkexpr(argR), mkexpr(argL)),
+                           binop(mkVecCMPEQ(insSz), mkexpr(argL), mkexpr(argR))));
+         szId = insSz + 4;
+         break;
+      case 0b011:
+         assign(res, binop(mkVecCMPGTS(insSz), mkexpr(argR), mkexpr(argL)));
+         break;
+      case 0b100:
+         assign(res, binop(mkVecCMPGTU(insSz), mkexpr(argR), mkexpr(argL)));
+         szId = insSz + 4;
+         break;
+      default:
+         return False;
+   }
+
+   const HChar *nm[5] = { "vseq",  "vsle",  "vsle",  "vslt",  "vslt" };
+
+   DIP("%s.%s %s, %s, %s\n", nm[insTy], mkInsSize(szId),
+                             nameVReg(vd), nameVReg(vj), nameVReg(vk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putVReg(vd, mkexpr(res));
+
+   return True;
+}
+
+static Bool gen_vcmpi_integer ( DisResult* dres, UInt insn,
+                                const VexArchInfo* archinfo,
+                                const VexAbiInfo* abiinfo )
+{
+   UInt vd    = SLICE(insn, 4, 0);
+   UInt vj    = SLICE(insn, 9, 5);
+   UInt si5   = SLICE(insn, 14, 10);
+   UInt insSz = SLICE(insn, 16, 15);
+   UInt isS   = SLICE(insn, 17, 17);
+   UInt insTy = SLICE(insn, 19, 17);
+
+   UInt szId   = insSz;
+   IRTemp res  = newTemp(Ity_V128);
+   IRTemp argL = newTemp(Ity_V128);
+   IRTemp argR = newTemp(Ity_V128);
+   assign(argL, getVReg(vj));
+
+   IRExpr *si5Expr;
+   IRTemp s64  = newTemp(Ity_I64);
+   assign(s64, mkU64(extend64(si5, 5)));
+
+   if (insTy == 0b000)
+      isS = 1;
+
+   switch (insSz) {
+      case 0b00:
+         si5Expr = isS ? unop(Iop_64to8, mkexpr(s64)) : mkU8(si5);
+         assign(argR, unop(Iop_Dup8x16, si5Expr));
+         break;
+      case 0b01:
+         si5Expr = isS ? unop(Iop_64to16, mkexpr(s64)) : mkU16(si5);
+         assign(argR, unop(Iop_Dup16x8, si5Expr));
+         break;
+      case 0b10:
+         si5Expr = isS ? unop(Iop_64to32, mkexpr(s64)) : mkU32(si5);
+         assign(argR, unop(Iop_Dup32x4, si5Expr));
+         break;
+      case 0b11:
+         si5Expr = isS ? mkexpr(s64) : mkU64(si5);
+         assign(argR, binop(Iop_64HLtoV128, si5Expr, si5Expr));
+         break;
+      default:
+         vassert(0);
+         break;
+   }
+
+   switch (insTy) {
+      case 0b000:
+         assign(res, binop(mkVecCMPEQ(insSz), mkexpr(argL), mkexpr(argR)));
+         break;
+      case 0b001:
+         assign(res, binop(Iop_OrV128,
+                           binop(mkVecCMPGTS(insSz), mkexpr(argR), mkexpr(argL)),
+                           binop(mkVecCMPEQ(insSz), mkexpr(argL), mkexpr(argR))));
+         break;
+      case 0b010:
+         assign(res, binop(Iop_OrV128,
+                           binop(mkVecCMPGTU(insSz), mkexpr(argR), mkexpr(argL)),
+                           binop(mkVecCMPEQ(insSz), mkexpr(argL), mkexpr(argR))));
+         szId = insSz + 4;
+         break;
+      case 0b011:
+         assign(res, binop(mkVecCMPGTS(insSz), mkexpr(argR), mkexpr(argL)));
+         break;
+      case 0b100:
+         assign(res, binop(mkVecCMPGTU(insSz), mkexpr(argR), mkexpr(argL)));
+         szId = insSz + 4;
+         break;
+      default:
+         vassert(0);
+         break;
+   }
+
+   const HChar *nm[10] = { "vseqi", "vslei", "vslei", "vslti", "vslti" };
+
+   DIP("%s.%s %s, %s, %d\n", nm[insTy], mkInsSize(szId), nameVReg(vd),
+                             nameVReg(vj), (Int)extend32(si5, 5));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putVReg(vd, mkexpr(res));
+
+   return True;
+}
+
+static Bool gen_xvcmp_integer ( DisResult* dres, UInt insn,
+                                const VexArchInfo* archinfo,
+                                const VexAbiInfo* abiinfo )
+{
+   UInt xd    = SLICE(insn, 4, 0);
+   UInt xj    = SLICE(insn, 9, 5);
+   UInt xk    = SLICE(insn, 14, 10);
+   UInt insSz = SLICE(insn, 16, 15);
+   UInt insTy = SLICE(insn, 19, 17);
+
+   UInt szId   = insSz;
+   IRTemp res  = newTemp(Ity_V256);
+   IRTemp argL = newTemp(Ity_V256);
+   IRTemp argR = newTemp(Ity_V256);
+   assign(argL, getXReg(xj));
+   assign(argR, getXReg(xk));
+
+   switch (insTy) {
+      case 0b000:
+         assign(res, binop(mkV256CMPEQ(insSz), mkexpr(argL), mkexpr(argR)));
+         break;
+      default:
+         return False;
+   }
+
+   const HChar *nm[5] = { "xvseq",  "xvsle",  "xvsle",  "xvslt",  "xvslt" };
+
+   DIP("%s.%s %s, %s, %s\n", nm[insTy], mkInsSize(szId),
+                             nameXReg(xd), nameXReg(xj), nameXReg(xk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LASX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putXReg(xd, mkexpr(res));
+
+   return True;
+}
+
+static Bool gen_vset ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo*  abiinfo )
+{
+   UInt cd    = SLICE(insn, 2, 0);
+   UInt vj    = SLICE(insn, 9, 5);
+   UInt insSz = SLICE(insn, 11, 10);
+   UInt insTy = SLICE(insn, 13, 12);
+
+   IROp ops64;
+   IRTemp resHi = newTemp(Ity_I64);
+   IRTemp resLo = newTemp(Ity_I64);
+   IRTemp res   = newTemp(Ity_V128);
+   IRTemp eq    = newTemp(Ity_V128);
+   IRTemp z128  = newTemp(Ity_V128);
+   assign(z128, mkV128(0x0000));
+
+   switch (insTy) {
+      case 0b01: {
+         if (SLICE(insn, 10, 10) == 0b0) {
+            DIP("vseteqz.v %u, %s", cd, nameVReg(vj));
+
+            if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+               dres->jk_StopHere = Ijk_SigILL;
+               dres->whatNext    = Dis_StopHere;
+               return True;
+            }
+
+            assign(res, binop(Iop_CmpEQ64x2, getVReg(vj), mkexpr(z128)));
+            ops64 = Iop_And64;
+         } else {
+            DIP("vsetnez.v %u, %s", cd, nameVReg(vj));
+
+            if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+               dres->jk_StopHere = Ijk_SigILL;
+               dres->whatNext    = Dis_StopHere;
+               return True;
+            }
+
+            assign(res, unop(Iop_NotV128,
+                             binop(Iop_CmpEQ64x2, getVReg(vj), mkexpr(z128))));
+            ops64 = Iop_Or64;
+         }
+         break;
+      }
+
+      case 0b10: {
+         DIP("vsetanyeqz.%s %u, %s", mkInsSize(insSz), cd, nameVReg(vj));
+
+         if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+            dres->jk_StopHere = Ijk_SigILL;
+            dres->whatNext    = Dis_StopHere;
+            return True;
+         }
+
+         assign(eq, binop(mkVecCMPEQ(insSz), getVReg(vj), mkexpr(z128)));
+         assign(res, unop(Iop_NotV128,
+                          binop(Iop_CmpEQ64x2, mkexpr(eq), mkexpr(z128))));
+         ops64 = Iop_Or64;
+         break;
+      }
+
+      case 0b11: {
+         DIP("vsetqllnez.%s %u, %s", mkInsSize(insSz), cd, nameVReg(vj));
+
+         if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+            dres->jk_StopHere = Ijk_SigILL;
+            dres->whatNext    = Dis_StopHere;
+            return True;
+         }
+
+         assign(eq, binop(mkVecCMPEQ(insSz), getVReg(vj), mkexpr(z128)));
+         assign(res, binop(Iop_CmpEQ64x2, mkexpr(eq), mkexpr(z128)));
+         ops64 = Iop_And64;
+         break;
+      }
+
+      default:
+         return False;
+   }
+
+   assign(resHi, binop(Iop_GetElem64x2, mkexpr(res), mkU8(1)));
+   assign(resLo, binop(Iop_GetElem64x2, mkexpr(res), mkU8(0)));
+   putFCC(cd, unop(Iop_64to8, binop(ops64, mkexpr(resHi), mkexpr(resLo))));
+
+   return True;
+}
+
+static Bool gen_xvset ( DisResult* dres, UInt insn,
+                        const VexArchInfo* archinfo,
+                        const VexAbiInfo*  abiinfo )
+{
+   UInt cd    = SLICE(insn, 2, 0);
+   UInt xj    = SLICE(insn, 9, 5);
+   UInt insSz = SLICE(insn, 11, 10);
+   UInt insTy = SLICE(insn, 13, 12);
+
+   IROp ops64  = Iop_INVALID;
+   IRTemp res  = newTemp(Ity_V256);
+   IRTemp z128 = newTemp(Ity_V128);
+   IRTemp src = newTemp(Ity_V256);
+   assign(z128, mkV128(0x0000));
+   assign(src, getXReg(xj));
+
+   switch (insTy) {
+      case 0b01: {
+         if (SLICE(insn, 10, 10) == 0b0) {
+            DIP("xvseteqz.v %u, %s", cd, nameXReg(xj));
+
+            if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LASX)) {
+               dres->jk_StopHere = Ijk_SigILL;
+               dres->whatNext    = Dis_StopHere;
+               return True;
+            }
+
+            IRTemp hi, lo;
+            hi = lo = IRTemp_INVALID;
+            breakupV256toV128s(src, &hi, &lo);
+            assign(res, binop(Iop_V128HLtoV256,
+                              binop(Iop_CmpEQ64x2, mkexpr(hi), mkexpr(z128)),
+                              binop(Iop_CmpEQ64x2, mkexpr(hi), mkexpr(z128))));
+            ops64 = Iop_And64;
+         } else {
+            return False;
+         }
+         break;
+      }
+
+      case 0b10: {
+         DIP("xvsetanyeqz.%s %u, %s", mkInsSize(insSz), cd, nameXReg(xj));
+
+         if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LASX)) {
+            dres->jk_StopHere = Ijk_SigILL;
+            dres->whatNext    = Dis_StopHere;
+            return True;
+         }
+
+         IRTemp eqHi = newTemp(Ity_V128);
+         IRTemp eqLo = newTemp(Ity_V128);
+         IRTemp hi, lo;
+         hi = lo = IRTemp_INVALID;
+         breakupV256toV128s(src, &hi, &lo);
+         assign(eqHi, binop(mkVecCMPEQ(insSz), mkexpr(hi), mkexpr(z128)));
+         assign(eqLo, binop(mkVecCMPEQ(insSz), mkexpr(lo), mkexpr(z128)));
+         assign(res, binop(Iop_V128HLtoV256,
+                           unop(Iop_NotV128,
+                                binop(Iop_CmpEQ64x2, mkexpr(eqHi), mkexpr(z128))),
+                           unop(Iop_NotV128,
+                                binop(Iop_CmpEQ64x2, mkexpr(eqLo), mkexpr(z128)))));
+         ops64 = Iop_Or64;
+         break;
+      }
+
+      default:
+         return False;
+   }
+
+   IRTemp r1, r2, r3, r4;
+   r1 = r2 = r3 = r4 = IRTemp_INVALID;
+   breakupV256to64s(res, &r1, &r2, &r3, &r4);
+   putFCC(cd, unop(Iop_64to8, binop(ops64,
+                                    binop(ops64, mkexpr(r1), mkexpr(r2)),
+                                    binop(ops64, mkexpr(r3), mkexpr(r4)))));
+
+   return True;
+}
+
+/*------------------------------------------------------------*/
+/*--- Helpers for vector moving and shuffling insns        ---*/
+/*------------------------------------------------------------*/
+
+static IROp mkVecPACKOD ( UInt size ) {
+   const IROp ops[4]
+      = { Iop_PackOddLanes8x16, Iop_PackOddLanes16x8,
+          Iop_PackOddLanes32x4, Iop_InterleaveHI64x2 };
+   vassert(size < 4);
+   return ops[size];
+}
+
+static IROp mkVecPACKEV ( UInt size ) {
+   const IROp ops[4]
+      = { Iop_PackEvenLanes8x16, Iop_PackEvenLanes16x8,
+          Iop_PackEvenLanes32x4, Iop_InterleaveLO64x2 };
+   vassert(size < 4);
+   return ops[size];
+}
+
+static IROp mkVecINTERLEAVELO ( UInt size ) {
+   const IROp ops[4]
+      = { Iop_InterleaveLO8x16, Iop_InterleaveLO16x8,
+          Iop_InterleaveLO32x4, Iop_InterleaveLO64x2 };
+   vassert(size < 4);
+   return ops[size];
+}
+
+static IROp mkVecINTERLEAVEHI ( UInt size ) {
+   const IROp ops[4]
+      = { Iop_InterleaveHI8x16, Iop_InterleaveHI16x8,
+          Iop_InterleaveHI32x4, Iop_InterleaveHI64x2 };
+   vassert(size < 4);
+   return ops[size];
+}
+
+static Bool gen_vpickve2gr ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo* abiinfo )
+{
+   UInt rd     = SLICE(insn, 4, 0);
+   UInt vj     = SLICE(insn, 9, 5);
+   UInt insImm = SLICE(insn, 15, 10);
+   UInt isS    = SLICE(insn, 18, 18);
+
+   UInt uImm, insSz;
+   IRExpr *immExpr;
+   IRType extTy = Ity_INVALID;
+   IRTemp res = newTemp(Ity_I64);
+
+   if ((insImm & 0x30) == 0x20) {        // 10mmmm; b
+      uImm = insImm & 0xf;
+      insSz = 0;
+      extTy = Ity_I8;
+   } else if ((insImm & 0x38) == 0x30) { // 110mmm; h
+      uImm = insImm & 0x7;
+      insSz = 1;
+      extTy = Ity_I16;
+   } else if ((insImm & 0x3c) == 0x38) { // 1110mm; w
+      uImm = insImm & 0x3;
+      insSz = 2;
+      extTy = Ity_I32;
+   } else if ((insImm & 0x3e) == 0x3c) { // 11110m; d
+      uImm = insImm & 0x1;
+      insSz = 3;
+   } else {
+      vassert(0);
+   }
+
+   immExpr = binop(mkVecGetElem(insSz), getVReg(vj), mkU8(uImm));
+   if (insSz != 3)
+      assign(res, isS ? extendS(extTy, immExpr) :
+                        extendU(extTy, immExpr));
+   else
+      assign(res, binop(Iop_Or64, mkU64(0), immExpr));
+
+   UInt nmId = isS ? insSz : (insSz + 4);
+
+   DIP("vpickve2gr.%s %s, %s", mkInsSize(nmId),
+                               nameIReg(rd), nameVReg(vj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putIReg(rd, mkexpr(res));
+
+   return True;
+}
+
+static Bool gen_vreplgr2vr ( DisResult* dres, UInt insn,
+                             const VexArchInfo* archinfo,
+                             const VexAbiInfo*  abiinfo )
+{
+   UInt vd    = SLICE(insn, 4, 0);
+   UInt rj    = SLICE(insn, 9, 5);
+   UInt insSz = SLICE(insn, 11, 10);
+
+   IRTemp res = newTemp(Ity_V128);
+   switch (insSz) {
+      case 0b00:
+         assign(res, unop(Iop_Dup8x16, getIReg8(rj)));
+         break;
+      case 0b01:
+         assign(res, unop(Iop_Dup16x8, getIReg16(rj)));
+         break;
+      case 0b10:
+         assign(res, unop(Iop_Dup32x4, getIReg32(rj)));
+         break;
+      case 0b11:
+         assign(res, binop(Iop_64HLtoV128, getIReg64(rj), getIReg64(rj)));
+         break;
+      default:
+         vassert(0);
+         break;
+   }
+
+   DIP("vreplgr2vr.%s %s, %s", mkInsSize(insSz),
+                               nameVReg(vd), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putVReg(vd, mkexpr(res));
+
+   return True;
+}
+
+static Bool gen_xvreplgr2vr ( DisResult* dres, UInt insn,
+                              const VexArchInfo* archinfo,
+                              const VexAbiInfo*  abiinfo )
+{
+   UInt xd    = SLICE(insn, 4, 0);
+   UInt rj    = SLICE(insn, 9, 5);
+   UInt insSz = SLICE(insn, 11, 10);
+
+   IRTemp res = newTemp(Ity_V128);
+   switch (insSz) {
+      case 0b00:
+         assign(res, unop(Iop_Dup8x16, getIReg8(rj)));
+         break;
+      case 0b01:
+         assign(res, unop(Iop_Dup16x8, getIReg16(rj)));
+         break;
+      case 0b10:
+         assign(res, unop(Iop_Dup32x4, getIReg32(rj)));
+         break;
+      case 0b11:
+         assign(res, binop(Iop_64HLtoV128, getIReg64(rj), getIReg64(rj)));
+         break;
+      default:
+         vassert(0);
+         break;
+   }
+
+   DIP("xvreplgr2vr.%s %s, %s", mkInsSize(insSz),
+                                nameXReg(xd), nameIReg(rj));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LASX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putXReg(xd, binop(Iop_V128HLtoV256, mkexpr(res), mkexpr(res)));
+
+   return True;
+}
+
+static Bool gen_vreplve ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo*  abiinfo )
+{
+   UInt vd    = SLICE(insn, 4, 0);
+   UInt vj    = SLICE(insn, 9, 5);
+   UInt rk    = SLICE(insn, 14, 10);
+   UInt insSz = SLICE(insn, 16, 15);
+
+   IRExpr *elem;
+   IRTemp mod  = newTemp(Ity_I8);
+   IRTemp res  = newTemp(Ity_V128);
+   UInt div[4] = { 0x10, 0x8, 0x4, 0x2 };
+
+   assign(mod, unop(Iop_64to8,
+                    unop(Iop_128HIto64,
+                         binop(Iop_DivModU64to64,
+                               getIReg64(rk),
+                               mkU64(div[insSz])))));
+
+   elem = binop(mkVecGetElem(insSz), getVReg(vj), mkexpr(mod));
+   switch (insSz) {
+      case 0b00:
+         assign(res, unop(Iop_Dup8x16, elem));
+         break;
+      case 0b01:
+         assign(res, unop(Iop_Dup16x8, elem));
+         break;
+      case 0b10:
+         assign(res, unop(Iop_Dup32x4, elem));
+         break;
+      case 0b11:
+         assign(res, binop(Iop_64HLtoV128, elem, elem));
+         break;
+      default:
+         vassert(0);
+         break;
+   }
+
+   DIP("vreplve.%s %s, %s, %s", mkInsSize(insSz),
+                                nameVReg(vd), nameVReg(vj), nameIReg(rk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putVReg(vd, mkexpr(res));
+
+   return True;
+}
+
+static Bool gen_xvpickve ( DisResult* dres, UInt insn,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo*  abiinfo )
+{
+   UInt xd     = SLICE(insn, 4, 0);
+   UInt xj     = SLICE(insn, 9, 5);
+   UInt insImm = SLICE(insn, 15, 10);
+
+   UInt sImm, insSz;
+   IRTemp res = newTemp(Ity_I64);
+   IRTemp z64 = newTemp(Ity_I64);
+   IRTemp src = newTemp(Ity_V256);
+   assign(z64, mkU64(0));
+   assign(src, getXReg(xj));
+
+   if ((insImm & 0x38) == 0x30) {        // 110ui3; w
+      IRTemp s[8];
+      s[7] = s[6] = s[5] = s[4] = s[3] = s[2] = s[1] = s[0] = IRTemp_INVALID;
+      breakupV256to32s(src, &s[7], &s[6], &s[5], &s[4],
+                            &s[3], &s[2], &s[1], &s[0]);
+      sImm = insImm & 0x7;
+      insSz = 0;
+      assign(res, extendU(Ity_I32, mkexpr(s[sImm])));
+   } else if ((insImm & 0x3c) == 0x38) { // 1110ui2; d
+      IRTemp s[4];
+      s[3] = s[2] = s[1] = s[0] = IRTemp_INVALID;
+      breakupV256to64s(src, &s[3], &s[2], &s[1], &s[0]);
+      sImm = insImm & 0x3;
+      insSz = 1;
+      assign(res, mkexpr(s[sImm]));
+   } else {
+      vassert(0);
+   }
+
+   const HChar arr = "wd"[insSz];
+   DIP("xvpickve.%c %s, %s, %u", arr, nameXReg(xd), nameXReg(xj), sImm);
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LASX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putXReg(xd, mkV256from64s(z64, z64, z64, res));
+
+   return True;
+}
+
+static Bool gen_evod ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt vd    = SLICE(insn, 4, 0);
+   UInt vj    = SLICE(insn, 9, 5);
+   UInt vk    = SLICE(insn, 14, 10);
+   UInt insSz = SLICE(insn, 16, 15);
+
+   const HChar *nm;
+   IRTemp argL = newTemp(Ity_V128);
+   IRTemp argR = newTemp(Ity_V128);
+   IRTemp res  = newTemp(Ity_V128);
+
+   switch (SLICE(insn, 19, 17)) {
+      case 0b011:
+         nm = "vpackev";
+         assign(argL, binop(mkVecPACKEV(insSz),
+                            getVReg(vj),
+                            mkV128(0x0000)));
+         assign(argR, binop(mkVecPACKEV(insSz),
+                            getVReg(vk),
+                            mkV128(0x0000)));
+         assign(res, binop(mkVecINTERLEAVEHI(insSz),
+                           mkexpr(argL),
+                           mkexpr(argR)));
+         break;
+      case 0b100:
+         nm = "vpackod";
+         assign(argL, binop(mkVecPACKOD(insSz),
+                            getVReg(vj),
+                            mkV128(0x0000)));
+         assign(argR, binop(mkVecPACKOD(insSz),
+                            getVReg(vk),
+                            mkV128(0x0000)));
+         assign(res, binop(mkVecINTERLEAVEHI(insSz),
+                           mkexpr(argL),
+                           mkexpr(argR)));
+         break;
+      case 0b101:
+         nm = "vilvl";
+         assign(res, binop(mkVecINTERLEAVELO(insSz),
+                           getVReg(vj),
+                           getVReg(vk)));
+         break;
+      case 0b110:
+         nm = "vilvh";
+         assign(res, binop(mkVecINTERLEAVEHI(insSz),
+                           getVReg(vj),
+                           getVReg(vk)));
+         break;
+      case 0b111:
+         nm = "vpickev";
+         assign(res, binop(mkVecPACKEV(insSz),
+                           getVReg(vj),
+                           getVReg(vk)));
+         break;
+      case 0b000:
+         nm = "vpickod";
+         assign(res, binop(mkVecPACKOD(insSz),
+                           getVReg(vj),
+                           getVReg(vk)));
+         break;
+      default:
+         return False;
+   }
+
+   DIP("%s.%s %s, %s, %s\n", nm, mkInsSize(insSz),
+                             nameVReg(vd), nameVReg(vj), nameVReg(vk));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putVReg(vd, mkexpr(res));
+   return True;
+}
+
+static Bool gen_vshuf_b ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo*  abiinfo )
+{
+   UInt va = SLICE(insn, 19, 15);
+   UInt vk = SLICE(insn, 14, 10);
+   UInt vj = SLICE(insn, 9, 5);
+   UInt vd = SLICE(insn, 4, 0);
+
+   DIP("vshuf.b %s, %s, %s, %s\n", nameVReg(vd), nameVReg(vj), nameVReg(vk),
+                                   nameVReg(va));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRTemp sHi = newTemp(Ity_V128);
+   IRTemp sLo = newTemp(Ity_V128);
+   IRTemp sId = newTemp(Ity_V128);
+   assign(sHi, getVReg(vj));
+   assign(sLo, getVReg(vk));
+   assign(sId, getVReg(va));
+   UInt i;
+   IRTemp id[16], res[16];
+
+   for (i = 0; i < 16; i++) {
+         id[i] = newTemp(Ity_I8);
+         res[i] = newTemp(Ity_I8);
+
+         assign(id[i], binop(Iop_GetElem8x16, mkexpr(sId), mkU8(i)));
+
+         assign(res[i], IRExpr_ITE(
+                           binop(Iop_CmpEQ64,
+                                 extendU(Ity_I8, binop(Iop_And8,
+                                                       mkexpr(id[i]),
+                                                       mkU8(0xC0))),
+                                 mkU64(0x0)),
+                           IRExpr_ITE(
+                              binop(Iop_CmpLT64U,
+                                    extendU(Ity_I8, binop(Iop_And8,
+                                                          mkexpr(id[i]),
+                                                          mkU8(0x1F))),
+                                    mkU64(0x10)),
+                              binop(Iop_GetElem8x16,
+                                    mkexpr(sLo),
+                                    mkexpr(id[i])),
+                              binop(Iop_GetElem8x16,
+                                    mkexpr(sHi),
+                                    unop(Iop_64to8,
+                                         binop(Iop_Sub64,
+                                               extendU(Ity_I8, mkexpr(id[i])),
+                                               mkU64(0x10))))),
+                           mkU8(0x0)));
+   }
+
+   putVReg(vd,
+            binop(Iop_64HLtoV128,
+                  binop(Iop_32HLto64,
+                        binop(Iop_16HLto32,
+                              binop(Iop_8HLto16, mkexpr(res[15]), mkexpr(res[14])),
+                              binop(Iop_8HLto16, mkexpr(res[13]), mkexpr(res[12]))),
+                        binop(Iop_16HLto32,
+                              binop(Iop_8HLto16, mkexpr(res[11]), mkexpr(res[10])),
+                              binop(Iop_8HLto16, mkexpr(res[9]), mkexpr(res[8])))),
+                  binop(Iop_32HLto64,
+                        binop(Iop_16HLto32,
+                              binop(Iop_8HLto16, mkexpr(res[7]), mkexpr(res[6])),
+                              binop(Iop_8HLto16, mkexpr(res[5]), mkexpr(res[4]))),
+                        binop(Iop_16HLto32,
+                              binop(Iop_8HLto16, mkexpr(res[3]), mkexpr(res[2])),
+                              binop(Iop_8HLto16, mkexpr(res[1]), mkexpr(res[0]))))));
+
+   return True;
+}
+
+static Bool gen_xvpermi ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo*  abiinfo )
+{
+   UInt xd    = SLICE(insn, 4, 0);
+   UInt xj    = SLICE(insn, 9, 5);
+   UInt ui8   = SLICE(insn, 17, 10);
+   UInt InsSz = SLICE(insn, 19, 18);
+
+   UInt id0 = ui8 & 0x03;
+   UInt id1 = (ui8 & 0x0c) >> 2;
+   UInt id2 = (ui8 & 0x30) >> 4;
+   UInt id3 = (ui8 & 0xc0) >> 6;
+
+   IRTemp res = newTemp(Ity_V256);
+   IRTemp sJ = newTemp(Ity_V256);
+   assign(sJ, getXReg(xj));
+   IRTemp sD = newTemp(Ity_V256);
+   assign(sD, getXReg(xd));
+
+   switch (InsSz) {
+      case 0b01: {
+         IRTemp s[16];
+         s[7] = s[6] = s[5] = s[4] = s[3] = s[2] = s[1] = s[0] = IRTemp_INVALID;
+         s[15] = s[14] = s[13] = s[12] = s[11] = s[10] = s[9] = s[8] = IRTemp_INVALID;
+         breakupV256to32s(sJ, &s[7], &s[6], &s[5], &s[4],
+                              &s[3], &s[2], &s[1], &s[0]);
+         breakupV256to32s(sD, &s[15], &s[14], &s[13], &s[12],
+                              &s[11], &s[10], &s[9], &s[8]);
+         assign(res, mkV256from32s(s[id0], s[id1], s[id2], s[id3],
+                                   s[id0 + 4], s[id1 + 4], s[id2 + 4], s[id3 + 4]));
+         break;
+      }
+      case 0b10: {
+         IRTemp s[4];
+         s[3] = s[2] = s[1] = s[0] = IRTemp_INVALID;
+         breakupV256to64s(sJ, &s[3], &s[2], &s[1], &s[0]);
+         assign(res, mkV256from64s(s[id0], s[id1], s[id2], s[id3]));
+         break;
+      }
+      case 0b11: {
+         IRTemp s[4];
+         s[3] = s[2] = s[1] = s[0] = IRTemp_INVALID;
+         breakupV256toV128s(sJ, &s[1], &s[0]);
+         breakupV256toV128s(sD, &s[3], &s[2]);
+         assign(res, binop(Iop_V128HLtoV256, mkexpr(s[id2]), mkexpr(s[id0])));
+         break;
+      }
+      default:
+         vassert(0);
+         break;
+   }
+
+   DIP("xvpermi.%s %s, %s, %u\n", mkInsSize(InsSz), nameXReg(xd), nameXReg(xj), ui8);
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LASX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putXReg(xd, mkexpr(res));
+
+   return True;
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Helpers for vector load/store insns                  ---*/
+/*------------------------------------------------------------*/
+
+static Bool gen_vld ( DisResult* dres, UInt insn,
+                      const VexArchInfo* archinfo,
+                      const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   vd = SLICE(insn, 4, 0);
+
+   DIP("vld %s, %s, %d\n", nameVReg(vd), nameIReg(rj),
+                           (Int)extend32(si12, 12));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), mkU64(extend64(si12, 12)));
+   putVReg(vd, load(Ity_V128, addr));
+
+   return True;
+}
+
+static Bool gen_vldrepl ( DisResult* dres, UInt insn,
+                          const VexArchInfo* archinfo,
+                          const VexAbiInfo*  abiinfo )
+{
+   UInt vd     = SLICE(insn, 4, 0);
+   UInt rj     = SLICE(insn, 9, 5);
+   UInt insImm = SLICE(insn, 23, 10);
+
+   UInt sImm, insSz;
+   IRTemp res = newTemp(Ity_V128);
+   IRTemp addr = newTemp(Ity_I64);
+
+   if ((insImm & 0x3000) == 0x2000) {        // 10si12; b
+      sImm = insImm & 0xfff;
+      insSz = 0;
+   } else if ((insImm & 0x3800) == 0x1000) { // 010si11; h
+      sImm = insImm & 0x7ff;
+      insSz = 1;
+   } else if ((insImm & 0x3c00) == 0x800) {  // 0010si10; w
+      sImm = insImm & 0x3ff;
+      insSz = 2;
+   } else if ((insImm & 0x3e00) == 0x400) {  // 00010si9; d
+      sImm = insImm & 0x1ff;
+      insSz = 3;
+   } else {
+      return False;
+   }
+
+   switch (insSz) {
+      case 0b00: {
+         assign(addr, binop(Iop_Add64,
+                            getIReg64(rj),
+                            mkU64(extend64(sImm, 12))));
+         assign(res, unop(Iop_Dup8x16, load(Ity_I8, mkexpr(addr))));
+         break;
+      }
+      case 0b01: {
+         assign(addr, binop(Iop_Add64,
+                            getIReg64(rj),
+                            mkU64(extend64(sImm << 1, 12))));
+         assign(res, unop(Iop_Dup16x8, load(Ity_I16, mkexpr(addr))));
+         break;
+      }
+      case 0b10: {
+         assign(addr, binop(Iop_Add64,
+                            getIReg64(rj),
+                            mkU64(extend64(sImm << 2, 12))));
+         assign(res, unop(Iop_Dup32x4, load(Ity_I32, mkexpr(addr))));
+         break;
+      }
+      case 0b11: {
+         assign(addr, binop(Iop_Add64,
+                            getIReg64(rj),
+                            mkU64(extend64(sImm << 3, 12))));
+         assign(res, binop(Iop_64HLtoV128,
+                           load(Ity_I64, mkexpr(addr)),
+                           load(Ity_I64, mkexpr(addr))));
+         break;
+      }
+      default:
+         vassert(0);
+         break;
+   }
+
+   DIP("vldrepl.%s %s, %s, %u\n", mkInsSize(insSz),
+                                  nameVReg(vd), nameIReg(rj), sImm);
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   putVReg(vd, mkexpr(res));
+
+   return True;
+}
+
+static Bool gen_vst ( DisResult* dres, UInt insn,
+                      const VexArchInfo* archinfo,
+                      const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   vd = SLICE(insn, 4, 0);
+
+   DIP("vst %s, %s, %d\n", nameVReg(vd), nameIReg(rj),
+                           (Int)extend32(si12, 12));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), mkU64(extend64(si12, 12)));
+   store(addr, getVReg(vd));
+
+   return True;
+}
+
+static Bool gen_xvld ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   xd = SLICE(insn, 4, 0);
+
+   DIP("xvld %s, %s, %d\n", nameXReg(xd), nameIReg(rj),
+                            (Int)extend32(si12, 12));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LASX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), mkU64(extend64(si12, 12)));
+   putXReg(xd, load(Ity_V256, addr));
+
+   return True;
+}
+
+static Bool gen_xvst ( DisResult* dres, UInt insn,
+                       const VexArchInfo* archinfo,
+                       const VexAbiInfo* abiinfo )
+{
+   UInt si12 = SLICE(insn, 21, 10);
+   UInt   rj = SLICE(insn, 9, 5);
+   UInt   xd = SLICE(insn, 4, 0);
+
+   DIP("xvst %s, %s, %d\n", nameXReg(xd), nameIReg(rj),
+                            (Int)extend32(si12, 12));
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LASX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   IRExpr* addr = binop(Iop_Add64, getIReg64(rj), mkU64(extend64(si12, 12)));
+   store(addr, getXReg(xd));
+
+   return True;
+}
+
+static Bool gen_vstelm ( DisResult* dres, UInt insn,
+                         const VexArchInfo* archinfo,
+                         const VexAbiInfo*  abiinfo )
+{
+   UInt vd     = SLICE(insn, 4, 0);
+   UInt rj     = SLICE(insn, 9, 5);
+   UInt si8    = SLICE(insn, 17, 10);
+   UInt insImm = SLICE(insn, 23, 18);
+
+   IRExpr* addr;
+   UInt idx, insSz;
+
+   if ((insImm & 0x30) == 0x20) {        // 10_idx; b
+      idx = insImm & 0xf;
+      insSz = 0;
+   } else if ((insImm & 0x38) == 0x10) { // 01_idx; h
+      idx = insImm & 0x7;
+      insSz = 1;
+   } else if ((insImm & 0x3c) == 0x8) {  // 001_idx; w
+      idx = insImm & 0x3;
+      insSz = 2;
+   } else if ((insImm & 0x3e) == 0x4) {  // 0001_idx; d
+      idx = insImm & 0x1;
+      insSz = 3;
+   } else {
+      return False;
+   }
+
+   switch (insSz) {
+      case 0b00:
+         addr = binop(Iop_Add64,
+                      getIReg64(rj),
+                      mkU64(extend64(si8, 8)));
+         break;
+      case 0b01:
+         addr = binop(Iop_Add64,
+                      getIReg64(rj),
+                      mkU64(extend64(si8 << 1, 9)));
+         break;
+      case 0b10:
+         addr = binop(Iop_Add64,
+                      getIReg64(rj),
+                      mkU64(extend64(si8 << 2, 10)));
+         break;
+      case 0b11:
+         addr = binop(Iop_Add64,
+                      getIReg64(rj),
+                      mkU64(extend64(si8 << 3, 11)));
+         break;
+      default:
+         vassert(0);
+         break;
+   }
+
+   DIP("vstelm.%s %s, %s, %d, %u\n", mkInsSize(insSz), nameVReg(vd), nameIReg(rj),
+                                     (Int)extend32(si8, 8), idx);
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   store(addr, binop(mkVecGetElem(insSz), getVReg(vd), mkU8(idx)));
+
+   return True;
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Disassemble a single LOONGARCH64 instruction         ---*/
+/*------------------------------------------------------------*/
+
+/* Disassemble a single LOONGARCH64 instruction into IR.  The instruction
+   has is located at |guest_instr| and has guest IP of |guest_PC_curr_instr|,
+   which will have been set before the call here.  Returns True iff the
+   instruction was decoded, in which case *dres will be set accordingly,
+   or False, in which case *dres should be ignored by the caller. */
+
+static Bool disInstr_LOONGARCH64_WRK_special ( DisResult* dres,
+                                               const UChar* guest_instr )
+{
+   const UChar* code = guest_instr;
+   /* Spot the 16-byte preamble:
+      00450c00  srli.d $zero, $zero, 3
+      00453400  srli.d $zero, $zero, 13
+      00457400  srli.d $zero, $zero, 29
+      00454c00  srli.d $zero, $zero, 19
+   */
+   if (getUInt(code +  0) == 0x00450c00 &&
+       getUInt(code +  4) == 0x00453400 &&
+       getUInt(code +  8) == 0x00457400 &&
+       getUInt(code + 12) == 0x00454c00) {
+      /* Got a "Special" instruction preamble.  Which one is it? */
+      if (getUInt(code + 16) == 0x001535ad) {        /* or $t1, $t1, $t1 */
+         DIP("$a7 = client_request ( $t0 )\n");
+         putPC(mkU64(guest_PC_curr_instr + 20));
+         dres->whatNext    = Dis_StopHere;
+         dres->len         = 20;
+         dres->jk_StopHere = Ijk_ClientReq;
+         return True;
+      } else if (getUInt(code + 16) == 0x001539ce) { /* or $t2, $t2, $t2 */
+         DIP("$a7 = guest_NRADDR\n");
+         putIReg(11, IRExpr_Get(offsetof(VexGuestLOONGARCH64State, guest_NRADDR),
+                     Ity_I64));
+         dres->len = 20;
+         return True;
+      } else if (getUInt(code + 16) == 0x00153def) { /* or $t3, $t3, $t3 */
+         DIP("branch-and-link-to-noredir $t8\n");
+         putIReg(1, mkU64(guest_PC_curr_instr + 20));
+         putPC(getIReg64(20));
+         dres->whatNext    = Dis_StopHere;
+         dres->len         = 20;
+         dres->jk_StopHere = Ijk_NoRedir;
+         return True;
+      } else if (getUInt(code + 16) == 0x00154210) { /* or $t4, $t4, $t4 */
+         DIP("IR injection\n");
+         vex_inject_ir(irsb, Iend_LE);
+         /* Invalidate the current insn. The reason is that the IRop we're
+            injecting here can change. In which case the translation has to
+            be redone. For ease of handling, we simply invalidate all the
+            time.
+          */
+         stmt(IRStmt_Put(offsetof(VexGuestLOONGARCH64State, guest_CMSTART),
+                         mkU64(guest_PC_curr_instr)));
+         stmt(IRStmt_Put(offsetof(VexGuestLOONGARCH64State, guest_CMLEN),
+                         mkU64(20)));
+         putPC(mkU64(guest_PC_curr_instr + 20));
+         dres->whatNext    = Dis_StopHere;
+         dres->len         = 20;
+         dres->jk_StopHere = Ijk_InvalICache;
+         return True;
+      }
+      /* We don't know what it is. */
+      vassert(0);
+      /*NOTREACHED*/
+   }
+   return False;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_00_0000_0000 ( DisResult* dres, UInt insn,
+                                                    const VexArchInfo* archinfo,
+                                                    const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 21, 15)) {
+      case 0b0000000:
+         switch (SLICE(insn, 14, 10)) {
+            case 0b00100:
+               ok = gen_clo_w(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b00101:
+               ok = gen_clz_w(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b00110:
+               ok = gen_cto_w(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b00111:
+               ok = gen_ctz_w(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01000:
+               ok = gen_clo_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01001:
+               ok = gen_clz_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01010:
+               ok = gen_cto_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01011:
+               ok = gen_ctz_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01100:
+               ok = gen_revb_2h(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01101:
+               ok = gen_revb_4h(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01110:
+               ok = gen_revb_2w(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01111:
+               ok = gen_revb_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b10000:
+               ok = gen_revh_2w(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b10001:
+               ok = gen_revh_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b10010:
+               ok = gen_bitrev_4b(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b10011:
+               ok = gen_bitrev_8b(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b10100:
+               ok = gen_bitrev_w(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b10101:
+               ok = gen_bitrev_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b10110:
+               ok = gen_ext_w_h(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b10111:
+               ok = gen_ext_w_b(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b11000:
+               ok = gen_rdtimel_w(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b11001:
+               ok = gen_rdtimeh_w(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b11010:
+               ok = gen_rdtime_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b11011:
+               ok = gen_cpucfg(dres, insn, archinfo, abiinfo);
+               break;
+            default:
+               ok = False;
+               break;
+         }
+         break;
+      case 0b0000010:
+         ok = gen_asrtle_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0000011:
+         ok = gen_asrtgt_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0100000:
+         ok = gen_add_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0100001:
+         ok = gen_add_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0100010:
+         ok = gen_sub_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0100011:
+         ok = gen_sub_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0100100:
+         ok = gen_slt(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0100101:
+         ok = gen_sltu(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0100110:
+         ok = gen_maskeqz(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0100111:
+         ok = gen_masknez(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0101000:
+         ok = gen_nor(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0101001:
+         ok = gen_and(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0101010:
+         ok = gen_or(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0101011:
+         ok = gen_xor(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0101100:
+         ok = gen_orn(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0101101:
+         ok = gen_andn(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0101110:
+         ok = gen_sll_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0101111:
+         ok = gen_srl_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0110000:
+         ok = gen_sra_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0110001:
+         ok = gen_sll_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0110010:
+         ok = gen_srl_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0110011:
+         ok = gen_sra_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0110110:
+         ok = gen_rotr_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0110111:
+         ok = gen_rotr_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0111000:
+         ok = gen_mul_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0111001:
+         ok = gen_mulh_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0111010:
+         ok = gen_mulh_wu(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0111011:
+         ok = gen_mul_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0111100:
+         ok = gen_mulh_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0111101:
+         ok = gen_mulh_du(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0111110:
+         ok = gen_mulw_d_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0111111:
+         ok = gen_mulw_d_wu(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1000000:
+         ok = gen_div_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1000001:
+         ok = gen_mod_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1000010:
+         ok = gen_div_wu(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1000011:
+         ok = gen_mod_wu(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1000100:
+         ok = gen_div_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1000101:
+         ok = gen_mod_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1000110:
+         ok = gen_div_du(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1000111:
+         ok = gen_mod_du(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1001000:
+         ok = gen_crc_w_b_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1001001:
+         ok = gen_crc_w_h_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1001010:
+         ok = gen_crc_w_w_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1001011:
+         ok = gen_crc_w_d_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1001100:
+         ok = gen_crcc_w_b_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1001101:
+         ok = gen_crcc_w_h_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1001110:
+         ok = gen_crcc_w_w_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1001111:
+         ok = gen_crcc_w_d_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1010100:
+         ok = gen_break(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1010110:
+         ok = gen_syscall(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   if (ok)
+      return ok;
+
+   switch (SLICE(insn, 21, 18)) {
+      case 0b0001:
+         if (SLICE(insn, 17, 17) == 0) {
+            ok = gen_alsl_w(dres, insn, archinfo, abiinfo);
+         } else {
+            ok = gen_alsl_wu(dres, insn, archinfo, abiinfo);
+         }
+         break;
+      case 0b0010:
+         if (SLICE(insn, 17, 17) == 0) {
+            ok = gen_bytepick_w(dres, insn, archinfo, abiinfo);
+         } else {
+            ok = False;
+         }
+         break;
+      case 0b0011:
+         ok = gen_bytepick_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1011:
+         if (SLICE(insn, 17, 17) == 0) {
+            ok = gen_alsl_d(dres, insn, archinfo, abiinfo);
+         } else {
+            ok = False;
+         }
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_00_0000_0001 ( DisResult* dres, UInt insn,
+                                                    const VexArchInfo* archinfo,
+                                                    const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   if (SLICE(insn, 21, 21) == 0) {
+      switch (SLICE(insn, 20, 16)) {
+         case 0b00000:
+            if (SLICE(insn, 15, 15) == 1) {
+               ok = gen_slli_w(dres, insn, archinfo, abiinfo);
+            } else {
+               ok = False;
+            }
+            break;
+         case 0b00001:
+            ok = gen_slli_d(dres, insn, archinfo, abiinfo);
+            break;
+         case 0b00100:
+            if (SLICE(insn, 15, 15) == 1) {
+               ok = gen_srli_w(dres, insn, archinfo, abiinfo);
+            } else {
+               ok = False;
+            }
+            break;
+         case 0b00101:
+            ok = gen_srli_d(dres, insn, archinfo, abiinfo);
+            break;
+         case 0b01000:
+            if (SLICE(insn, 15, 15) == 1) {
+               ok = gen_srai_w(dres, insn, archinfo, abiinfo);
+            } else {
+               ok = False;
+            }
+            break;
+         case 0b01001:
+            ok = gen_srai_d(dres, insn, archinfo, abiinfo);
+            break;
+         case 0b01100:
+            if (SLICE(insn, 15, 15) == 1) {
+               ok = gen_rotri_w(dres, insn, archinfo, abiinfo);
+            } else {
+               ok = False;
+            }
+            break;
+         case 0b01101:
+            ok = gen_rotri_d(dres, insn, archinfo, abiinfo);
+            break;
+         default:
+            ok = False;
+            break;
+      }
+   } else {
+      if (SLICE(insn, 15, 15) == 0) {
+         ok = gen_bstrins_w(dres, insn, archinfo, abiinfo);
+      } else {
+         ok = gen_bstrpick_w(dres, insn, archinfo, abiinfo);
+      }
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_00_0000_0100 ( DisResult* dres, UInt insn,
+                                                    const VexArchInfo* archinfo,
+                                                    const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 21, 15)) {
+      case 0b0000001:
+         ok = gen_fadd_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0000010:
+         ok = gen_fadd_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0000101:
+         ok = gen_fsub_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0000110:
+         ok = gen_fsub_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0001001:
+         ok = gen_fmul_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0001010:
+         ok = gen_fmul_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0001101:
+         ok = gen_fdiv_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0001110:
+         ok = gen_fdiv_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0010001:
+         ok = gen_fmax_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0010010:
+         ok = gen_fmax_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0010101:
+         ok = gen_fmin_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0010110:
+         ok = gen_fmin_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0011001:
+         ok = gen_fmaxa_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0011010:
+         ok = gen_fmaxa_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0011101:
+         ok = gen_fmina_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0011110:
+         ok = gen_fmina_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0100001:
+         ok = gen_fscaleb_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0100010:
+         ok = gen_fscaleb_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0100101:
+         ok = gen_fcopysign_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0100110:
+         ok = gen_fcopysign_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0101000:
+         switch (SLICE(insn, 14, 10)) {
+            case 0b00001:
+               ok = gen_fabs_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b00010:
+               ok = gen_fabs_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b00101:
+               ok = gen_fneg_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b00110:
+               ok = gen_fneg_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01001:
+               ok = gen_flogb_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01010:
+               ok = gen_flogb_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01101:
+               ok = gen_fclass_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01110:
+               ok = gen_fclass_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b10001:
+               ok = gen_fsqrt_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b10010:
+               ok = gen_fsqrt_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b10101:
+               ok = gen_frecip_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b10110:
+               ok = gen_frecip_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b11001:
+               ok = gen_frsqrt_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b11010:
+               ok = gen_frsqrt_d(dres, insn, archinfo, abiinfo);
+               break;
+            default:
+               ok = False;
+               break;
+         }
+         break;
+      case 0b0101001:
+         switch (SLICE(insn, 14, 10)) {
+            case 0b00101:
+               ok = gen_fmov_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b00110:
+               ok = gen_fmov_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01001:
+               ok = gen_movgr2fr_w(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01010:
+               ok = gen_movgr2fr_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01011:
+               ok = gen_movgr2frh_w(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01101:
+               ok = gen_movfr2gr_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01110:
+               ok = gen_movfr2gr_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01111:
+               ok = gen_movfrh2gr_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b10000:
+               ok = gen_movgr2fcsr(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b10010:
+               ok = gen_movfcsr2gr(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b10100:
+               if (SLICE(insn, 4, 3) == 0b00) {
+                  ok = gen_movfr2cf(dres, insn, archinfo, abiinfo);
+               } else {
+                  ok = False;
+               }
+               break;
+            case 0b10101:
+               if (SLICE(insn, 9, 8) == 0b00) {
+                  ok = gen_movcf2fr(dres, insn, archinfo, abiinfo);
+               } else {
+                  ok = False;
+               }
+               break;
+            case 0b10110:
+               if (SLICE(insn, 4, 3) == 0b00) {
+                  ok = gen_movgr2cf(dres, insn, archinfo, abiinfo);
+               } else {
+                  ok = False;
+               }
+               break;
+            case 0b10111:
+               if (SLICE(insn, 9, 8) == 0b00) {
+                  ok = gen_movcf2gr(dres, insn, archinfo, abiinfo);
+               } else {
+                  ok = False;
+               }
+               break;
+            default:
+               ok = False;
+               break;
+         }
+         break;
+      case 0b0110010:
+         switch (SLICE(insn, 14, 10)) {
+            case 0b00110:
+               ok = gen_fcvt_s_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01001:
+               ok = gen_fcvt_d_s(dres, insn, archinfo, abiinfo);
+               break;
+            default:
+               ok = False;
+               break;
+         }
+         break;
+      case 0b0110100:
+         switch (SLICE(insn, 14, 10)) {
+            case 0b00001:
+               ok = gen_ftintrm_w_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b00010:
+               ok = gen_ftintrm_w_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01001:
+               ok = gen_ftintrm_l_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01010:
+               ok = gen_ftintrm_l_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b10001:
+               ok = gen_ftintrp_w_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b10010:
+               ok = gen_ftintrp_w_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b11001:
+               ok = gen_ftintrp_l_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b11010:
+               ok = gen_ftintrp_l_d(dres, insn, archinfo, abiinfo);
+               break;
+            default:
+               ok = False;
+               break;
+         }
+         break;
+      case 0b0110101:
+         switch (SLICE(insn, 14, 10)) {
+            case 0b00001:
+               ok = gen_ftintrz_w_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b00010:
+               ok = gen_ftintrz_w_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01001:
+               ok = gen_ftintrz_l_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01010:
+               ok = gen_ftintrz_l_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b10001:
+               ok = gen_ftintrne_w_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b10010:
+               ok = gen_ftintrne_w_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b11001:
+               ok = gen_ftintrne_l_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b11010:
+               ok = gen_ftintrne_l_d(dres, insn, archinfo, abiinfo);
+               break;
+            default:
+               ok = False;
+               break;
+         }
+         break;
+      case 0b0110110:
+         switch (SLICE(insn, 14, 10)) {
+            case 0b00001:
+               ok = gen_ftint_w_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b00010:
+               ok = gen_ftint_w_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01001:
+               ok = gen_ftint_l_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01010:
+               ok = gen_ftint_l_d(dres, insn, archinfo, abiinfo);
+               break;
+            default:
+               ok = False;
+               break;
+         }
+         break;
+      case 0b0111010:
+         switch (SLICE(insn, 14, 10)) {
+            case 0b00100:
+               ok = gen_ffint_s_w(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b00110:
+               ok = gen_ffint_s_l(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01000:
+               ok = gen_ffint_d_w(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01010:
+               ok = gen_ffint_d_l(dres, insn, archinfo, abiinfo);
+               break;
+            default:
+               ok = False;
+               break;
+         }
+         break;
+      case 0b0111100:
+         switch (SLICE(insn, 14, 10)) {
+            case 0b10001:
+               ok = gen_frint_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b10010:
+               ok = gen_frint_d(dres, insn, archinfo, abiinfo);
+               break;
+            default:
+               ok = False;
+               break;
+         }
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_00_0000 ( DisResult* dres, UInt insn,
+                                               const VexArchInfo* archinfo,
+                                               const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 25, 22)) {
+      case 0b0000:
+         ok = disInstr_LOONGARCH64_WRK_00_0000_0000(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0001:
+         ok = disInstr_LOONGARCH64_WRK_00_0000_0001(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0010:
+         ok = gen_bstrins_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0011:
+         ok = gen_bstrpick_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0100:
+         ok = disInstr_LOONGARCH64_WRK_00_0000_0100(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1000:
+         ok = gen_slti(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1001:
+         ok = gen_sltui(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1010:
+         ok = gen_addi_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1011:
+         ok = gen_addi_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1100:
+         ok = gen_lu52i_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1101:
+         ok = gen_andi(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1110:
+         ok = gen_ori(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1111:
+         ok = gen_xori(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_00_1010 ( DisResult* dres, UInt insn,
+                                               const VexArchInfo* archinfo,
+                                               const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 25, 22)) {
+      case 0b0000:
+         ok = gen_ld_b(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0001:
+         ok = gen_ld_h(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0010:
+         ok = gen_ld_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0011:
+         ok = gen_ld_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0100:
+         ok = gen_st_b(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0101:
+         ok = gen_st_h(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0110:
+         ok = gen_st_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0111:
+         ok = gen_st_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1000:
+         ok = gen_ld_bu(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1001:
+         ok = gen_ld_hu(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1010:
+         ok = gen_ld_wu(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1011:
+         ok = gen_preld(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1100:
+         ok = gen_fld_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1101:
+         ok = gen_fst_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1110:
+         ok = gen_fld_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1111:
+         ok = gen_fst_d(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_00_1011 ( DisResult* dres, UInt insn,
+                                               const VexArchInfo* archinfo,
+                                               const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 23, 22)) {
+      case 0b00:
+         ok = gen_vld(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b01:
+         ok = gen_vst(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b10:
+         ok = gen_xvld(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b11:
+         ok = gen_xvst(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_00_1100 ( DisResult* dres, UInt insn,
+                                               const VexArchInfo* archinfo,
+                                               const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   if (!(archinfo->hwcaps & VEX_HWCAPS_LOONGARCH_LSX)) {
+      dres->jk_StopHere = Ijk_SigILL;
+      dres->whatNext    = Dis_StopHere;
+      return True;
+   }
+
+   switch (SLICE(insn, 25, 24)) {
+      case 0b00:
+         ok = gen_vldrepl(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b01:
+         ok = gen_vstelm(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_00_1110_0000 ( DisResult* dres, UInt insn,
+                                                    const VexArchInfo* archinfo,
+                                                    const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 21, 15)) {
+      case 0b0000000:
+         ok = gen_ldx_b(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0001000:
+         ok = gen_ldx_h(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0010000:
+         ok = gen_ldx_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0011000:
+         ok = gen_ldx_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0100000:
+         ok = gen_stx_b(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0101000:
+         ok = gen_stx_h(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0110000:
+         ok = gen_stx_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0111000:
+         ok = gen_stx_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1000000:
+         ok = gen_ldx_bu(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1001000:
+         ok = gen_ldx_hu(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1010000:
+         ok = gen_ldx_wu(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1011000:
+         ok = gen_preldx(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1100000:
+         ok = gen_fldx_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1101000:
+         ok = gen_fldx_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1110000:
+         ok = gen_fstx_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1111000:
+         ok = gen_fstx_d(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_00_1110_0001 ( DisResult* dres, UInt insn,
+                                                    const VexArchInfo* archinfo,
+                                                    const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 21, 15)) {
+      case 0b1000000:
+         ok = gen_amswap_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1000001:
+         ok = gen_amswap_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1000010:
+         ok = gen_amadd_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1000011:
+         ok = gen_amadd_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1000100:
+         ok = gen_amand_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1000101:
+         ok = gen_amand_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1000110:
+         ok = gen_amor_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1000111:
+         ok = gen_amor_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1001000:
+         ok = gen_amxor_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1001001:
+         ok = gen_amxor_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1001010:
+         ok = gen_ammax_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1001011:
+         ok = gen_ammax_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1001100:
+         ok = gen_ammin_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1001101:
+         ok = gen_ammin_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1001110:
+         ok = gen_ammax_wu(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1001111:
+         ok = gen_ammax_du(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1010000:
+         ok = gen_ammin_wu(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1010001:
+         ok = gen_ammin_du(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1010010:
+         ok = gen_amswap_db_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1010011:
+         ok = gen_amswap_db_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1010100:
+         ok = gen_amadd_db_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1010101:
+         ok = gen_amadd_db_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1010110:
+         ok = gen_amand_db_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1010111:
+         ok = gen_amand_db_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1011000:
+         ok = gen_amor_db_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1011001:
+         ok = gen_amor_db_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1011010:
+         ok = gen_amxor_db_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1011011:
+         ok = gen_amxor_db_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1011100:
+         ok = gen_ammax_db_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1011101:
+         ok = gen_ammax_db_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1011110:
+         ok = gen_ammin_db_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1011111:
+         ok = gen_ammin_db_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1100000:
+         ok = gen_ammax_db_wu(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1100001:
+         ok = gen_ammax_db_du(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1100010:
+         ok = gen_ammin_db_wu(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1100011:
+         ok = gen_ammin_db_du(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1100100:
+         ok = gen_dbar(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1100101:
+         ok = gen_ibar(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1101000:
+         ok = gen_fldgt_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1101001:
+         ok = gen_fldgt_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1101010:
+         ok = gen_fldle_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1101011:
+         ok = gen_fldle_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1101100:
+         ok = gen_fstgt_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1101101:
+         ok = gen_fstgt_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1101110:
+         ok = gen_fstle_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1101111:
+         ok = gen_fstle_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1110000:
+         ok = gen_ldgt_b(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1110001:
+         ok = gen_ldgt_h(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1110010:
+         ok = gen_ldgt_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1110011:
+         ok = gen_ldgt_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1110100:
+         ok = gen_ldle_b(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1110101:
+         ok = gen_ldle_h(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1110110:
+         ok = gen_ldle_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1110111:
+         ok = gen_ldle_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1111000:
+         ok = gen_stgt_b(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1111001:
+         ok = gen_stgt_h(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1111010:
+         ok = gen_stgt_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1111011:
+         ok = gen_stgt_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1111100:
+         ok = gen_stle_b(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1111101:
+         ok = gen_stle_h(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1111110:
+         ok = gen_stle_w(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1111111:
+         ok = gen_stle_d(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_FCMP_S ( DisResult* dres, UInt insn,
+                                              const VexArchInfo* archinfo,
+                                              const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 19, 15)) {
+      case 0x0:
+         ok = gen_fcmp_caf_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x1:
+         ok = gen_fcmp_saf_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x2:
+         ok = gen_fcmp_clt_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x3:
+         ok = gen_fcmp_slt_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x4:
+         ok = gen_fcmp_ceq_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x5:
+         ok = gen_fcmp_seq_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x6:
+         ok = gen_fcmp_cle_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x7:
+         ok = gen_fcmp_sle_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x8:
+         ok = gen_fcmp_cun_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x9:
+         ok = gen_fcmp_sun_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0xa:
+         ok = gen_fcmp_cult_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0xb:
+         ok = gen_fcmp_sult_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0xc:
+         ok = gen_fcmp_cueq_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0xd:
+         ok = gen_fcmp_sueq_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0xe:
+         ok = gen_fcmp_cule_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0xf:
+         ok = gen_fcmp_sule_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x10:
+         ok = gen_fcmp_cne_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x11:
+         ok = gen_fcmp_sne_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x14:
+         ok = gen_fcmp_cor_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x15:
+         ok = gen_fcmp_sor_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x18:
+         ok = gen_fcmp_cune_s(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x19:
+         ok = gen_fcmp_sune_s(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_FCMP_D ( DisResult* dres, UInt insn,
+                                              const VexArchInfo* archinfo,
+                                              const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 19, 15)) {
+      case 0x0:
+         ok = gen_fcmp_caf_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x1:
+         ok = gen_fcmp_saf_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x2:
+         ok = gen_fcmp_clt_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x3:
+         ok = gen_fcmp_slt_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x4:
+         ok = gen_fcmp_ceq_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x5:
+         ok = gen_fcmp_seq_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x6:
+         ok = gen_fcmp_cle_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x7:
+         ok = gen_fcmp_sle_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x8:
+         ok = gen_fcmp_cun_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x9:
+         ok = gen_fcmp_sun_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0xa:
+         ok = gen_fcmp_cult_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0xb:
+         ok = gen_fcmp_sult_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0xc:
+         ok = gen_fcmp_cueq_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0xd:
+         ok = gen_fcmp_sueq_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0xe:
+         ok = gen_fcmp_cule_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0xf:
+         ok = gen_fcmp_sule_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x10:
+         ok = gen_fcmp_cne_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x11:
+         ok = gen_fcmp_sne_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x14:
+         ok = gen_fcmp_cor_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x15:
+         ok = gen_fcmp_sor_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x18:
+         ok = gen_fcmp_cune_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0x19:
+         ok = gen_fcmp_sune_d(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_00 ( DisResult* dres, UInt insn,
+                                          const VexArchInfo* archinfo,
+                                          const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 29, 26)) {
+      case 0b0000:
+         ok = disInstr_LOONGARCH64_WRK_00_0000(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0010:
+         switch (SLICE(insn, 25, 20)) {
+            case 0b000001:
+               ok = gen_fmadd_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b000010:
+               ok = gen_fmadd_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b000101:
+               ok = gen_fmsub_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b000110:
+               ok = gen_fmsub_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b001001:
+               ok = gen_fnmadd_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b001010:
+               ok = gen_fnmadd_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b001101:
+               ok = gen_fnmsub_s(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b001110:
+               ok = gen_fnmsub_d(dres, insn, archinfo, abiinfo);
+               break;
+            default:
+               ok = False;
+               break;
+         }
+         break;
+      case 0b0011:
+         switch (SLICE(insn, 25, 20)) {
+            case 0b000001:
+               if (SLICE(insn, 4, 3) == 0b00) {
+                  ok = disInstr_LOONGARCH64_WRK_FCMP_S(dres, insn, archinfo, abiinfo);
+               } else {
+                  ok = False;
+               }
+               break;
+            case 0b000010:
+               if (SLICE(insn, 4, 3) == 0b00) {
+                  ok = disInstr_LOONGARCH64_WRK_FCMP_D(dres, insn, archinfo, abiinfo);
+               } else {
+                  ok = False;
+               }
+               break;
+            case 0b010000:
+               if (SLICE(insn, 19, 18) == 0b00) {
+                  ok = gen_fsel(dres, insn, archinfo, abiinfo);
+               } else {
+                  ok = False;
+               }
+               break;
+            case 0b010101:
+               ok = gen_vshuf_b(dres, insn, archinfo, abiinfo);
+               break;
+            default:
+               ok = False;
+               break;
+         }
+         break;
+      case 0b0100:
+         ok = gen_addu16i_d(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0101:
+         if (SLICE(insn, 25, 25) == 0) {
+            ok = gen_lu12i_w(dres, insn, archinfo, abiinfo);
+         } else {
+            ok = gen_lu32i_d(dres, insn, archinfo, abiinfo);
+         }
+         break;
+      case 0b0110:
+         if (SLICE(insn, 25, 25) == 0) {
+            ok = gen_pcaddi(dres, insn, archinfo, abiinfo);
+         } else {
+            ok = gen_pcalau12i(dres, insn, archinfo, abiinfo);
+         }
+         break;
+      case 0b0111:
+         if (SLICE(insn, 25, 25) == 0) {
+            ok = gen_pcaddu12i(dres, insn, archinfo, abiinfo);
+         } else {
+            ok = gen_pcaddu18i(dres, insn, archinfo, abiinfo);
+         }
+         break;
+      case 0b1000:
+         switch (SLICE(insn, 25, 24)) {
+            case 0b00:
+               ok = gen_ll_w(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01:
+               ok = gen_sc_w(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b10:
+               ok = gen_ll_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b11:
+               ok = gen_sc_d(dres, insn, archinfo, abiinfo);
+               break;
+            default:
+               ok = False;
+               break;
+         }
+         break;
+      case 0b1001:
+         switch (SLICE(insn, 25, 24)) {
+            case 0b00:
+               ok = gen_ldptr_w(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01:
+               ok = gen_stptr_w(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b10:
+               ok = gen_ldptr_d(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b11:
+               ok = gen_stptr_d(dres, insn, archinfo, abiinfo);
+               break;
+            default:
+               ok = False;
+               break;
+         }
+         break;
+      case 0b1010:
+         ok = disInstr_LOONGARCH64_WRK_00_1010(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1011:
+         ok = disInstr_LOONGARCH64_WRK_00_1011(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1100:
+         ok = disInstr_LOONGARCH64_WRK_00_1100(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1110:
+         switch (SLICE(insn, 25, 22)) {
+            case 0b0000:
+               ok = disInstr_LOONGARCH64_WRK_00_1110_0000(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b0001:
+               ok = disInstr_LOONGARCH64_WRK_00_1110_0001(dres, insn, archinfo, abiinfo);
+               break;
+            default:
+               ok = False;
+               break;
+         }
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_01_1100_0000 ( DisResult* dres, UInt insn,
+                                                    const VexArchInfo* archinfo,
+                                                    const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 21, 17)) {
+      case 0b00000:
+      case 0b00001:
+      case 0b00010:
+      case 0b00011:
+      case 0b00100:
+         ok = gen_vcmp_integer(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b00101:
+      case 0b00110:
+         ok = gen_vadd_vsub(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_01_1100_0001 ( DisResult* dres, UInt insn,
+                                                    const VexArchInfo* archinfo,
+                                                    const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 21, 18)) {
+      case 0b1100:
+      case 0b1101:
+         ok = gen_vmax_vmin(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_01_1100_0100 ( DisResult* dres, UInt insn,
+                                                    const VexArchInfo* archinfo,
+                                                    const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 21, 17)) {
+      case 0b01011:
+      case 0b01100:
+      case 0b01101:
+      case 0b01110:
+      case 0b01111:
+      case 0b10000:
+         ok = gen_evod(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b10001:
+         ok = gen_vreplve(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b10011:
+      case 0b10100:
+         ok = gen_logical_v(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_01_1100_1010_01110 ( DisResult* dres, UInt insn,
+                                                          const VexArchInfo* archinfo,
+                                                          const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 16, 14)) {
+      case 0b001:
+         ok = gen_vmsk(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b010:
+         ok = gen_vset(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+
+static Bool disInstr_LOONGARCH64_WRK_01_1100_1010_01111 ( DisResult* dres, UInt insn,
+                                                          const VexArchInfo* archinfo,
+                                                          const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 16, 14)) {
+      case 0b100:
+         ok = gen_vreplgr2vr(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_01_1100_1010 ( DisResult* dres, UInt insn,
+                                                    const VexArchInfo* archinfo,
+                                                    const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 21, 17)) {
+      case 0b00000:
+      case 0b00001:
+      case 0b00010:
+      case 0b00011:
+      case 0b00100:
+         ok = gen_vcmpi_integer(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b00101:
+      case 0b00110:
+         ok = gen_vaddi_vsubi(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b01101:
+         ok = gen_vfrstpi(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b01110:
+         ok = disInstr_LOONGARCH64_WRK_01_1100_1010_01110(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b01111:
+         ok = disInstr_LOONGARCH64_WRK_01_1100_1010_01111(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_01_1100_1011 ( DisResult* dres, UInt insn,
+                                                    const VexArchInfo* archinfo,
+                                                    const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 21, 16)) {
+      case 0b101111:
+      case 0b110011:
+         ok = gen_vpickve2gr(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_01_1100_1100 ( DisResult* dres, UInt insn,
+                                                    const VexArchInfo* archinfo,
+                                                    const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 21, 18)) {
+      case 0b0100:
+      case 0b0101:
+      case 0b0110:
+         ok = gen_vbiti(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_01_1100_1111 ( DisResult* dres, UInt insn,
+                                                    const VexArchInfo* archinfo,
+                                                    const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 21, 18)) {
+      case 0b0100:
+      case 0b0101:
+      case 0b0110:
+      case 0b0111:
+         ok = gen_vlogical_u8(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_01_1100 ( DisResult* dres, UInt insn,
+                                               const VexArchInfo* archinfo,
+                                               const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 25, 22)) {
+      case 0b0000:
+         ok = disInstr_LOONGARCH64_WRK_01_1100_0000(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0001:
+         ok = disInstr_LOONGARCH64_WRK_01_1100_0001(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0100:
+         ok = disInstr_LOONGARCH64_WRK_01_1100_0100(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1010:
+         ok = disInstr_LOONGARCH64_WRK_01_1100_1010(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1011:
+         ok = disInstr_LOONGARCH64_WRK_01_1100_1011(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1100:
+         ok = disInstr_LOONGARCH64_WRK_01_1100_1100(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1111:
+         ok = disInstr_LOONGARCH64_WRK_01_1100_1111(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_01_1101_0000 ( DisResult* dres, UInt insn,
+                                                    const VexArchInfo* archinfo,
+                                                    const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 21, 18)) {
+      case 0b0000:
+         ok = gen_xvcmp_integer(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_01_1101_0001 ( DisResult* dres, UInt insn,
+                                                    const VexArchInfo* archinfo,
+                                                    const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 21, 18)) {
+      case 0b1101:
+         ok = gen_xvmax_xvmin(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_01_1101_0100 ( DisResult* dres, UInt insn,
+                                                    const VexArchInfo* archinfo,
+                                                    const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 21, 18)) {
+      case 0b1001:
+         ok = gen_logical_xv(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_01_1101_1010_0111 ( DisResult* dres, UInt insn,
+                                                         const VexArchInfo* archinfo,
+                                                         const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 17, 14)) {
+      case 0b0001:
+         ok = gen_xvmsk(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0010:
+         ok = gen_xvset(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1100:
+         ok = gen_xvreplgr2vr(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_01_1101_1010 ( DisResult* dres, UInt insn,
+                                                    const VexArchInfo* archinfo,
+                                                    const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 21, 18)) {
+      case 0b0111:
+         ok = disInstr_LOONGARCH64_WRK_01_1101_1010_0111(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_01_1101_1100 ( DisResult* dres, UInt insn,
+                                                    const VexArchInfo* archinfo,
+                                                    const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 21, 18)) {
+      case 0b0000:
+         ok = gen_xvpickve(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_01_1101_1111 ( DisResult* dres, UInt insn,
+                                                    const VexArchInfo* archinfo,
+                                                    const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 21, 18)) {
+      case 0b1001:
+      case 0b1010:
+      case 0b1011:
+         ok = gen_xvpermi(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_01_1101 ( DisResult* dres, UInt insn,
+                                               const VexArchInfo* archinfo,
+                                               const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 25, 22)) {
+      case 0b0000:
+         ok = disInstr_LOONGARCH64_WRK_01_1101_0000(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0001:
+         ok = disInstr_LOONGARCH64_WRK_01_1101_0001(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0100:
+         ok = disInstr_LOONGARCH64_WRK_01_1101_0100(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1010:
+         ok = disInstr_LOONGARCH64_WRK_01_1101_1010(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1100:
+         ok = disInstr_LOONGARCH64_WRK_01_1101_1100(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1111:
+         ok = disInstr_LOONGARCH64_WRK_01_1101_1111(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK_01 ( DisResult* dres, UInt insn,
+                                          const VexArchInfo* archinfo,
+                                          const VexAbiInfo*  abiinfo )
+{
+   Bool ok;
+
+   switch (SLICE(insn, 29, 26)) {
+      case 0b0000:
+         ok = gen_beqz(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0001:
+         ok = gen_bnez(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0010:
+         switch (SLICE(insn, 9, 8)) {
+            case 0b00:
+               ok = gen_bceqz(dres, insn, archinfo, abiinfo);
+               break;
+            case 0b01:
+               ok = gen_bcnez(dres, insn, archinfo, abiinfo);
+               break;
+            default:
+               ok = False;
+               break;
+         }
+         break;
+      case 0b0011:
+         ok = gen_jirl(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0100:
+         ok = gen_b(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0101:
+         ok = gen_bl(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0110:
+         ok = gen_beq(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b0111:
+         ok = gen_bne(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1000:
+         ok = gen_blt(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1001:
+         ok = gen_bge(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1010:
+         ok = gen_bltu(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1011:
+         ok = gen_bgeu(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1100:
+         ok = disInstr_LOONGARCH64_WRK_01_1100(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b1101:
+         ok = disInstr_LOONGARCH64_WRK_01_1101(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   return ok;
+}
+
+static Bool disInstr_LOONGARCH64_WRK ( /*MB_OUT*/DisResult* dres,
+                                       const UChar* guest_instr,
+                                       const VexArchInfo* archinfo,
+                                       const VexAbiInfo*  abiinfo,
+                                       Bool sigill_diag )
+{
+   /* Set result defaults. */
+   dres->whatNext    = Dis_Continue;
+   dres->len         = 4;
+   dres->jk_StopHere = Ijk_INVALID;
+   dres->hint        = Dis_HintNone;
+
+   /* At least this is simple on LOONGARCH64: insns are all 4 bytes long,
+      and 4-aligned.  So just fish the whole thing out of memory right now
+      and have done. */
+   UInt insn = getUInt(guest_instr);
+   DIP("\t0x%llx:\t0x%08x\t", (Addr64)guest_PC_curr_instr, insn);
+   vassert((guest_PC_curr_instr & 3ULL) == 0);
+
+   /* Spot "Special" instructions (see comment at top of file). */
+   Bool ok = disInstr_LOONGARCH64_WRK_special(dres, guest_instr);
+   if (ok)
+      return ok;
+
+   /* Main LOONGARCH64 instruction decoder starts here. */
+   switch (SLICE(insn, 31, 30)) {
+      case 0b00:
+         ok = disInstr_LOONGARCH64_WRK_00(dres, insn, archinfo, abiinfo);
+         break;
+      case 0b01:
+         ok = disInstr_LOONGARCH64_WRK_01(dres, insn, archinfo, abiinfo);
+         break;
+      default:
+         ok = False;
+         break;
+   }
+
+   /* If the next-level down decoders failed, make sure |dres| didn't
+      get changed. */
+   if (!ok) {
+      vassert(dres->whatNext    == Dis_Continue);
+      vassert(dres->len         == 4);
+      vassert(dres->jk_StopHere == Ijk_INVALID);
+   }
+   return ok;
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Top-level fn                                         ---*/
+/*------------------------------------------------------------*/
+
+/* Disassemble a single instruction into IR.  The instruction
+   is located in host memory at &guest_code[delta]. */
+
+DisResult disInstr_LOONGARCH64 ( IRSB*              irsb_IN,
+                                 const UChar*       guest_code_IN,
+                                 Long               delta_IN,
+                                 Addr               guest_IP,
+                                 VexArch            guest_arch,
+                                 const VexArchInfo* archinfo,
+                                 const VexAbiInfo*  abiinfo,
+                                 VexEndness         host_endness_IN,
+                                 Bool               sigill_diag_IN )
+{
+   DisResult dres;
+   vex_bzero(&dres, sizeof(dres));
+
+   /* Set globals (see top of this file) */
+   vassert(guest_arch == VexArchLOONGARCH64);
+
+   irsb                = irsb_IN;
+   host_endness        = host_endness_IN;
+   guest_PC_curr_instr = (Addr64)guest_IP;
+
+   /* Try to decode */
+   Bool ok = disInstr_LOONGARCH64_WRK(&dres,
+                                      &guest_code_IN[delta_IN],
+                                      archinfo, abiinfo, sigill_diag_IN);
+
+   if (ok) {
+      /* All decode successes end up here. */
+      vassert(dres.len == 4 || dres.len == 20);
+      switch (dres.whatNext) {
+         case Dis_Continue:
+            putPC(mkU64(dres.len + guest_PC_curr_instr));
+            break;
+         case Dis_StopHere:
+            break;
+         default:
+            vassert(0);
+            break;
+      }
+      DIP("\n");
+   } else {
+      /* All decode failures end up here. */
+      if (sigill_diag_IN) {
+         Int   i, j;
+         UChar buf[64];
+         UInt  insn = getUInt(&guest_code_IN[delta_IN]);
+         vex_bzero(buf, sizeof(buf));
+         for (i = j = 0; i < 32; i++) {
+            if (i > 0 && (i & 3) == 0)
+               buf[j++] = ' ';
+            buf[j++] = (insn & (1 << (31 - i))) ? '1' : '0';
+         }
+         vex_printf("disInstr(loongarch64): unhandled instruction 0x%08x\n", insn);
+         vex_printf("disInstr(loongarch64): %s\n", buf);
+      }
+
+      /* Tell the dispatcher that this insn cannot be decoded, and so
+         has not been executed, and (is currently) the next to be
+         executed.  PC should be up-to-date since it is made so at the
+         start of each insn, but nevertheless be paranoid and update
+         it again right now. */
+      putPC(mkU64(guest_PC_curr_instr));
+      dres.len         = 0;
+      dres.whatNext    = Dis_StopHere;
+      dres.jk_StopHere = Ijk_NoDecode;
+   }
+
+   return dres;
+}
+
+
+/*--------------------------------------------------------------------*/
+/*--- end                                 guest_loongarch64_toIR.c ---*/
+/*--------------------------------------------------------------------*/
diff --git a/VEX/priv/host_loongarch64_defs.c b/VEX/priv/host_loongarch64_defs.c
new file mode 100644
index 0000000..6b50970
--- /dev/null
+++ b/VEX/priv/host_loongarch64_defs.c
@@ -0,0 +1,3929 @@
+
+/*---------------------------------------------------------------*/
+/*--- begin                           host_loongarch64_defs.c ---*/
+/*---------------------------------------------------------------*/
+
+/*
+   This file is part of Valgrind, a dynamic binary instrumentation
+   framework.
+
+   Copyright (C) 2021-2022 Loongson Technology Corporation Limited
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, see <http://www.gnu.org/licenses/>.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "libvex_basictypes.h"
+#include "libvex.h"
+#include "libvex_trc_values.h"
+
+#include "main_util.h"
+#include "host_generic_regs.h"
+#include "host_loongarch64_defs.h"
+
+
+/* --------- Local helpers. --------- */
+
+static inline void mapReg ( HRegRemap* m, HReg* r )
+{
+   *r = lookupHRegRemap(m, *r);
+}
+
+static inline Int extend ( UInt imm, UInt size )
+{
+   UInt shift = 32 - size;
+   return (((Int)imm << shift) >> shift);
+}
+
+
+/* --------- Registers. --------- */
+
+const RRegUniverse* getRRegUniverse_LOONGARCH64 ( void )
+{
+   /* The real-register universe is a big constant, so we just want to
+      initialise it once. */
+   static RRegUniverse rRegUniverse_LOONGARCH64;
+   static Bool         rRegUniverse_LOONGARCH64_initted = False;
+
+   /* Handy shorthand, nothing more */
+   RRegUniverse* ru = &rRegUniverse_LOONGARCH64;
+
+   /* This isn't thread-safe.  Sigh. */
+   if (LIKELY(rRegUniverse_LOONGARCH64_initted == True))
+      return ru;
+
+   RRegUniverse__init(ru);
+
+   /* Add the registers.  The initial segment of this array must be
+      those available for allocation by reg-alloc, and those that
+      follow are not available for allocation. */
+   ru->allocable_start[HRcInt64] = ru->size;
+   ru->regs[ru->size++] = hregLOONGARCH64_R23();
+   ru->regs[ru->size++] = hregLOONGARCH64_R24();
+   ru->regs[ru->size++] = hregLOONGARCH64_R25();
+   ru->regs[ru->size++] = hregLOONGARCH64_R26();
+   ru->regs[ru->size++] = hregLOONGARCH64_R27();
+   ru->regs[ru->size++] = hregLOONGARCH64_R28();
+   ru->regs[ru->size++] = hregLOONGARCH64_R29();
+   ru->regs[ru->size++] = hregLOONGARCH64_R30();
+   // $r31 is used as guest stack pointer, not available to regalloc.
+
+   // $r12 is used as a chaining/ProfInc/Cmove/genSpill/genReload temporary
+   // $r13 is used as a ProfInc temporary
+   ru->regs[ru->size++] = hregLOONGARCH64_R14();
+   ru->regs[ru->size++] = hregLOONGARCH64_R15();
+   ru->regs[ru->size++] = hregLOONGARCH64_R16();
+   ru->regs[ru->size++] = hregLOONGARCH64_R17();
+   ru->regs[ru->size++] = hregLOONGARCH64_R18();
+   ru->regs[ru->size++] = hregLOONGARCH64_R19();
+   ru->regs[ru->size++] = hregLOONGARCH64_R20();
+   ru->allocable_end[HRcInt64] = ru->size - 1;
+
+   ru->allocable_start[HRcFlt64] = ru->size;
+   ru->regs[ru->size++] = hregLOONGARCH64_F24();
+   ru->regs[ru->size++] = hregLOONGARCH64_F25();
+   ru->regs[ru->size++] = hregLOONGARCH64_F26();
+   ru->regs[ru->size++] = hregLOONGARCH64_F27();
+   ru->regs[ru->size++] = hregLOONGARCH64_F28();
+   ru->regs[ru->size++] = hregLOONGARCH64_F29();
+   ru->regs[ru->size++] = hregLOONGARCH64_F30();
+   ru->regs[ru->size++] = hregLOONGARCH64_F31();
+   ru->allocable_end[HRcFlt64] = ru->size - 1;
+
+   ru->allocable_start[HRcVec128] = ru->size;
+   ru->regs[ru->size++] = hregLOONGARCH64_V24();
+   ru->regs[ru->size++] = hregLOONGARCH64_V25();
+   ru->regs[ru->size++] = hregLOONGARCH64_V26();
+   ru->regs[ru->size++] = hregLOONGARCH64_V27();
+   ru->regs[ru->size++] = hregLOONGARCH64_V28();
+   ru->regs[ru->size++] = hregLOONGARCH64_V29();
+   ru->regs[ru->size++] = hregLOONGARCH64_V30();
+   ru->regs[ru->size++] = hregLOONGARCH64_V31();
+   ru->allocable_end[HRcVec128] = ru->size - 1;
+
+   ru->allocable = ru->size;
+
+   /* And other regs, not available to the allocator. */
+   ru->regs[ru->size++] = hregLOONGARCH64_R0();
+   ru->regs[ru->size++] = hregLOONGARCH64_R1();
+   ru->regs[ru->size++] = hregLOONGARCH64_R2();
+   ru->regs[ru->size++] = hregLOONGARCH64_R3();
+   ru->regs[ru->size++] = hregLOONGARCH64_R4();
+   ru->regs[ru->size++] = hregLOONGARCH64_R5();
+   ru->regs[ru->size++] = hregLOONGARCH64_R6();
+   ru->regs[ru->size++] = hregLOONGARCH64_R7();
+   ru->regs[ru->size++] = hregLOONGARCH64_R8();
+   ru->regs[ru->size++] = hregLOONGARCH64_R9();
+   ru->regs[ru->size++] = hregLOONGARCH64_R10();
+   ru->regs[ru->size++] = hregLOONGARCH64_R11();
+   ru->regs[ru->size++] = hregLOONGARCH64_R12();
+   ru->regs[ru->size++] = hregLOONGARCH64_R13();
+   ru->regs[ru->size++] = hregLOONGARCH64_R21();
+   ru->regs[ru->size++] = hregLOONGARCH64_R22();
+   ru->regs[ru->size++] = hregLOONGARCH64_R31();
+   ru->regs[ru->size++] = hregLOONGARCH64_FCSR3();
+
+   rRegUniverse_LOONGARCH64_initted = True;
+
+   RRegUniverse__check_is_sane(ru);
+   return ru;
+}
+
+UInt ppHRegLOONGARCH64 ( HReg reg )
+{
+   Int r;
+   Int ret = 0;
+   static const HChar* ireg_names[32] = {
+      "$zero",
+      "$ra",
+      "$tp",
+      "$sp",
+      "$a0", "$a1", "$a2", "$a3", "$a4", "$a5", "$a6", "$a7",
+      "$t0", "$t1", "$t2", "$t3", "$t4", "$t5", "$t6", "$t7", "$t8",
+      "$r21", /* Reserved */
+      "$fp",
+      "$s0", "$s1", "$s2", "$s3", "$s4", "$s5", "$s6", "$s7", "$s8"
+   };
+   static const HChar* freg_names[32] = {
+      "$fa0",  "$fa1",  "$fa2",  "$fa3",  "$fa4",  "$fa5",  "$fa6",  "$fa7",
+      "$ft0",  "$ft1",  "$ft2",  "$ft3",  "$ft4",  "$ft5",  "$ft6",  "$ft7",
+      "$ft8",  "$ft9",  "$ft10", "$ft11", "$ft12", "$ft13", "$ft14", "$ft15",
+      "$fs0",  "$fs1",  "$fs2",  "$fs3",  "$fs4",  "$fs5",  "$fs6",  "$fs7"
+   };
+   static const HChar* vreg_names[32] = {
+      "$vr0",  "$vr1",  "$vr2",  "$vr3",  "$vr4",  "$vr5",  "$vr6",  "$vr7",
+      "$vr8",  "$vr9",  "$vr10", "$vr11", "$vr12", "$vr13", "$vr14", "$vr15",
+      "$vr16", "$vr17", "$vr18", "$vr19", "$vr20", "$vr21", "$vr22", "$vr23",
+      "$vr24", "$vr25", "$vr26", "$vr27", "$vr28", "$vr29", "$vr30", "$vr31"
+   };
+
+   /* Be generic for all virtual regs. */
+   if (hregIsVirtual(reg)) {
+      return ppHReg(reg);
+   }
+
+   /* But specific for real regs. */
+   switch (hregClass(reg)) {
+      case HRcInt32:
+         r = hregEncoding(reg);
+         vassert(r < 4);
+         ret = vex_printf("$fcsr%d", r);
+         break;
+      case HRcInt64:
+         r = hregEncoding(reg);
+         vassert(r < 32);
+         ret = vex_printf("%s", ireg_names[r]);
+         break;
+      case HRcFlt64:
+         r = hregEncoding(reg);
+         vassert(r < 32);
+         ret = vex_printf("%s", freg_names[r]);
+         break;
+      case HRcVec128:
+         r = hregEncoding(reg);
+         vassert(r < 32);
+         ret = vex_printf("%s", vreg_names[r]);
+         break;
+      default:
+         vpanic("ppHRegLOONGARCH64");
+         break;
+   }
+
+   return ret;
+}
+
+
+/* --------- Condition codes, LOONGARCH64 encoding. --------- */
+
+static inline const HChar* showLOONGARCH64CondCode ( LOONGARCH64CondCode cond )
+{
+   const HChar* ret;
+   switch (cond) {
+      case LAcc_EQ:
+         ret = "eq";  /* equal */
+         break;
+      case LAcc_NE:
+         ret = "ne";  /* not equal */
+         break;
+      case LAcc_LT:
+         ret = "lt";  /* less than (signed) */
+         break;
+      case LAcc_GE:
+         ret = "ge";  /* great equal (signed) */
+         break;
+      case LAcc_LTU:
+         ret = "ltu"; /* less than (unsigned) */
+         break;
+      case LAcc_GEU:
+         ret = "geu"; /* great equal (unsigned) */
+         break;
+      case LAcc_AL:
+         ret = "al";  /* always (unconditional) */
+         break;
+      default:
+         vpanic("showLOONGARCH64CondCode");
+         break;
+   }
+   return ret;
+}
+
+
+/* --------- Memory address expressions (amodes). --------- */
+
+LOONGARCH64AMode* LOONGARCH64AMode_RI ( HReg reg, UShort imm )
+{
+   LOONGARCH64AMode* am = LibVEX_Alloc_inline(sizeof(LOONGARCH64AMode));
+   am->tag = LAam_RI;
+   am->LAam.RI.base = reg;
+   am->LAam.RI.index = imm;
+   return am;
+}
+
+LOONGARCH64AMode* LOONGARCH64AMode_RR ( HReg base, HReg index )
+{
+   LOONGARCH64AMode* am = LibVEX_Alloc_inline(sizeof(LOONGARCH64AMode));
+   am->tag = LAam_RR;
+   am->LAam.RR.base = base;
+   am->LAam.RR.index = index;
+   return am;
+}
+
+static inline void ppLOONGARCH64AMode ( LOONGARCH64AMode* am )
+{
+   switch (am->tag) {
+      case LAam_RI:
+         ppHRegLOONGARCH64(am->LAam.RI.base);
+         vex_printf(", ");
+         vex_printf("%d", extend((UInt)am->LAam.RI.index, 12));
+         break;
+      case LAam_RR:
+         ppHRegLOONGARCH64(am->LAam.RR.base);
+         vex_printf(", ");
+         ppHRegLOONGARCH64(am->LAam.RR.index);
+         break;
+      default:
+         vpanic("ppLOONGARCH64AMode");
+         break;
+   }
+}
+
+static inline void addRegUsage_LOONGARCH64AMode( HRegUsage* u,
+                                                 LOONGARCH64AMode* am )
+{
+   switch (am->tag) {
+      case LAam_RI:
+         addHRegUse(u, HRmRead, am->LAam.RI.base);
+         break;
+      case LAam_RR:
+         addHRegUse(u, HRmRead, am->LAam.RR.base);
+         addHRegUse(u, HRmRead, am->LAam.RR.index);
+         break;
+      default:
+         vpanic("addRegUsage_LOONGARCH64AMode");
+         break;
+   }
+}
+
+static inline void mapRegs_LOONGARCH64AMode( HRegRemap* m,
+                                             LOONGARCH64AMode* am )
+{
+   switch (am->tag) {
+      case LAam_RI:
+         mapReg(m, &am->LAam.RI.base);
+         break;
+      case LAam_RR:
+         mapReg(m, &am->LAam.RR.base);
+         mapReg(m, &am->LAam.RR.index);
+         break;
+      default:
+         vpanic("mapRegs_LOONGARCH64AMode");
+         break;
+   }
+}
+
+
+/* --------- Operand, which can be reg or imm. --------- */
+
+LOONGARCH64RI* LOONGARCH64RI_R ( HReg reg )
+{
+   LOONGARCH64RI* op = LibVEX_Alloc_inline(sizeof(LOONGARCH64RI));
+   op->tag = LAri_Reg;
+   op->LAri.R.reg = reg;
+   return op;
+}
+
+LOONGARCH64RI* LOONGARCH64RI_I ( UShort imm, UChar size, Bool isSigned )
+{
+   LOONGARCH64RI* op = LibVEX_Alloc_inline(sizeof(LOONGARCH64RI));
+   op->tag = LAri_Imm;
+   op->LAri.I.imm = imm;
+   op->LAri.I.size = size;
+   op->LAri.I.isSigned = isSigned;
+   return op;
+}
+
+static inline void ppLOONGARCH64RI ( LOONGARCH64RI* ri )
+{
+   switch (ri->tag) {
+      case LAri_Reg:
+         ppHRegLOONGARCH64(ri->LAri.R.reg);
+         break;
+      case LAri_Imm:
+         if (ri->LAri.I.isSigned) {
+            vex_printf("%d", extend((UInt)ri->LAri.I.imm, ri->LAri.I.size));
+         } else {
+            vex_printf("%u", (UInt)ri->LAri.I.imm);
+         }
+         break;
+      default:
+         vpanic("ppLOONGARCH64RI");
+         break;
+   }
+}
+
+static inline void addRegUsage_LOONGARCH64RI( HRegUsage* u, LOONGARCH64RI* ri )
+{
+   switch (ri->tag) {
+      case LAri_Reg:
+         addHRegUse(u, HRmRead, ri->LAri.R.reg);
+         break;
+      case LAri_Imm:
+         break;
+      default:
+         vpanic("addRegUsage_LOONGARCH64RI");
+         break;
+   }
+}
+
+static inline void mapRegs_LOONGARCH64RI( HRegRemap* m, LOONGARCH64RI* ri )
+{
+   switch (ri->tag) {
+      case LAri_Reg:
+         mapReg(m, &ri->LAri.R.reg);
+         break;
+      case LAri_Imm:
+         break;
+      default:
+         vpanic("mapRegs_LOONGARCH64RI");
+         break;
+   }
+}
+
+
+/* --------- Instructions. --------- */
+
+static inline const HChar* showLOONGARCH64UnOp ( LOONGARCH64UnOp op )
+{
+   switch (op) {
+      case LAun_CLZ_W:
+         return "clz.w";
+      case LAun_CTZ_W:
+         return "ctz.w";
+      case LAun_CLZ_D:
+         return "clz.d";
+      case LAun_CTZ_D:
+         return "ctz.w";
+      case LAun_EXT_W_H:
+         return "ext.w.h";
+      case LAun_EXT_W_B:
+         return "ext.w.b";
+      default:
+         vpanic("showLOONGARCH64UnOp");
+         break;
+   }
+}
+
+static inline const HChar* showLOONGARCH64BinOp ( LOONGARCH64BinOp op )
+{
+   switch (op) {
+      case LAbin_ADD_W:
+         return "add.w";
+      case LAbin_ADD_D:
+         return "add.d";
+      case LAbin_SUB_W:
+         return "sub.w";
+      case LAbin_SUB_D:
+         return "sub.d";
+      case LAbin_NOR:
+         return "nor";
+      case LAbin_AND:
+         return "and";
+      case LAbin_OR:
+         return "or";
+      case LAbin_XOR:
+         return "xor";
+      case LAbin_SLL_W:
+         return "sll.w";
+      case LAbin_SRL_W:
+         return "srl.w";
+      case LAbin_SRA_W:
+         return "sra.w";
+      case LAbin_SLL_D:
+         return "sll.d";
+      case LAbin_SRL_D:
+         return "srl.d";
+      case LAbin_SRA_D:
+         return "sra.d";
+      case LAbin_MUL_W:
+         return "mul.w";
+      case LAbin_MUL_D:
+         return "mul.d";
+      case LAbin_MULH_W:
+         return "mulh.w";
+      case LAbin_MULH_WU:
+         return "mulh.wu";
+      case LAbin_MULH_D:
+         return "mulh.d";
+      case LAbin_MULH_DU:
+         return "mulh.du";
+      case LAbin_MULW_D_W:
+         return "mulw.d.w";
+      case LAbin_MULW_D_WU:
+         return "mulw.d.wu";
+      case LAbin_DIV_W:
+         return "div.w";
+      case LAbin_MOD_W:
+         return "mod.w";
+      case LAbin_DIV_WU:
+         return "div.wu";
+      case LAbin_MOD_WU:
+         return "mod.wu";
+      case LAbin_DIV_D:
+         return "div.d";
+      case LAbin_MOD_D:
+         return "mod.d";
+      case LAbin_DIV_DU:
+         return "div.du";
+      case LAbin_MOD_DU:
+         return "mod.du";
+      case LAbin_SLLI_W:
+         return "slli.w";
+      case LAbin_SLLI_D:
+         return "slli.d";
+      case LAbin_SRLI_W:
+         return "srli.w";
+      case LAbin_SRLI_D:
+         return "srli.d";
+      case LAbin_SRAI_W:
+         return "srai.w";
+      case LAbin_SRAI_D:
+         return "srai.d";
+      case LAbin_ADDI_W:
+         return "addi.w";
+      case LAbin_ADDI_D:
+         return "addi.d";
+      case LAbin_ANDI:
+         return "andi";
+      case LAbin_ORI:
+         return "ori";
+      case LAbin_XORI:
+         return "xori";
+      default:
+         vpanic("showLOONGARCH64BinOp");
+         break;
+   }
+}
+
+static inline const HChar* showLOONGARCH64LoadOp ( LOONGARCH64LoadOp op )
+{
+   switch (op) {
+      case LAload_LD_D:
+         return "ld.d";
+      case LAload_LD_BU:
+         return "ld.bu";
+      case LAload_LD_HU:
+         return "ld.hu";
+      case LAload_LD_WU:
+         return "ld.wu";
+      case LAload_LDX_D:
+         return "ldx.d";
+      case LAload_LDX_BU:
+         return "ldx.bu";
+      case LAload_LDX_HU:
+         return "ldx.hu";
+      case LAload_LDX_WU:
+         return "ldx.wu";
+      default:
+         vpanic("LOONGARCH64LoadOp");
+         break;
+   }
+}
+
+static inline const HChar* showLOONGARCH64StoreOp ( LOONGARCH64StoreOp op )
+{
+   switch (op) {
+      case LAstore_ST_B:
+         return "st.b";
+      case LAstore_ST_H:
+         return "st.h";
+      case LAstore_ST_W:
+         return "st.w";
+      case LAstore_ST_D:
+         return "st.d";
+      case LAstore_STX_B:
+         return "stx.b";
+      case LAstore_STX_H:
+         return "stx.h";
+      case LAstore_STX_W:
+         return "stx.w";
+      case LAstore_STX_D:
+         return "stx.d";
+      default:
+         vpanic("LOONGARCH64StoreOp");
+         break;
+   }
+}
+
+static inline const HChar* showLOONGARCH64LLSCOp ( LOONGARCH64LLSCOp op )
+{
+   switch (op) {
+      case LAllsc_LL_W:
+         return "ll.w";
+      case LAllsc_SC_W:
+         return "sc.w";
+      case LAllsc_LL_D:
+         return "ll.d";
+      case LAllsc_SC_D:
+         return "sc.d";
+      default:
+         vpanic("LOONGARCH64LLSCOp");
+         break;
+   }
+}
+
+static inline const HChar* showLOONGARCH64BarOp ( LOONGARCH64BarOp op )
+{
+   const HChar* ret;
+   switch (op) {
+      case LAbar_DBAR:
+         return "dbar";
+      case LAbar_IBAR:
+         return "ibar";
+      default:
+         vpanic("showLOONGARCH64BarOp");
+         break;
+   }
+   return ret;
+}
+
+static inline const HChar* showLOONGARCH64FpUnOp ( LOONGARCH64FpUnOp op )
+{
+   const HChar* ret;
+   switch (op) {
+      case LAfpun_FABS_S:
+         return "fabs.s";
+      case LAfpun_FABS_D:
+         return "fabs.d";
+      case LAfpun_FNEG_S:
+         return "fneg.s";
+      case LAfpun_FNEG_D:
+         return "fneg.d";
+      case LAfpun_FLOGB_S:
+         return "flogb.s";
+      case LAfpun_FLOGB_D:
+         return "flogb.d";
+      case LAfpun_FSQRT_S:
+         return "fsqrt.s";
+      case LAfpun_FSQRT_D:
+         return "fsqrt.d";
+      case LAfpun_FRSQRT_S:
+         return "frsqrt.s";
+      case LAfpun_FRSQRT_D:
+         return "frsqrt.d";
+      case LAfpun_FCVT_S_D:
+         return "fcvt.s.d";
+      case LAfpun_FCVT_D_S:
+         return "fcvt.d.s";
+      case LAfpun_FTINT_W_S:
+         return "ftint.w.s";
+      case LAfpun_FTINT_W_D:
+         return "ftint.w.d";
+      case LAfpun_FTINT_L_S:
+         return "ftint.l.s";
+      case LAfpun_FTINT_L_D:
+         return "ftint.l.d";
+      case LAfpun_FFINT_S_W:
+         return "ffint.s.w";
+      case LAfpun_FFINT_S_L:
+         return "ffint.s.l";
+      case LAfpun_FFINT_D_W:
+         return "ffint.d.w";
+      case LAfpun_FFINT_D_L:
+         return "ffint.d.l";
+      case LAfpun_FRINT_S:
+         return "frint.s";
+      case LAfpun_FRINT_D:
+         return "frint.d";
+      default:
+         vpanic("showLOONGARCH64FpUnOp");
+         break;
+   }
+   return ret;
+}
+
+static inline const HChar* showLOONGARCH64FpBinOp ( LOONGARCH64FpBinOp op )
+{
+   const HChar* ret;
+   switch (op) {
+      case LAfpbin_FADD_S:
+         return "fadd.s";
+      case LAfpbin_FADD_D:
+         return "fadd.d";
+      case LAfpbin_FSUB_S:
+         return "fsub.s";
+      case LAfpbin_FSUB_D:
+         return "fsub.d";
+      case LAfpbin_FMUL_S:
+         return "fmul.s";
+      case LAfpbin_FMUL_D:
+         return "fmul.d";
+      case LAfpbin_FDIV_S:
+         return "fdiv.s";
+      case LAfpbin_FDIV_D:
+         return "fdiv.d";
+      case LAfpbin_FMAX_S:
+         return "fmax.s";
+      case LAfpbin_FMAX_D:
+         return "fmax.d";
+      case LAfpbin_FMIN_S:
+         return "fmin.s";
+      case LAfpbin_FMIN_D:
+         return "fmin.d";
+      case LAfpbin_FMAXA_S:
+         return "fmaxa.s";
+      case LAfpbin_FMAXA_D:
+         return "fmaxa.d";
+      case LAfpbin_FMINA_S:
+         return "fmina.s";
+      case LAfpbin_FMINA_D:
+         return "fmina.d";
+      case LAfpbin_FSCALEB_S:
+         return "fscaleb.s";
+      case LAfpbin_FSCALEB_D:
+         return "fscaleb.d";
+      default:
+         vpanic("showLOONGARCH64FpBinOp");
+         break;
+   }
+   return ret;
+}
+
+static inline const HChar* showLOONGARCH64FpTriOp ( LOONGARCH64FpTriOp op )
+{
+   const HChar* ret;
+   switch (op) {
+      case LAfpbin_FMADD_S:
+         return "fmadd.s";
+      case LAfpbin_FMADD_D:
+         return "fmadd.d";
+      case LAfpbin_FMSUB_S:
+         return "fmsub.s";
+      case LAfpbin_FMSUB_D:
+         return "fmsub.d";
+      default:
+         vpanic("showLOONGARCH64FpTriOp");
+         break;
+   }
+   return ret;
+}
+
+static inline const HChar* showLOONGARCH64FpLoadOp ( LOONGARCH64FpLoadOp op )
+{
+   switch (op) {
+      case LAfpload_FLD_S:
+         return "fld.s";
+      case LAfpload_FLD_D:
+         return "fld.d";
+      case LAfpload_FLDX_S:
+         return "fldx.s";
+      case LAfpload_FLDX_D:
+         return "fldx.d";
+      default:
+         vpanic("LOONGARCH64FpLoadOp");
+         break;
+   }
+}
+
+static inline const HChar* showLOONGARCH64FpStoreOp ( LOONGARCH64FpStoreOp op )
+{
+   switch (op) {
+      case LAfpstore_FST_S:
+         return "fst.s";
+      case LAfpstore_FST_D:
+         return "fst.d";
+      case LAfpstore_FSTX_S:
+         return "fstx.s";
+      case LAfpstore_FSTX_D:
+         return "fstx.d";
+      default:
+         vpanic("LOONGARCH64FpStoreOp");
+         break;
+   }
+}
+
+static inline const HChar* showLOONGARCH64FpMoveOp ( LOONGARCH64FpMoveOp op )
+{
+   switch (op) {
+      case LAfpmove_FMOV_S:
+         return "fmov.s";
+      case LAfpmove_FMOV_D:
+         return "fmov.d";
+      case LAfpmove_MOVGR2FR_W:
+         return "movgr2fr.w";
+      case LAfpmove_MOVGR2FR_D:
+         return "movgr2fr.d";
+      case LAfpmove_MOVFR2GR_S:
+         return "movfr2gr.s";
+      case LAfpmove_MOVFR2GR_D:
+         return "movfr2gr.d";
+      case LAfpmove_MOVGR2FCSR:
+         return "movgr2fcsr";
+      case LAfpmove_MOVFCSR2GR:
+         return "movfcsr2gr";
+      default:
+         vpanic("showLOONGARCH64FpMoveOp");
+         break;
+   }
+}
+
+static inline const HChar* showLOONGARCH64FpCmpOp ( LOONGARCH64FpCmpOp op )
+{
+   const HChar* ret;
+   switch (op) {
+      case LAfpcmp_FCMP_CLT_S:
+         return "fcmp.clt.s";
+      case LAfpcmp_FCMP_CLT_D:
+         return "fcmp.clt.d";
+      case LAfpcmp_FCMP_CEQ_S:
+         return "fcmp.ceq.s";
+      case LAfpcmp_FCMP_CEQ_D:
+         return "fcmp.ceq.d";
+      case LAfpcmp_FCMP_CUN_S:
+         return "fcmp.cun.s";
+      case LAfpcmp_FCMP_CUN_D:
+         return "fcmp.cun.d";
+      default:
+         vpanic("showLOONGARCH64FpCmpOp");
+         break;
+   }
+   return ret;
+}
+
+static inline const HChar* showLOONGARCH64VecUnOp ( LOONGARCH64VecUnOp op )
+{
+   switch (op) {
+      case LAvecun_VCLO_B:
+         return "vclo.b";
+      case LAvecun_VCLO_H:
+         return "vclo.h";
+      case LAvecun_VCLO_W:
+         return "vclo.w";
+      case LAvecun_VCLZ_B:
+         return "vclz.b";
+      case LAvecun_VCLZ_H:
+         return "vclz.h";
+      case LAvecun_VCLZ_W:
+         return "vclz.w";
+      case LAvecun_VCLZ_D:
+         return "vclz.d";
+      case LAvecun_VPCNT_B:
+         return "vpcnt.b";
+      case LAvecun_VEXTH_H_B:
+         return "vexth.h.b";
+      case LAvecun_VEXTH_W_H:
+         return "vexth.w.h";
+      case LAvecun_VEXTH_D_W:
+         return "vexth.d.w";
+      case LAvecun_VEXTH_Q_D:
+         return "vexth.q.d";
+      case LAvecun_VEXTH_HU_BU:
+         return "vexth.hu.bu";
+      case LAvecun_VEXTH_WU_HU:
+         return "vexth.wu.hu";
+      case LAvecun_VEXTH_DU_WU:
+         return "vexth.du.wu";
+      case LAvecun_VEXTH_QU_DU:
+         return "vexth.qu.du";
+      case LAvecun_VREPLGR2VR_B:
+         return "vreplgr2vr.b";
+      case LAvecun_VREPLGR2VR_H:
+         return "vreplgr2vr.h";
+      case LAvecun_VREPLGR2VR_W:
+         return "vreplgr2vr.w";
+      case LAvecun_VREPLGR2VR_D:
+         return "vreplgr2vr.d";
+      default:
+         vpanic("showLOONGARCH64VecUnOp");
+         break;
+   }
+}
+
+static inline const HChar* showLOONGARCH64VecBinOp ( LOONGARCH64VecBinOp op )
+{
+   switch (op) {
+      case LAvecbin_VSEQ_B:
+         return "vseq.b";
+      case LAvecbin_VSEQ_H:
+         return "vseq.h";
+      case LAvecbin_VSEQ_W:
+         return "vseq.w";
+      case LAvecbin_VSEQ_D:
+         return "vseq.d";
+      case LAvecbin_VSLT_B:
+         return "vslt.b";
+      case LAvecbin_VSLT_H:
+         return "vslt.h";
+      case LAvecbin_VSLT_W:
+         return "vslt.w";
+      case LAvecbin_VSLT_D:
+         return "vslt.d";
+      case LAvecbin_VSLT_BU:
+         return "vslt.bu";
+      case LAvecbin_VSLT_HU:
+         return "vslt.hu";
+      case LAvecbin_VSLT_WU:
+         return "vslt.wu";
+      case LAvecbin_VSLT_DU:
+         return "vslt.du";
+      case LAvecbin_VADD_B:
+         return "vadd.b";
+      case LAvecbin_VADD_H:
+         return "vadd.h";
+      case LAvecbin_VADD_W:
+         return "vadd.w";
+      case LAvecbin_VADD_D:
+         return "vadd.d";
+      case LAvecbin_VSUB_B:
+         return "vsub.b";
+      case LAvecbin_VSUB_H:
+         return "vsub.h";
+      case LAvecbin_VSUB_W:
+         return "vsub.w";
+      case LAvecbin_VSUB_D:
+         return "vsub.d";
+      case LAvecbin_VSADD_B:
+         return "vsadd.b";
+      case LAvecbin_VSADD_H:
+         return "vsadd.h";
+      case LAvecbin_VSADD_W:
+         return "vsadd.w";
+      case LAvecbin_VSADD_D:
+         return "vsadd.d";
+      case LAvecbin_VSSUB_B:
+         return "vssub.b";
+      case LAvecbin_VSSUB_H:
+         return "vssub.h";
+      case LAvecbin_VSSUB_W:
+         return "vssub.w";
+      case LAvecbin_VSSUB_D:
+         return "vssub.d";
+      case LAvecbin_VSADD_BU:
+         return "vsadd.bu";
+      case LAvecbin_VSADD_HU:
+         return "vsadd.hu";
+      case LAvecbin_VSADD_WU:
+         return "vsadd.wu";
+      case LAvecbin_VSADD_DU:
+         return "vsadd.du";
+      case LAvecbin_VSSUB_BU:
+         return "vssub.bu";
+      case LAvecbin_VSSUB_HU:
+         return "vssub.hu";
+      case LAvecbin_VSSUB_WU:
+         return "vssub.wu";
+      case LAvecbin_VSSUB_DU:
+         return "vssub.du";
+      case LAvecbin_VADDA_B:
+         return "vadda.b";
+      case LAvecbin_VADDA_H:
+         return "vadda.h";
+      case LAvecbin_VADDA_W:
+         return "vadda.w";
+      case LAvecbin_VADDA_D:
+         return "vadda.d";
+      case LAvecbin_VAVGR_B:
+         return "vavgr.b";
+      case LAvecbin_VAVGR_H:
+         return "vavgr.h";
+      case LAvecbin_VAVGR_W:
+         return "vavgr.w";
+      case LAvecbin_VAVGR_D:
+         return "vavgr.d";
+      case LAvecbin_VAVGR_BU:
+         return "vavgr.bu";
+      case LAvecbin_VAVGR_HU:
+         return "vavgr.hu";
+      case LAvecbin_VAVGR_WU:
+         return "vavgr.wu";
+      case LAvecbin_VAVGR_DU:
+         return "vavgr.du";
+      case LAvecbin_VMAX_B:
+         return "vmax.b";
+      case LAvecbin_VMAX_H:
+         return "vmax.h";
+      case LAvecbin_VMAX_W:
+         return "vmax.w";
+      case LAvecbin_VMAX_D:
+         return "vmax.d";
+      case LAvecbin_VMIN_B:
+         return "vmin.b";
+      case LAvecbin_VMIN_H:
+         return "vmin.h";
+      case LAvecbin_VMIN_W:
+         return "vmin.w";
+      case LAvecbin_VMIN_D:
+         return "vmin.d";
+      case LAvecbin_VMAX_BU:
+         return "vmax.bu";
+      case LAvecbin_VMAX_HU:
+         return "vmax.hu";
+      case LAvecbin_VMAX_WU:
+         return "vmax.wu";
+      case LAvecbin_VMAX_DU:
+         return "vmax.du";
+      case LAvecbin_VMIN_BU:
+         return "vmin.bu";
+      case LAvecbin_VMIN_HU:
+         return "vmin.hu";
+      case LAvecbin_VMIN_WU:
+         return "vmin.wu";
+      case LAvecbin_VMIN_DU:
+         return "vmin.du";
+      case LAvecbin_VMUL_B:
+         return "vmul.b";
+      case LAvecbin_VMUL_H:
+         return "vmul.h";
+      case LAvecbin_VMUL_W:
+         return "vmul.w";
+      case LAvecbin_VMUH_B:
+         return "vmuh.b";
+      case LAvecbin_VMUH_H:
+         return "vmuh.h";
+      case LAvecbin_VMUH_W:
+         return "vmuh.w";
+      case LAvecbin_VMUH_BU:
+         return "vmuh.bu";
+      case LAvecbin_VMUH_HU:
+         return "vmuh.hu";
+      case LAvecbin_VMUH_WU:
+         return "vmuh.wu";
+      case LAvecbin_VSLL_B:
+         return "vsll.b";
+      case LAvecbin_VSLL_H:
+         return "vsll.h";
+      case LAvecbin_VSLL_W:
+         return "vsll.w";
+      case LAvecbin_VSLL_D:
+         return "vsll.d";
+      case LAvecbin_VSRL_B:
+         return "vsrl.b";
+      case LAvecbin_VSRL_H:
+         return "vsrl.h";
+      case LAvecbin_VSRL_W:
+         return "vsrl.w";
+      case LAvecbin_VSRL_D:
+         return "vsrl.d";
+      case LAvecbin_VSRA_B:
+         return "vsra.b";
+      case LAvecbin_VSRA_H:
+         return "vsra.h";
+      case LAvecbin_VSRA_W:
+         return "vsra.w";
+      case LAvecbin_VSRA_D:
+         return "vsra.d";
+      case LAvecbin_VILVL_B:
+         return "vilvl.b";
+      case LAvecbin_VILVL_H:
+         return "vilvl.h";
+      case LAvecbin_VILVL_W:
+         return "vilvl.w";
+      case LAvecbin_VILVL_D:
+         return "vilvl.d";
+      case LAvecbin_VILVH_B:
+         return "vilvh.b";
+      case LAvecbin_VILVH_H:
+         return "vilvh.h";
+      case LAvecbin_VILVH_W:
+         return "vilvh.w";
+      case LAvecbin_VILVH_D:
+         return "vilvh.d";
+      case LAvecbin_VPICKEV_B:
+         return "vpickev.b";
+      case LAvecbin_VPICKEV_H:
+         return "vpickev.h";
+      case LAvecbin_VPICKEV_W:
+         return "vpickev.w";
+      case LAvecbin_VPICKOD_B:
+         return "vpickod.b";
+      case LAvecbin_VPICKOD_H:
+         return "vpickod.h";
+      case LAvecbin_VPICKOD_W:
+         return "vpickod.w";
+      case LAvecbin_VREPLVE_B:
+         return "vreplve.b";
+      case LAvecbin_VREPLVE_H:
+         return "vreplve.h";
+      case LAvecbin_VREPLVE_W:
+         return "vreplve.w";
+      case LAvecbin_VREPLVE_D:
+         return "vreplve.d";
+      case LAvecbin_VAND_V:
+         return "vand.v";
+      case LAvecbin_VOR_V:
+         return "vor.v";
+      case LAvecbin_VXOR_V:
+         return "vxor.v";
+      case LAvecbin_VNOR_V:
+         return "vnor.v";
+      case LAvecbin_VADD_Q:
+         return "vadd.q";
+      case LAvecbin_VSUB_Q:
+         return "vsub.q";
+      case LAvecbin_VFADD_S:
+         return "vfadd.s";
+      case LAvecbin_VFADD_D:
+         return "vfadd.d";
+      case LAvecbin_VFSUB_S:
+         return "vfsub.s";
+      case LAvecbin_VFSUB_D:
+         return "vfsub.d";
+      case LAvecbin_VFMUL_S:
+         return "vfmul.s";
+      case LAvecbin_VFMUL_D:
+         return "vfmul.d";
+      case LAvecbin_VFDIV_S:
+         return "vfdiv.s";
+      case LAvecbin_VFDIV_D:
+         return "vfdiv.d";
+      case LAvecbin_VFMAX_S:
+         return "vfmax.s";
+      case LAvecbin_VFMAX_D:
+         return "vfmax.d";
+      case LAvecbin_VFMIN_S:
+         return "vfmin.s";
+      case LAvecbin_VFMIN_D:
+         return "vfmin.d";
+      case LAvecbin_VBSLL_V:
+         return "vbsll.v";
+      case LAvecbin_VBSRL_V:
+         return "vbsrl.v";
+      case LAvecbin_VINSGR2VR_B:
+         return "vinsgr2vr.b";
+      case LAvecbin_VINSGR2VR_H:
+         return "vinsgr2vr.h";
+      case LAvecbin_VINSGR2VR_W:
+         return "vinsgr2vr.w";
+      case LAvecbin_VINSGR2VR_D:
+         return "vinsgr2vr.d";
+      case LAvecbin_VPICKVE2GR_W:
+         return "vpickve2gr.w";
+      case LAvecbin_VPICKVE2GR_D:
+         return "vpickve2gr.d";
+      case LAvecbin_VPICKVE2GR_BU:
+         return "vpickve2gr.bu";
+      case LAvecbin_VPICKVE2GR_HU:
+         return "vpickve2gr.hu";
+      case LAvecbin_VPICKVE2GR_WU:
+         return "vpickve2gr.wu";
+      case LAvecbin_VPICKVE2GR_DU:
+         return "vpickve2gr.du";
+      case LAvecbin_VSLLI_B:
+         return "vslli.b";
+      case LAvecbin_VSLLI_H:
+         return "vslli.h";
+      case LAvecbin_VSLLI_W:
+         return "vslli.w";
+      case LAvecbin_VSLLI_D:
+         return "vslli.d";
+      case LAvecbin_VSRLI_B:
+         return "vsrli.b";
+      case LAvecbin_VSRLI_H:
+         return "vsrli.h";
+      case LAvecbin_VSRLI_W:
+         return "vsrli.w";
+      case LAvecbin_VSRLI_D:
+         return "vsrli.d";
+      case LAvecbin_VSRAI_B:
+         return "vsrai.b";
+      case LAvecbin_VSRAI_H:
+         return "vsrai.h";
+      case LAvecbin_VSRAI_W:
+         return "vsrai.w";
+      case LAvecbin_VSRAI_D:
+         return "vsrai.d";
+      case LAvecbin_VORI_B:
+         return "vori.b";
+      default:
+         vpanic("showLOONGARCH64VecBinOp");
+         break;
+   }
+}
+
+static inline const HChar* showLOONGARCH64VecLoadOp ( LOONGARCH64VecLoadOp op )
+{
+   switch (op) {
+      case LAvecload_VLD:
+         return "vld";
+      case LAvecload_VLDX:
+         return "vldx";
+      default:
+         vpanic("showLOONGARCH64VecLoadOp");
+         break;
+   }
+}
+
+static inline const HChar* showLOONGARCH64VecStoreOp ( LOONGARCH64VecStoreOp op )
+{
+   switch (op) {
+      case LAvecstore_VST:
+         return "vst";
+      case LAvecstore_VSTX:
+         return "vstx";
+      default:
+         vpanic("showLOONGARCH64VecStoreOp");
+         break;
+   }
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_LI ( ULong imm, HReg dst )
+{
+   LOONGARCH64Instr* i = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag              = LAin_LI;
+   i->LAin.LI.imm      = imm;
+   i->LAin.LI.dst      = dst;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_Unary ( LOONGARCH64UnOp op,
+                                           HReg src, HReg dst )
+{
+   LOONGARCH64Instr* i = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag              = LAin_Un;
+   i->LAin.Unary.op    = op;
+   i->LAin.Unary.src   = src;
+   i->LAin.Unary.dst   = dst;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_Binary ( LOONGARCH64BinOp op,
+                                            LOONGARCH64RI* src2,
+                                            HReg src1, HReg dst )
+{
+   LOONGARCH64Instr* i = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag              = LAin_Bin;
+   i->LAin.Binary.op   = op;
+   i->LAin.Binary.src2 = src2;
+   i->LAin.Binary.src1 = src1;
+   i->LAin.Binary.dst  = dst;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_Load ( LOONGARCH64LoadOp op,
+                                          LOONGARCH64AMode* src, HReg dst )
+{
+   LOONGARCH64Instr* i = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag              = LAin_Load;
+   i->LAin.Load.op     = op;
+   i->LAin.Load.src    = src;
+   i->LAin.Load.dst    = dst;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_Store ( LOONGARCH64StoreOp op,
+                                           LOONGARCH64AMode* dst, HReg src )
+{
+   LOONGARCH64Instr* i = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag              = LAin_Store;
+   i->LAin.Store.op    = op;
+   i->LAin.Store.dst   = dst;
+   i->LAin.Store.src   = src;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_LLSC ( LOONGARCH64LLSCOp op, Bool isLoad,
+                                          LOONGARCH64AMode* addr, HReg val )
+{
+   LOONGARCH64Instr* i = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag              = LAin_LLSC;
+   i->LAin.LLSC.op     = op;
+   i->LAin.LLSC.isLoad = isLoad;
+   i->LAin.LLSC.addr   = addr;
+   i->LAin.LLSC.val    = val;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_Bar ( LOONGARCH64BarOp op, UShort hint )
+{
+   LOONGARCH64Instr* i = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag              = LAin_Bar;
+   i->LAin.Bar.op      = op;
+   i->LAin.Bar.hint    = hint;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_FpUnary ( LOONGARCH64FpUnOp op,
+                                             HReg src, HReg dst )
+{
+   LOONGARCH64Instr* i = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag              = LAin_FpUn;
+   i->LAin.FpUnary.op  = op;
+   i->LAin.FpUnary.src = src;
+   i->LAin.FpUnary.dst = dst;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_FpBinary ( LOONGARCH64FpBinOp op, HReg src2,
+                                              HReg src1, HReg dst )
+{
+   LOONGARCH64Instr* i   = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag                = LAin_FpBin;
+   i->LAin.FpBinary.op   = op;
+   i->LAin.FpBinary.src2 = src2;
+   i->LAin.FpBinary.src1 = src1;
+   i->LAin.FpBinary.dst  = dst;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_FpTrinary ( LOONGARCH64FpTriOp op,
+                                               HReg src3, HReg src2,
+                                               HReg src1, HReg dst )
+{
+   LOONGARCH64Instr* i    = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag                 = LAin_FpTri;
+   i->LAin.FpTrinary.op   = op;
+   i->LAin.FpTrinary.src3 = src3;
+   i->LAin.FpTrinary.src2 = src2;
+   i->LAin.FpTrinary.src1 = src1;
+   i->LAin.FpTrinary.dst  = dst;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_FpLoad ( LOONGARCH64FpLoadOp op,
+                                            LOONGARCH64AMode* src, HReg dst )
+{
+   LOONGARCH64Instr* i = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag              = LAin_FpLoad;
+   i->LAin.FpLoad.op   = op;
+   i->LAin.FpLoad.src  = src;
+   i->LAin.FpLoad.dst  = dst;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_FpStore ( LOONGARCH64FpStoreOp op,
+                                             LOONGARCH64AMode* dst, HReg src )
+{
+   LOONGARCH64Instr* i = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag              = LAin_FpStore;
+   i->LAin.FpStore.op  = op;
+   i->LAin.FpStore.dst = dst;
+   i->LAin.FpStore.src = src;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_FpMove ( LOONGARCH64FpMoveOp op,
+                                            HReg src, HReg dst )
+{
+   LOONGARCH64Instr* i = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag              = LAin_FpMove;
+   i->LAin.FpMove.op   = op;
+   i->LAin.FpMove.src  = src;
+   i->LAin.FpMove.dst  = dst;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_FpCmp ( LOONGARCH64FpCmpOp op, HReg src2,
+                                           HReg src1, HReg dst )
+{
+   LOONGARCH64Instr* i   = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag                = LAin_FpCmp;
+   i->LAin.FpCmp.op      = op;
+   i->LAin.FpCmp.src2    = src2;
+   i->LAin.FpCmp.src1    = src1;
+   i->LAin.FpCmp.dst     = dst;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_VecUnary ( LOONGARCH64VecUnOp op,
+                                              HReg src, HReg dst )
+{
+   LOONGARCH64Instr* i    = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag                 = LAin_VecUn;
+   i->LAin.VecUnary.op    = op;
+   i->LAin.VecUnary.src   = src;
+   i->LAin.VecUnary.dst   = dst;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_VecBinary ( LOONGARCH64VecBinOp op,
+                                               LOONGARCH64RI* src2,
+                                               HReg src1, HReg dst )
+{
+   LOONGARCH64Instr* i    = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag                 = LAin_VecBin;
+   i->LAin.VecBinary.op   = op;
+   i->LAin.VecBinary.src2 = src2;
+   i->LAin.VecBinary.src1 = src1;
+   i->LAin.VecBinary.dst  = dst;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_VecLoad ( LOONGARCH64VecLoadOp op,
+                                             LOONGARCH64AMode* src, HReg dst )
+{
+   LOONGARCH64Instr* i = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag              = LAin_VecLoad;
+   i->LAin.VecLoad.op  = op;
+   i->LAin.VecLoad.src = src;
+   i->LAin.VecLoad.dst = dst;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_VecStore ( LOONGARCH64VecStoreOp op,
+                                              LOONGARCH64AMode* dst, HReg src)
+{
+   LOONGARCH64Instr* i  = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag               = LAin_VecStore;
+   i->LAin.VecStore.op  = op;
+   i->LAin.VecStore.dst = dst;
+   i->LAin.VecStore.src = src;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_Cas ( HReg old, HReg addr, HReg expd,
+                                         HReg data, Bool size64 )
+{
+   LOONGARCH64Instr* i = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag              = LAin_Cas;
+   i->LAin.Cas.old     = old;
+   i->LAin.Cas.addr    = addr;
+   i->LAin.Cas.expd    = expd;
+   i->LAin.Cas.data    = data;
+   i->LAin.Cas.size64  = size64;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_Cmp ( LOONGARCH64CondCode cond,
+                                         HReg src2, HReg src1, HReg dst )
+{
+   LOONGARCH64Instr* i  = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag               = LAin_Cmp;
+   i->LAin.Cmp.cond     = cond;
+   i->LAin.Cmp.src2     = src2;
+   i->LAin.Cmp.src1     = src1;
+   i->LAin.Cmp.dst      = dst;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_CMove ( HReg cond, HReg r0, HReg r1,
+                                           HReg dst, Bool isInt )
+{
+   LOONGARCH64Instr* i  = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag               = LAin_CMove;
+   i->LAin.CMove.cond   = cond;
+   i->LAin.CMove.r0     = r0;
+   i->LAin.CMove.r1     = r1;
+   i->LAin.CMove.dst    = dst;
+   i->LAin.CMove.isInt  = isInt;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_Call ( HReg cond, Addr64 target,
+                                          UInt nArgRegs, RetLoc rloc )
+{
+   LOONGARCH64Instr* i   = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag                = LAin_Call;
+   i->LAin.Call.cond     = cond;
+   i->LAin.Call.target   = target;
+   i->LAin.Call.nArgRegs = nArgRegs;
+   i->LAin.Call.rloc     = rloc;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_XDirect ( Addr64 dstGA,
+                                             LOONGARCH64AMode* amPC,
+                                             HReg cond, Bool toFastEP )
+{
+   LOONGARCH64Instr* i      = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag                   = LAin_XDirect;
+   i->LAin.XDirect.dstGA    = dstGA;
+   i->LAin.XDirect.amPC     = amPC;
+   i->LAin.XDirect.cond     = cond;
+   i->LAin.XDirect.toFastEP = toFastEP;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_XIndir ( HReg dstGA, LOONGARCH64AMode* amPC,
+                                            HReg cond )
+{
+   LOONGARCH64Instr* i  = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag               = LAin_XIndir;
+   i->LAin.XIndir.dstGA = dstGA;
+   i->LAin.XIndir.amPC  = amPC;
+   i->LAin.XIndir.cond  = cond;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_XAssisted ( HReg dstGA,
+                                               LOONGARCH64AMode* amPC,
+                                               HReg cond, IRJumpKind jk )
+{
+   LOONGARCH64Instr* i     = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag                  = LAin_XAssisted;
+   i->LAin.XAssisted.dstGA = dstGA;
+   i->LAin.XAssisted.amPC  = amPC;
+   i->LAin.XAssisted.cond  = cond;
+   i->LAin.XAssisted.jk    = jk;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_EvCheck ( LOONGARCH64AMode* amCounter,
+                                             LOONGARCH64AMode* amFailAddr )
+{
+   LOONGARCH64Instr* i        = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag                     = LAin_EvCheck;
+   i->LAin.EvCheck.amCounter  = amCounter;
+   i->LAin.EvCheck.amFailAddr = amFailAddr;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_ProfInc ( void )
+{
+   LOONGARCH64Instr* i = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag              = LAin_ProfInc;
+   return i;
+}
+
+
+/* -------- Pretty Print instructions ------------- */
+
+static inline void ppLI ( ULong imm, HReg dst )
+{
+   vex_printf("li ");
+   ppHRegLOONGARCH64(dst);
+   vex_printf(", 0x%llx", imm);
+}
+
+static inline void ppUnary ( LOONGARCH64UnOp op, HReg src, HReg dst )
+{
+   vex_printf("%s ", showLOONGARCH64UnOp(op));
+   ppHRegLOONGARCH64(dst);
+   vex_printf(", ");
+   ppHRegLOONGARCH64(src);
+}
+
+static inline void ppBinary ( LOONGARCH64BinOp op, LOONGARCH64RI* src2,
+                              HReg src1, HReg dst )
+{
+   vex_printf("%s ", showLOONGARCH64BinOp(op));
+   ppHRegLOONGARCH64(dst);
+   vex_printf(", ");
+   ppHRegLOONGARCH64(src1);
+   vex_printf(", ");
+   ppLOONGARCH64RI(src2);
+}
+
+static inline void ppLoad ( LOONGARCH64LoadOp op, LOONGARCH64AMode* src,
+                            HReg dst )
+{
+   vex_printf("%s ", showLOONGARCH64LoadOp(op));
+   ppHRegLOONGARCH64(dst);
+   vex_printf(", ");
+   ppLOONGARCH64AMode(src);
+}
+
+static inline void ppStore ( LOONGARCH64StoreOp op, LOONGARCH64AMode* dst,
+                             HReg src )
+{
+   vex_printf("%s ", showLOONGARCH64StoreOp(op));
+   ppHRegLOONGARCH64(src);
+   vex_printf(", ");
+   ppLOONGARCH64AMode(dst);
+}
+
+static inline void ppLLSC ( LOONGARCH64LLSCOp op, LOONGARCH64AMode* addr,
+                            HReg val )
+{
+   vex_printf("%s ", showLOONGARCH64LLSCOp(op));
+   ppHRegLOONGARCH64(val);
+   vex_printf(", ");
+   ppLOONGARCH64AMode(addr);
+}
+
+static inline void ppBar ( LOONGARCH64BarOp op, UShort hint )
+{
+   vex_printf("%s %u", showLOONGARCH64BarOp(op), (UInt)hint);
+}
+
+static inline void ppFpUnary ( LOONGARCH64FpUnOp op, HReg src, HReg dst )
+{
+   vex_printf("%s ", showLOONGARCH64FpUnOp(op));
+   ppHRegLOONGARCH64(dst);
+   vex_printf(", ");
+   ppHRegLOONGARCH64(src);
+}
+
+static inline void ppFpBinary ( LOONGARCH64FpBinOp op, HReg src2,
+                                HReg src1, HReg dst )
+{
+   vex_printf("%s ", showLOONGARCH64FpBinOp(op));
+   ppHRegLOONGARCH64(dst);
+   vex_printf(", ");
+   ppHRegLOONGARCH64(src1);
+   vex_printf(", ");
+   ppHRegLOONGARCH64(src2);
+}
+
+static inline void ppFpTrinary ( LOONGARCH64FpTriOp op, HReg src3,
+                                HReg src2, HReg src1, HReg dst )
+{
+   vex_printf("%s ", showLOONGARCH64FpTriOp(op));
+   ppHRegLOONGARCH64(dst);
+   vex_printf(", ");
+   ppHRegLOONGARCH64(src1);
+   vex_printf(", ");
+   ppHRegLOONGARCH64(src2);
+   vex_printf(", ");
+   ppHRegLOONGARCH64(src3);
+}
+
+static inline void ppFpLoad ( LOONGARCH64FpLoadOp op, LOONGARCH64AMode* src,
+                              HReg dst )
+{
+   vex_printf("%s ", showLOONGARCH64FpLoadOp(op));
+   ppHRegLOONGARCH64(dst);
+   vex_printf(", ");
+   ppLOONGARCH64AMode(src);
+}
+
+static inline void ppFpStore ( LOONGARCH64FpStoreOp op, LOONGARCH64AMode* dst,
+                               HReg src )
+{
+   vex_printf("%s ", showLOONGARCH64FpStoreOp(op));
+   ppHRegLOONGARCH64(src);
+   vex_printf(", ");
+   ppLOONGARCH64AMode(dst);
+}
+
+static inline void ppFpMove ( LOONGARCH64FpMoveOp op, HReg src, HReg dst )
+{
+   vex_printf("%s ", showLOONGARCH64FpMoveOp(op));
+   ppHRegLOONGARCH64(dst);
+   vex_printf(", ");
+   ppHRegLOONGARCH64(src);
+}
+
+static inline void ppFpCmp ( LOONGARCH64FpCmpOp op, HReg src2,
+                             HReg src1, HReg dst )
+{
+   vex_printf("%s ", showLOONGARCH64FpCmpOp(op));
+   vex_printf("$fcc0, ");
+   ppHRegLOONGARCH64(src1);
+   vex_printf(", ");
+   ppHRegLOONGARCH64(src2);
+   vex_printf("; movcf2gr ");
+   ppHRegLOONGARCH64(dst);
+   vex_printf(", $fcc0");
+}
+
+static inline void ppVecUnary ( LOONGARCH64VecUnOp op, HReg src, HReg dst )
+{
+   vex_printf("%s ", showLOONGARCH64VecUnOp(op));
+   ppHRegLOONGARCH64(dst);
+   vex_printf(", ");
+   ppHRegLOONGARCH64(src);
+}
+
+static inline void ppVecBinary ( LOONGARCH64VecBinOp op, LOONGARCH64RI* src2,
+                                 HReg src1, HReg dst )
+{
+   vex_printf("%s ", showLOONGARCH64VecBinOp(op));
+   ppHRegLOONGARCH64(dst);
+   vex_printf(", ");
+   ppHRegLOONGARCH64(src1);
+   vex_printf(", ");
+   ppLOONGARCH64RI(src2);
+}
+
+static inline void ppVecLoad ( LOONGARCH64VecLoadOp op, LOONGARCH64AMode* src,
+                               HReg dst )
+{
+   vex_printf("%s ", showLOONGARCH64VecLoadOp(op));
+   ppHRegLOONGARCH64(dst);
+   vex_printf(", ");
+   ppLOONGARCH64AMode(src);
+}
+
+static inline void ppVecStore ( LOONGARCH64VecStoreOp op, LOONGARCH64AMode* dst,
+                                HReg src )
+{
+   vex_printf("%s ", showLOONGARCH64VecStoreOp(op));
+   ppHRegLOONGARCH64(src);
+   vex_printf(", ");
+   ppLOONGARCH64AMode(dst);
+}
+
+static inline void ppCas ( HReg old, HReg addr, HReg expd,
+                           HReg data, Bool size64)
+{
+   ppHRegLOONGARCH64(old);
+   vex_printf(" = cas(%dbit)(", size64 ? 64 : 32);
+   ppHRegLOONGARCH64(expd);
+   vex_printf(", ");
+   ppHRegLOONGARCH64(data);
+   vex_printf(" -> ");
+   ppHRegLOONGARCH64(addr);
+   vex_printf(")");
+}
+
+static inline void ppCmp ( LOONGARCH64CondCode cond, HReg src2,
+                           HReg src1, HReg dst )
+{
+   ppHRegLOONGARCH64(dst);
+   vex_printf(" = cmp%s(", showLOONGARCH64CondCode(cond));
+   ppHRegLOONGARCH64(src1);
+   vex_printf(", ");
+   ppHRegLOONGARCH64(src2);
+   vex_printf(")");
+}
+
+static inline void ppCMove ( HReg cond, HReg r0, HReg r1,
+                             HReg dst, Bool isInt )
+{
+   if (isInt) {
+      vex_printf("masknez $t0, ");
+      ppHRegLOONGARCH64(r0);
+      vex_printf(", ");
+      ppHRegLOONGARCH64(cond);
+      vex_printf("; maskeqz ");
+      ppHRegLOONGARCH64(dst);
+      vex_printf(", ");
+      ppHRegLOONGARCH64(r1);
+      vex_printf(", ");
+      ppHRegLOONGARCH64(cond);
+      vex_printf("; or ");
+      ppHRegLOONGARCH64(dst);
+      vex_printf(", $t0, ");
+      ppHRegLOONGARCH64(dst);
+   } else {
+      vex_printf("movgr2cf ");
+      ppHRegLOONGARCH64(cond);
+      vex_printf(", $fcc0; fsel ");
+      ppHRegLOONGARCH64(dst);
+      vex_printf(", ");
+      ppHRegLOONGARCH64(r0);
+      vex_printf(", ");
+      ppHRegLOONGARCH64(r1);
+      vex_printf(", $fcc0");
+   }
+}
+
+static inline void ppCall ( HReg cond, Addr64 target,
+                            UInt nArgRegs, RetLoc rloc )
+{
+   if (!hregIsInvalid(cond)) {
+      vex_printf("if (");
+      ppHRegLOONGARCH64(cond);
+      vex_printf(") { ");
+   }
+   vex_printf("call 0x%llx [nArgRegs=%u, ", target, nArgRegs);
+   ppRetLoc(rloc);
+   vex_printf("]");
+   if (!hregIsInvalid(cond))
+      vex_printf(" }");
+}
+
+static inline void ppXDirect ( Addr64 dstGA, LOONGARCH64AMode* amPC,
+                               HReg cond, Bool toFastEP )
+{
+   vex_printf("(xDirect) ");
+   if (!hregIsInvalid(cond)) {
+      vex_printf("if (");
+      ppHRegLOONGARCH64(cond);
+      vex_printf(") { ");
+   }
+   vex_printf("li $t0, 0x%llx; ", (ULong)dstGA);
+   vex_printf("st.w $t0, ");
+   ppLOONGARCH64AMode(amPC);
+   vex_printf("; li $t0, $disp_cp_chain_me_to_%sEP; ",
+              toFastEP ? "fast" : "slow");
+   vex_printf("jirl $ra, $t0, 0");
+   if (!hregIsInvalid(cond))
+      vex_printf(" }");
+}
+
+static inline void ppXIndir ( HReg dstGA, LOONGARCH64AMode* amPC,
+                              HReg cond )
+{
+   vex_printf("(xIndir) ");
+   if (!hregIsInvalid(cond)) {
+      vex_printf("if (");
+      ppHRegLOONGARCH64(cond);
+      vex_printf(") { ");
+   }
+   vex_printf("st.w ");
+   ppHRegLOONGARCH64(dstGA);
+   vex_printf(", ");
+   ppLOONGARCH64AMode(amPC);
+   vex_printf("; la $t0, disp_indir; ");
+   vex_printf("jirl $ra, $t0, 0");
+   if (!hregIsInvalid(cond))
+      vex_printf(" }");
+}
+
+static inline void ppXAssisted ( HReg dstGA, LOONGARCH64AMode* amPC,
+                                 HReg cond, IRJumpKind jk)
+{
+   vex_printf("(xAssisted) ");
+   if (!hregIsInvalid(cond)) {
+      vex_printf("if (");
+      ppHRegLOONGARCH64(cond);
+      vex_printf(") { ");
+   }
+   vex_printf("st.w ");
+   ppHRegLOONGARCH64(dstGA);
+   vex_printf(", ");
+   ppLOONGARCH64AMode(amPC);
+   vex_printf("; li.w $s8, IRJumpKind_to_TRCVAL(%d); ", (Int)jk);
+   vex_printf("la $t0, disp_assisted; ");
+   vex_printf("jirl $ra, $t0, 0");
+   if (!hregIsInvalid(cond))
+      vex_printf(" }");
+}
+
+static inline void ppEvCheck ( LOONGARCH64AMode* amCounter,
+                               LOONGARCH64AMode* amFailAddr )
+{
+   vex_printf("(evCheck) ");
+   vex_printf("ld.w $t0, ");
+   ppLOONGARCH64AMode(amCounter);
+   vex_printf("; addi.d $t0, $t0, -1; ");
+   vex_printf("st.w $t0, ");
+   ppLOONGARCH64AMode(amCounter);
+   vex_printf("; bge $t0, $zero, nofail; ");
+   vex_printf("ld.d $t0, ");
+   ppLOONGARCH64AMode(amFailAddr);
+   vex_printf("; jirl $ra, $t0, 0");
+   vex_printf("; nofail:");
+}
+
+static inline void ppProfInc ( void )
+{
+   vex_printf("(profInc) ");
+   vex_printf("li $t0, NotKnownYet; ");
+   vex_printf("ld.d $t1, $t0, 0; ");
+   vex_printf("addi.d $t1, $t1, 1; ");
+   vex_printf("st.d $t1, $t0, 0;");
+}
+
+void ppLOONGARCH64Instr ( const LOONGARCH64Instr* i, Bool mode64 )
+{
+   vassert(mode64 == True);
+   switch (i->tag) {
+      case LAin_LI:
+         ppLI(i->LAin.LI.imm, i->LAin.LI.dst);
+         break;
+      case LAin_Un:
+         ppUnary(i->LAin.Unary.op, i->LAin.Unary.src, i->LAin.Unary.dst);
+         break;
+      case LAin_Bin:
+         ppBinary(i->LAin.Binary.op, i->LAin.Binary.src2,
+                  i->LAin.Binary.src1, i->LAin.Binary.dst);
+         break;
+      case LAin_Load:
+         ppLoad(i->LAin.Load.op, i->LAin.Load.src, i->LAin.Load.dst);
+         break;
+      case LAin_Store:
+         ppStore(i->LAin.Store.op, i->LAin.Store.dst, i->LAin.Store.src);
+         break;
+      case LAin_LLSC:
+         ppLLSC(i->LAin.LLSC.op, i->LAin.LLSC.addr, i->LAin.LLSC.val);
+         break;
+      case LAin_Bar:
+         ppBar(i->LAin.Bar.op, i->LAin.Bar.hint);
+         break;
+      case LAin_FpUn:
+         ppFpUnary(i->LAin.FpUnary.op, i->LAin.FpUnary.src,
+                   i->LAin.FpUnary.dst);
+         break;
+      case LAin_FpBin:
+         ppFpBinary(i->LAin.FpBinary.op, i->LAin.FpBinary.src2,
+                    i->LAin.FpBinary.src1, i->LAin.FpBinary.dst);
+         break;
+      case LAin_FpTri:
+         ppFpTrinary(i->LAin.FpTrinary.op, i->LAin.FpTrinary.src3,
+                     i->LAin.FpTrinary.src2, i->LAin.FpTrinary.src1,
+                     i->LAin.FpTrinary.dst);
+         break;
+      case LAin_FpLoad:
+         ppFpLoad(i->LAin.FpLoad.op, i->LAin.FpLoad.src, i->LAin.FpLoad.dst);
+         break;
+      case LAin_FpStore:
+         ppFpStore(i->LAin.FpStore.op, i->LAin.FpStore.dst,
+                   i->LAin.FpStore.src);
+         break;
+      case LAin_FpMove:
+         ppFpMove(i->LAin.FpMove.op, i->LAin.FpMove.src,
+                   i->LAin.FpMove.dst);
+         break;
+      case LAin_FpCmp:
+         ppFpCmp(i->LAin.FpCmp.op, i->LAin.FpCmp.src2,
+                 i->LAin.FpCmp.src1, i->LAin.FpCmp.dst);
+         break;
+      case LAin_VecUn:
+         ppVecUnary(i->LAin.VecUnary.op, i->LAin.VecUnary.src,
+                    i->LAin.VecUnary.dst);
+         break;
+      case LAin_VecBin:
+         ppVecBinary(i->LAin.VecBinary.op, i->LAin.VecBinary.src2,
+                     i->LAin.VecBinary.src1, i->LAin.VecBinary.dst);
+         break;
+      case LAin_VecLoad:
+         ppVecLoad(i->LAin.VecLoad.op, i->LAin.VecLoad.src,
+                   i->LAin.VecLoad.dst);
+         break;
+      case LAin_VecStore:
+         ppVecStore(i->LAin.VecStore.op, i->LAin.VecStore.dst,
+                    i->LAin.VecStore.src);
+         break;
+      case LAin_Cas:
+         ppCas(i->LAin.Cas.old, i->LAin.Cas.addr, i->LAin.Cas.expd,
+               i->LAin.Cas.data, i->LAin.Cas.size64);
+         break;
+      case LAin_Cmp:
+         ppCmp(i->LAin.Cmp.cond, i->LAin.Cmp.src2,
+               i->LAin.Cmp.src1, i->LAin.Cmp.dst);
+         break;
+      case LAin_CMove:
+         ppCMove(i->LAin.CMove.cond, i->LAin.CMove.r0,
+                 i->LAin.CMove.r1, i->LAin.CMove.dst,
+                 i->LAin.CMove.isInt);
+         break;
+      case LAin_Call:
+         ppCall(i->LAin.Call.cond, i->LAin.Call.target,
+                i->LAin.Call.nArgRegs, i->LAin.Call.rloc);
+         break;
+      case LAin_XDirect:
+         ppXDirect(i->LAin.XDirect.dstGA, i->LAin.XDirect.amPC,
+                   i->LAin.XDirect.cond, i->LAin.XDirect.toFastEP);
+         break;
+      case LAin_XIndir:
+         ppXIndir(i->LAin.XIndir.dstGA, i->LAin.XIndir.amPC,
+                  i->LAin.XIndir.cond);
+         break;
+      case LAin_XAssisted:
+         ppXAssisted(i->LAin.XAssisted.dstGA, i->LAin.XAssisted.amPC,
+                     i->LAin.XAssisted.cond, i->LAin.XAssisted.jk);
+         break;
+      case LAin_EvCheck:
+         ppEvCheck(i->LAin.EvCheck.amCounter, i->LAin.EvCheck.amFailAddr);
+         break;
+      case LAin_ProfInc:
+         ppProfInc();
+         break;
+      default:
+         vpanic("ppLOONGARCH64Instr");
+         break;
+   }
+}
+
+
+/* --------- Helpers for register allocation. --------- */
+
+void getRegUsage_LOONGARCH64Instr ( HRegUsage* u, const LOONGARCH64Instr* i,
+                                    Bool mode64 )
+{
+   vassert(mode64 == True);
+   initHRegUsage(u);
+   switch (i->tag) {
+      case LAin_LI:
+         addHRegUse(u, HRmWrite, i->LAin.LI.dst);
+         break;
+      case LAin_Un:
+         addHRegUse(u, HRmRead, i->LAin.Unary.src);
+         addHRegUse(u, HRmWrite, i->LAin.Unary.dst);
+         break;
+      case LAin_Bin:
+         addRegUsage_LOONGARCH64RI(u, i->LAin.Binary.src2);
+         addHRegUse(u, HRmRead, i->LAin.Binary.src1);
+         addHRegUse(u, HRmWrite, i->LAin.Binary.dst);
+         break;
+      case LAin_Load:
+         addRegUsage_LOONGARCH64AMode(u, i->LAin.Load.src);
+         addHRegUse(u, HRmWrite, i->LAin.Load.dst);
+         break;
+      case LAin_Store:
+         addRegUsage_LOONGARCH64AMode(u, i->LAin.Store.dst);
+         addHRegUse(u, HRmRead, i->LAin.Store.src);
+         break;
+      case LAin_LLSC:
+         addRegUsage_LOONGARCH64AMode(u, i->LAin.LLSC.addr);
+         if (i->LAin.LLSC.isLoad)
+            addHRegUse(u, HRmWrite, i->LAin.LLSC.val);
+         else
+            addHRegUse(u, HRmRead, i->LAin.LLSC.val);
+         break;
+      case LAin_Bar:
+         /* No regs. */
+         break;
+      case LAin_FpUn:
+         addHRegUse(u, HRmRead, i->LAin.FpUnary.src);
+         addHRegUse(u, HRmWrite, i->LAin.FpUnary.dst);
+         break;
+      case LAin_FpBin:
+         addHRegUse(u, HRmRead, i->LAin.FpBinary.src2);
+         addHRegUse(u, HRmRead, i->LAin.FpBinary.src1);
+         addHRegUse(u, HRmWrite, i->LAin.FpBinary.dst);
+         break;
+      case LAin_FpTri:
+         addHRegUse(u, HRmRead, i->LAin.FpTrinary.src3);
+         addHRegUse(u, HRmRead, i->LAin.FpTrinary.src2);
+         addHRegUse(u, HRmRead, i->LAin.FpTrinary.src1);
+         addHRegUse(u, HRmWrite, i->LAin.FpTrinary.dst);
+         break;
+      case LAin_FpLoad:
+         addRegUsage_LOONGARCH64AMode(u, i->LAin.FpLoad.src);
+         addHRegUse(u, HRmWrite, i->LAin.FpLoad.dst);
+         break;
+      case LAin_FpStore:
+         addRegUsage_LOONGARCH64AMode(u, i->LAin.FpStore.dst);
+         addHRegUse(u, HRmRead, i->LAin.FpStore.src);
+         break;
+      case LAin_FpMove:
+         addHRegUse(u, HRmRead, i->LAin.FpMove.src);
+         addHRegUse(u, HRmWrite, i->LAin.FpMove.dst);
+         break;
+      case LAin_FpCmp:
+         addHRegUse(u, HRmRead, i->LAin.FpCmp.src2);
+         addHRegUse(u, HRmRead, i->LAin.FpCmp.src1);
+         addHRegUse(u, HRmWrite, i->LAin.FpCmp.dst);
+         break;
+      case LAin_VecUn:
+         addHRegUse(u, HRmRead, i->LAin.VecUnary.src);
+         addHRegUse(u, HRmWrite, i->LAin.VecUnary.dst);
+         break;
+      case LAin_VecBin:
+         addRegUsage_LOONGARCH64RI(u, i->LAin.VecBinary.src2);
+         addHRegUse(u, HRmRead, i->LAin.VecBinary.src1);
+         addHRegUse(u, HRmWrite, i->LAin.VecBinary.dst);
+         break;
+      case LAin_VecLoad:
+         addRegUsage_LOONGARCH64AMode(u, i->LAin.VecLoad.src);
+         addHRegUse(u, HRmWrite, i->LAin.VecLoad.dst);
+         break;
+      case LAin_VecStore:
+         addRegUsage_LOONGARCH64AMode(u, i->LAin.VecStore.dst);
+         addHRegUse(u, HRmRead, i->LAin.VecStore.src);
+         break;
+      case LAin_Cas:
+         addHRegUse(u, HRmWrite, i->LAin.Cas.old);
+         addHRegUse(u, HRmRead, i->LAin.Cas.addr);
+         addHRegUse(u, HRmRead, i->LAin.Cas.expd);
+         addHRegUse(u, HRmModify, i->LAin.Cas.data);
+         break;
+      case LAin_Cmp:
+         addHRegUse(u, HRmRead, i->LAin.Cmp.src2);
+         addHRegUse(u, HRmRead, i->LAin.Cmp.src1);
+         addHRegUse(u, HRmWrite, i->LAin.Cmp.dst);
+         break;
+      case LAin_CMove:
+         addHRegUse(u, HRmRead, i->LAin.CMove.cond);
+         addHRegUse(u, HRmRead, i->LAin.CMove.r0);
+         addHRegUse(u, HRmRead, i->LAin.CMove.r1);
+         addHRegUse(u, HRmWrite, i->LAin.CMove.dst);
+         break;
+      case LAin_Call:
+         /* logic and comments copied/modified from mips and arm64 back end */
+         /* This is a bit subtle. */
+         /* First off, we need to consider the cond register. */
+         if (!hregIsInvalid(i->LAin.Call.cond))
+            addHRegUse(u, HRmRead, i->LAin.Call.cond);
+         /* Then, claim it trashes all the caller-saved regs
+            which fall within the register allocator's jurisdiction. */
+         addHRegUse(u, HRmWrite, hregLOONGARCH64_R14());
+         addHRegUse(u, HRmWrite, hregLOONGARCH64_R15());
+         addHRegUse(u, HRmWrite, hregLOONGARCH64_R16());
+         addHRegUse(u, HRmWrite, hregLOONGARCH64_R17());
+         addHRegUse(u, HRmWrite, hregLOONGARCH64_R18());
+         addHRegUse(u, HRmWrite, hregLOONGARCH64_R19());
+         addHRegUse(u, HRmWrite, hregLOONGARCH64_R20());
+         /* Now we have to state any parameter-carrying registers
+            which might be read.  This depends on nArgRegs. */
+            switch (i->LAin.Call.nArgRegs) {
+            case 8: addHRegUse(u, HRmRead, hregLOONGARCH64_R11()); /* fallthrough */
+            case 7: addHRegUse(u, HRmRead, hregLOONGARCH64_R10()); /* fallthrough */
+            case 6: addHRegUse(u, HRmRead, hregLOONGARCH64_R9());  /* fallthrough */
+            case 5: addHRegUse(u, HRmRead, hregLOONGARCH64_R8());  /* fallthrough */
+            case 4: addHRegUse(u, HRmRead, hregLOONGARCH64_R7());  /* fallthrough */
+            case 3: addHRegUse(u, HRmRead, hregLOONGARCH64_R6());  /* fallthrough */
+            case 2: addHRegUse(u, HRmRead, hregLOONGARCH64_R5());  /* fallthrough */
+            case 1: addHRegUse(u, HRmRead, hregLOONGARCH64_R4());  /* fallthrough */
+            case 0: break;
+            default: vpanic("getRegUsage_LOONGARCH64:Call:regparms"); break;
+         }
+         /* Finally, there is the issue that the insn trashes a
+            register because the literal target address has to be
+            loaded into a register.  However, we reserve $t0 for that
+            purpose so there's no further complexity here.  Stating $t0
+            as trashed is pointless since it's not under the control
+            of the allocator, but what the hell. */
+         addHRegUse(u, HRmWrite, hregT0());
+         break;
+      /* XDirect/XIndir/XAssisted are also a bit subtle.  They
+         conditionally exit the block.  Hence we only need to list (1)
+         the registers that they read, and (2) the registers that they
+         write in the case where the block is not exited.  (2) is
+         empty, hence only (1) is relevant here. */
+      case LAin_XDirect:
+         addRegUsage_LOONGARCH64AMode(u, i->LAin.XDirect.amPC);
+         if (!hregIsInvalid(i->LAin.XDirect.cond))
+            addHRegUse(u, HRmRead, i->LAin.XDirect.cond);
+         addHRegUse(u, HRmWrite, hregT0()); /* unavail to RA */
+         break;
+      case LAin_XIndir:
+         addHRegUse(u, HRmRead, i->LAin.XIndir.dstGA);
+         addRegUsage_LOONGARCH64AMode(u, i->LAin.XIndir.amPC);
+         if (!hregIsInvalid(i->LAin.XIndir.cond))
+            addHRegUse(u, HRmRead, i->LAin.XIndir.cond);
+         addHRegUse(u, HRmWrite, hregT0()); /* unavail to RA */
+         break;
+      case LAin_XAssisted:
+         addHRegUse(u, HRmRead, i->LAin.XAssisted.dstGA);
+         addRegUsage_LOONGARCH64AMode(u, i->LAin.XAssisted.amPC);
+         if (!hregIsInvalid(i->LAin.XAssisted.cond))
+            addHRegUse(u, HRmRead, i->LAin.XAssisted.cond);
+         addHRegUse(u, HRmWrite, hregT0()); /* unavail to RA */
+         break;
+      case LAin_EvCheck:
+         /* We expect both amodes only to mention $r31, so this is in
+            fact pointless, since $r31 isn't allocatable, but anyway.. */
+         addRegUsage_LOONGARCH64AMode(u, i->LAin.EvCheck.amCounter);
+         addRegUsage_LOONGARCH64AMode(u, i->LAin.EvCheck.amFailAddr);
+         addHRegUse(u, HRmWrite, hregT0()); /* unavail to RA */
+         break;
+      case LAin_ProfInc:
+         /* Again, pointless to actually state these since neither
+            is available to RA. */
+         addHRegUse(u, HRmWrite, hregT0()); /* unavail to RA */
+         addHRegUse(u, HRmWrite, hregT1()); /* unavail to RA */
+         break;
+      default:
+         ppLOONGARCH64Instr(i, mode64);
+         vpanic("getRegUsage_LOONGARCH64Instr");
+         break;
+   }
+}
+
+void mapRegs_LOONGARCH64Instr ( HRegRemap* m, LOONGARCH64Instr* i,
+                                Bool mode64 )
+{
+   vassert(mode64 == True);
+   switch (i->tag) {
+      case LAin_LI:
+         mapReg(m, &i->LAin.LI.dst);
+         break;
+      case LAin_Un:
+         mapReg(m, &i->LAin.Unary.src);
+         mapReg(m, &i->LAin.Unary.dst);
+         break;
+      case LAin_Bin:
+         mapRegs_LOONGARCH64RI(m, i->LAin.Binary.src2);
+         mapReg(m, &i->LAin.Binary.src1);
+         mapReg(m, &i->LAin.Binary.dst);
+         break;
+      case LAin_Load:
+         mapRegs_LOONGARCH64AMode(m, i->LAin.Load.src);
+         mapReg(m, &i->LAin.Load.dst);
+         break;
+      case LAin_Store:
+         mapRegs_LOONGARCH64AMode(m, i->LAin.Store.dst);
+         mapReg(m, &i->LAin.Store.src);
+         break;
+      case LAin_LLSC:
+         mapRegs_LOONGARCH64AMode(m, i->LAin.LLSC.addr);
+         mapReg(m, &i->LAin.LLSC.val);
+         break;
+      case LAin_Bar:
+         /* No regs. */
+         break;
+      case LAin_FpUn:
+         mapReg(m, &i->LAin.FpUnary.src);
+         mapReg(m, &i->LAin.FpUnary.dst);
+         break;
+      case LAin_FpBin:
+         mapReg(m, &i->LAin.FpBinary.src2);
+         mapReg(m, &i->LAin.FpBinary.src1);
+         mapReg(m, &i->LAin.FpBinary.dst);
+         break;
+      case LAin_FpTri:
+         mapReg(m, &i->LAin.FpTrinary.src3);
+         mapReg(m, &i->LAin.FpTrinary.src2);
+         mapReg(m, &i->LAin.FpTrinary.src1);
+         mapReg(m, &i->LAin.FpTrinary.dst);
+         break;
+      case LAin_FpLoad:
+         mapRegs_LOONGARCH64AMode(m, i->LAin.FpLoad.src);
+         mapReg(m, &i->LAin.FpLoad.dst);
+         break;
+      case LAin_FpStore:
+         mapRegs_LOONGARCH64AMode(m, i->LAin.FpStore.dst);
+         mapReg(m, &i->LAin.FpStore.src);
+         break;
+      case LAin_FpMove:
+         mapReg(m, &i->LAin.FpMove.src);
+         mapReg(m, &i->LAin.FpMove.dst);
+         break;
+      case LAin_FpCmp:
+         mapReg(m, &i->LAin.FpCmp.src2);
+         mapReg(m, &i->LAin.FpCmp.src1);
+         mapReg(m, &i->LAin.FpCmp.dst);
+         break;
+      case LAin_VecUn:
+         mapReg(m, &i->LAin.VecUnary.src);
+         mapReg(m, &i->LAin.VecUnary.dst);
+         break;
+      case LAin_VecBin:
+         mapRegs_LOONGARCH64RI(m, i->LAin.VecBinary.src2);
+         mapReg(m, &i->LAin.VecBinary.src1);
+         mapReg(m, &i->LAin.VecBinary.dst);
+         break;
+      case LAin_VecLoad:
+         mapRegs_LOONGARCH64AMode(m, i->LAin.VecLoad.src);
+         mapReg(m, &i->LAin.VecLoad.dst);
+         break;
+      case LAin_VecStore:
+         mapRegs_LOONGARCH64AMode(m, i->LAin.VecStore.dst);
+         mapReg(m, &i->LAin.VecStore.src);
+         break;
+      case LAin_Cas:
+         mapReg(m, &i->LAin.Cas.old);
+         mapReg(m, &i->LAin.Cas.addr);
+         mapReg(m, &i->LAin.Cas.expd);
+         mapReg(m, &i->LAin.Cas.data);
+         break;
+      case LAin_Cmp:
+         mapReg(m, &i->LAin.Cmp.src2);
+         mapReg(m, &i->LAin.Cmp.src1);
+         mapReg(m, &i->LAin.Cmp.dst);
+         break;
+      case LAin_CMove:
+         mapReg(m, &i->LAin.CMove.cond);
+         mapReg(m, &i->LAin.CMove.r0);
+         mapReg(m, &i->LAin.CMove.r1);
+         mapReg(m, &i->LAin.CMove.dst);
+         break;
+      case LAin_Call:
+         if (!hregIsInvalid(i->LAin.Call.cond))
+            mapReg(m, &i->LAin.Call.cond);
+         /* Hardwires $r12. */
+         break;
+      /* XDirect/XIndir/XAssisted are also a bit subtle.  They
+         conditionally exit the block.  Hence we only need to list (1)
+         the registers that they read, and (2) the registers that they
+         write in the case where the block is not exited.  (2) is
+         empty, hence only (1) is relevant here. */
+      case LAin_XDirect:
+         mapRegs_LOONGARCH64AMode(m, i->LAin.XDirect.amPC);
+         if (!hregIsInvalid(i->LAin.XDirect.cond))
+            mapReg(m, &i->LAin.XDirect.cond);
+         break;
+      case LAin_XIndir:
+         mapReg(m, &i->LAin.XIndir.dstGA);
+         mapRegs_LOONGARCH64AMode(m, i->LAin.XIndir.amPC);
+         if (!hregIsInvalid(i->LAin.XIndir.cond))
+            mapReg(m, &i->LAin.XIndir.cond);
+         break;
+      case LAin_XAssisted:
+         mapReg(m, &i->LAin.XAssisted.dstGA);
+         mapRegs_LOONGARCH64AMode(m, i->LAin.XAssisted.amPC);
+         if (!hregIsInvalid(i->LAin.XAssisted.cond))
+            mapReg(m, &i->LAin.XAssisted.cond);
+         break;
+      case LAin_EvCheck:
+         /* We expect both amodes only to mention $r31, so this is in
+            fact pointless, since $r31 isn't allocatable, but anyway.. */
+         mapRegs_LOONGARCH64AMode(m, i->LAin.EvCheck.amCounter);
+         mapRegs_LOONGARCH64AMode(m, i->LAin.EvCheck.amFailAddr);
+         break;
+      case LAin_ProfInc:
+         /* Hardwires $r12 and $r13 -- nothing to modify. */
+         break;
+      default:
+         ppLOONGARCH64Instr(i, mode64);
+         vpanic("mapRegs_LOONGARCH64Instr");
+         break;
+   }
+}
+
+/* Generate loongarch64 spill instructions under the direction of the
+   register allocator. */
+void genSpill_LOONGARCH64 ( /*OUT*/ HInstr** i1, /*OUT*/ HInstr** i2,
+                            HReg rreg, Int offsetB, Bool mode64 )
+{
+   vassert(mode64 == True);
+   vassert(offsetB >= 0);
+   vassert(!hregIsVirtual(rreg));
+
+   LOONGARCH64AMode* am;
+   *i1 = *i2 = NULL;
+
+   switch (hregClass(rreg)) {
+      case HRcInt64:
+         if (offsetB < 1024) {
+            am = LOONGARCH64AMode_RI(hregGSP(), offsetB);
+            *i1 = LOONGARCH64Instr_Store(LAstore_ST_D, am, rreg);
+         } else {
+            *i1 = LOONGARCH64Instr_LI(offsetB, hregT0());
+            am = LOONGARCH64AMode_RR(hregGSP(), hregT0());
+            *i2 = LOONGARCH64Instr_Store(LAstore_STX_D, am, rreg);
+         }
+         break;
+      case HRcFlt64:
+         if (offsetB < 1024) {
+            am = LOONGARCH64AMode_RI(hregGSP(), offsetB);
+            *i1 = LOONGARCH64Instr_FpStore(LAfpstore_FST_D, am, rreg);
+         } else {
+            *i1 = LOONGARCH64Instr_LI(offsetB, hregT0());
+            am = LOONGARCH64AMode_RR(hregGSP(), hregT0());
+            *i2 = LOONGARCH64Instr_FpStore(LAfpstore_FSTX_D, am, rreg);
+         }
+         break;
+      case HRcVec128:
+         if (offsetB < 1024) {
+            am = LOONGARCH64AMode_RI(hregGSP(), offsetB);
+            *i1 = LOONGARCH64Instr_VecStore(LAvecstore_VST, am, rreg);
+         } else {
+            am = LOONGARCH64AMode_RR(hregGSP(), hregT0());
+            *i1 = LOONGARCH64Instr_LI(offsetB, hregT0());
+            *i2 = LOONGARCH64Instr_VecStore(LAvecstore_VSTX, am, rreg);
+         }
+         break;
+      default:
+         ppHRegClass(hregClass(rreg));
+         vpanic("genSpill_LOONGARCH64: unimplemented regclass");
+         break;
+   }
+}
+
+/* Generate loongarch64 reload instructions under the direction of the
+   register allocator. */
+void genReload_LOONGARCH64 ( /*OUT*/ HInstr** i1, /*OUT*/ HInstr** i2,
+                             HReg rreg, Int offsetB, Bool mode64 )
+{
+   vassert(mode64 == True);
+   vassert(offsetB >= 0);
+   vassert(!hregIsVirtual(rreg));
+
+   LOONGARCH64AMode* am;
+   *i1 = *i2 = NULL;
+
+   switch (hregClass(rreg)) {
+      case HRcInt64:
+         if (offsetB < 1024) {
+            am = LOONGARCH64AMode_RI(hregGSP(), offsetB);
+            *i1 = LOONGARCH64Instr_Load(LAload_LD_D, am, rreg);
+         } else {
+            *i1 = LOONGARCH64Instr_LI(offsetB, hregT0());
+            am = LOONGARCH64AMode_RR(hregGSP(), hregT0());
+            *i2 = LOONGARCH64Instr_Load(LAload_LDX_D, am, rreg);
+         }
+         break;
+      case HRcFlt64:
+         if (offsetB < 1024) {
+            am = LOONGARCH64AMode_RI(hregGSP(), offsetB);
+            *i1 = LOONGARCH64Instr_FpLoad(LAfpload_FLD_D, am, rreg);
+         } else {
+            *i1 = LOONGARCH64Instr_LI(offsetB, hregT0());
+            am = LOONGARCH64AMode_RR(hregGSP(), hregT0());
+            *i2 = LOONGARCH64Instr_FpLoad(LAfpload_FLDX_D, am, rreg);
+         }
+         break;
+      case HRcVec128:
+         if (offsetB < 1024) {
+            am = LOONGARCH64AMode_RI(hregGSP(), offsetB);
+            *i1 = LOONGARCH64Instr_VecLoad(LAvecload_VLD, am, rreg);
+         } else {
+            am = LOONGARCH64AMode_RR(hregGSP(), hregT0());
+            *i1 = LOONGARCH64Instr_LI(offsetB, hregT0());
+            *i2 = LOONGARCH64Instr_VecLoad(LAvecload_VLDX, am, rreg);
+         }
+         break;
+      default:
+         ppHRegClass(hregClass(rreg));
+         vpanic("genReload_LOONGARCH64: unimplemented regclass");
+         break;
+   }
+}
+
+/* Generate loongarch64 move instructions under the direction of the
+   register allocator. */
+LOONGARCH64Instr* genMove_LOONGARCH64 ( HReg from, HReg to, Bool mode64 )
+{
+   vassert(mode64 == True);
+   switch (hregClass(from)) {
+      case HRcInt64:
+         return LOONGARCH64Instr_Binary(LAbin_OR,
+                                        LOONGARCH64RI_R(hregZERO()),
+                                        from, to);
+      case HRcFlt64:
+         return LOONGARCH64Instr_FpMove(LAfpmove_FMOV_D, from, to);
+      case HRcVec128:
+         return LOONGARCH64Instr_VecBinary(LAvecbin_VORI_B,
+                                           LOONGARCH64RI_I(0, 8, False),
+                                           from, to);
+      default:
+         ppHRegClass(hregClass(from));
+         vpanic("genMove_LOONGARCH64: unimplemented regclass");
+   }
+}
+
+
+/* --------- The loongarch64 assembler --------- */
+
+static inline UInt iregEnc ( HReg r )
+{
+   vassert(hregClass(r) == HRcInt64);
+   vassert(!hregIsVirtual(r));
+   UInt n = hregEncoding(r);
+   vassert(n < 32);
+   return n;
+}
+
+static inline UInt fregEnc ( HReg r )
+{
+   vassert(hregClass(r) == HRcFlt64);
+   vassert(!hregIsVirtual(r));
+   UInt n = hregEncoding(r);
+   vassert(n < 32);
+   return n;
+}
+
+static inline UInt vregEnc ( HReg r )
+{
+   vassert(hregClass(r) == HRcVec128);
+   vassert(!hregIsVirtual(r));
+   UInt n = hregEncoding(r);
+   vassert(n < 32);
+   return n;
+}
+
+static inline UInt fcsrEnc ( HReg r )
+{
+   vassert(hregClass(r) == HRcInt32);
+   vassert(!hregIsVirtual(r));
+   UInt n = hregEncoding(r);
+   vassert(n < 32);
+   return n;
+}
+
+static inline UInt emit_op_rj_rd ( UInt op, UInt rj, UInt rd )
+{
+   vassert(rj < (1 << 5));
+   vassert(rd < (1 << 5));
+   return op | (rj << 5) | rd;
+}
+
+static inline UInt emit_op_rk_rj_rd ( UInt op, UInt rk, UInt rj, UInt rd )
+{
+   vassert(rk < (1 << 5));
+   vassert(rj < (1 << 5));
+   vassert(rd < (1 << 5));
+   return op | (rk << 10) | (rj << 5) | rd;
+}
+
+static inline UInt emit_op_fj_fd ( UInt op, UInt fj, UInt fd )
+{
+   vassert(fj < (1 << 5));
+   vassert(fd < (1 << 5));
+   return op | (fj << 5) | fd;
+}
+
+static inline UInt emit_op_fa_fk_fj_fd ( UInt op, UInt fa, UInt fk, UInt fj, UInt fd )
+{
+   vassert(fa < (1 << 5));
+   vassert(fk < (1 << 5));
+   vassert(fj < (1 << 5));
+   vassert(fd < (1 << 5));
+   return op | (fa << 15) | (fk << 10) | (fj << 5) | fd;
+}
+
+static inline UInt emit_op_fk_fj_fd ( UInt op, UInt fk, UInt fj, UInt fd )
+{
+   vassert(fk < (1 << 5));
+   vassert(fj < (1 << 5));
+   vassert(fd < (1 << 5));
+   return op | (fk << 10) | (fj << 5) | fd;
+}
+
+static inline UInt emit_op_ca_fk_fj_fd ( UInt op, UInt ca, UInt fk, UInt fj, UInt fd )
+{
+   vassert(ca < (1 << 3));
+   vassert(fk < (1 << 5));
+   vassert(fj < (1 << 5));
+   vassert(fd < (1 << 5));
+   return op | (ca << 15) | (fk << 10) | (fj << 5) | fd;
+}
+
+static inline UInt emit_op_fk_fj_cd ( UInt op, UInt fk, UInt fj, UInt cd )
+{
+   vassert(fk < (1 << 5));
+   vassert(fj < (1 << 5));
+   vassert(cd < (1 << 3));
+   return op | (fk << 10) | (fj << 5) | cd;
+}
+
+static inline UInt emit_op_cj_rd ( UInt op, UInt cj, UInt rd )
+{
+   vassert(cj < (1 << 3));
+   vassert(rd < (1 << 5));
+   return op | (cj << 5) | rd;
+}
+
+static inline UInt emit_op_rj_cd ( UInt op, UInt rj, UInt cd )
+{
+   vassert(rj < (1 << 5));
+   vassert(cd < (1 << 3));
+   return op | (rj << 5) | cd;
+}
+
+static inline UInt emit_op_rj_fd ( UInt op, UInt rj, UInt fd )
+{
+   vassert(rj < (1 << 5));
+   vassert(fd < (1 << 5));
+   return op | (rj << 5) | fd;
+}
+
+static inline UInt emit_op_fj_rd ( UInt op, UInt fj, UInt rd )
+{
+   vassert(fj < (1 << 5));
+   vassert(rd < (1 << 5));
+   return op | (fj << 5) | rd;
+}
+
+static inline UInt emit_op_rj_fcsr ( UInt op, UInt rj, UInt fcsr )
+{
+   vassert(rj < (1 << 5));
+   vassert(fcsr < (1 << 5));
+   return op | (rj << 5) | fcsr;
+}
+
+static inline UInt emit_op_fcsr_rd ( UInt op, UInt fcsr, UInt rd )
+{
+   vassert(fcsr < (1 << 5));
+   vassert(rd < (1 << 5));
+   return op | (fcsr << 5) | rd;
+}
+
+static inline UInt emit_op_ui5_rj_rd ( UInt op, UInt ui5, UInt rj, UInt rd )
+{
+   vassert(ui5 < (1 << 5));
+   vassert(rj < (1 << 5));
+   vassert(rd < (1 << 5));
+   return op | (ui5 << 10) | (rj << 5) | rd;
+}
+
+static inline UInt emit_op_ui6_rj_rd ( UInt op, UInt ui6, UInt rj, UInt rd )
+{
+   vassert(ui6 < (1 << 6));
+   vassert(rj < (1 << 5));
+   vassert(rd < (1 << 5));
+   return op | (ui6 << 10) | (rj << 5) | rd;
+}
+
+static inline UInt emit_op_ui12_rj_rd ( UInt op, UInt ui12, UInt rj, UInt rd )
+{
+   vassert(ui12 < (1 << 12));
+   vassert(rj < (1 << 5));
+   vassert(rd < (1 << 5));
+   return op | (ui12 << 10) | (rj << 5) | rd;
+}
+
+static inline UInt emit_op_si12_rj_rd ( UInt op, UInt si12, UInt rj, UInt rd )
+{
+   vassert(si12 < (1 << 12));
+   vassert(rj < (1 << 5));
+   vassert(rd < (1 << 5));
+   return op | (si12 << 10) | (rj << 5) | rd;
+}
+
+static inline UInt emit_op_si14_rj_rd ( UInt op, UInt si14, UInt rj, UInt rd )
+{
+   vassert(si14 < (1 << 14));
+   vassert(rj < (1 << 5));
+   vassert(rd < (1 << 5));
+   return op | (si14 << 10) | (rj << 5) | rd;
+}
+
+static inline UInt emit_op_si20_rd ( UInt op, UInt si20, UInt rd )
+{
+   vassert(si20 < (1 << 20));
+   vassert(rd < (1 << 5));
+   return op | (si20 << 5) | rd;
+}
+
+static inline UInt emit_op_offs16_rj_rd ( UInt op, UInt offs16, UInt rj, UInt rd )
+{
+   vassert(offs16 < (1 << 16));
+   vassert(rj < (1 << 5));
+   vassert(rd < (1 << 5));
+   return op | (offs16 << 10) | (rj << 5) | rd;
+}
+
+static inline UInt emit_op_offs26 ( UInt op, UInt offs26 )
+{
+   vassert(offs26 < (1 << 26));
+   return op | ((offs26 & 0xffff) << 10) | (offs26 >> 16);
+}
+
+static inline UInt emit_op_hint15 ( UInt op, UInt hint )
+{
+   vassert(hint < (1 << 15));
+   return op | hint;
+}
+
+static inline UInt emit_op_si12_rj_vd ( UInt op, UInt si12, UInt rj, UInt vd )
+{
+   vassert(si12 < (1 << 12));
+   vassert(rj < (1 << 5));
+   vassert(vd < (1 << 5));
+   return op | (si12 << 10) | (rj << 5) | vd;
+}
+
+static inline UInt emit_op_rk_rj_vd ( UInt op, UInt rk, UInt rj, UInt vd )
+{
+   vassert(rk < (1 << 5));
+   vassert(rj < (1 << 5));
+   vassert(vd < (1 << 5));
+   return op | (rk << 10) | (rj << 5) | vd;
+}
+
+static inline UInt emit_unop_r ( UInt op, Int src, UInt dst, UInt dst_size)
+{
+   vassert(src < (1 << 5));
+   vassert(dst < (1 << dst_size));
+   return op | (src << 5) | dst;
+}
+
+static inline UInt emit_binop_rr ( UInt op, UInt src2, UInt src1, UInt dst )
+{
+   vassert(src2 < (1 << 5));
+   vassert(src1 < (1 << 5));
+   vassert(dst < (1 << 5));
+   return op | (src2 << 10) | (src1 << 5) | dst;
+}
+
+static inline UInt emit_binop_ri ( UInt op, UInt imm, UInt size, UInt src, UInt dst )
+{
+   vassert(imm < (1 << size));
+   vassert(src < (1 << 5));
+   vassert(dst < (1 << 5));
+   return op | (imm << 10) | (src << 5) | dst;
+}
+
+
+static UInt* mkLoadImm_EXACTLY4 ( UInt* p, HReg dst, ULong imm )
+{
+   /*
+      lu12i.w dst, imm[31:12]
+      ori     dst, dst, imm[11:0]
+      lu32i.d dst, imm[51:32]
+      lu52i.d dst, dst, imm[63:52]
+    */
+   UInt d = iregEnc(dst);
+   *p++ = emit_op_si20_rd(LAextra_LU12I_W, (imm >> 12) & 0xfffff, d);
+   *p++ = emit_op_si12_rj_rd(LAbin_ORI, imm & 0xfff, d, d);
+   *p++ = emit_op_si20_rd(LAextra_LU32I_D, (imm >> 32) & 0xfffff, d);
+   *p++ = emit_op_si12_rj_rd(LAextra_LU52I_D, (imm >> 52) & 0xfff, d, d);
+   return p;
+}
+
+static inline UInt* mkLoadImm_EXACTLY2 ( UInt* p, HReg dst, ULong imm )
+{
+   /*
+      lu12i.w dst, imm[31:12]
+      ori     dst, dst, imm[11:0]
+    */
+   UInt d = iregEnc(dst);
+   *p++ = emit_op_si20_rd(LAextra_LU12I_W, (imm >> 12) & 0xfffff, d);
+   *p++ = emit_op_si12_rj_rd(LAbin_ORI, imm & 0xfff, d, d);
+   return p;
+}
+
+static inline UInt* mkLoadImm_EXACTLY1 ( UInt* p, HReg dst, ULong imm )
+{
+   /* ori dst, $zero, imm[11:0] */
+   *p++ = emit_op_si12_rj_rd(LAbin_ORI, imm, 0, iregEnc(dst));
+   return p;
+}
+
+static UInt* mkLoadImm ( UInt* p, HReg dst, ULong imm )
+{
+   if ((imm >> 12) == 0)
+      p = mkLoadImm_EXACTLY1(p, dst, imm);
+   else if (imm < 0x80000000 || (imm >> 31) == 0x1ffffffffUL)
+      p = mkLoadImm_EXACTLY2(p, dst, imm);
+   else
+      p = mkLoadImm_EXACTLY4(p, dst, imm);
+   return p;
+}
+
+static Bool is_LoadImm_EXACTLY4 ( UInt* p, HReg dst, ULong imm )
+{
+   UInt expect[4];
+   mkLoadImm_EXACTLY4(expect, dst, imm);
+   return toBool(p[0] == expect[0] && p[1] == expect[1] &&
+                 p[2] == expect[2] && p[3] == expect[3]);
+}
+
+static inline UInt* mkUnary ( UInt* p, LOONGARCH64UnOp op, HReg src, HReg dst )
+{
+   switch (op) {
+      case LAun_CLZ_W:
+      case LAun_CTZ_W:
+      case LAun_CLZ_D:
+      case LAun_CTZ_D:
+      case LAun_EXT_W_H:
+      case LAun_EXT_W_B:
+         *p++ = emit_op_rj_rd(op, iregEnc(src), iregEnc(dst));
+         return p;
+      default:
+         return NULL;
+   }
+}
+
+static inline UInt* mkBinary ( UInt* p, LOONGARCH64BinOp op,
+                               LOONGARCH64RI* src2, HReg src1, HReg dst )
+{
+   switch (op) {
+      case LAbin_ADD_W:
+      case LAbin_ADD_D:
+      case LAbin_SUB_W:
+      case LAbin_SUB_D:
+      case LAbin_NOR:
+      case LAbin_AND:
+      case LAbin_OR:
+      case LAbin_XOR:
+      case LAbin_SLL_W:
+      case LAbin_SRL_W:
+      case LAbin_SRA_W:
+      case LAbin_SLL_D:
+      case LAbin_SRL_D:
+      case LAbin_SRA_D:
+      case LAbin_MUL_W:
+      case LAbin_MUL_D:
+      case LAbin_MULH_W:
+      case LAbin_MULH_WU:
+      case LAbin_MULH_D:
+      case LAbin_MULH_DU:
+      case LAbin_MULW_D_W:
+      case LAbin_MULW_D_WU:
+      case LAbin_DIV_W:
+      case LAbin_MOD_W:
+      case LAbin_DIV_WU:
+      case LAbin_MOD_WU:
+      case LAbin_DIV_D:
+      case LAbin_MOD_D:
+      case LAbin_DIV_DU:
+      case LAbin_MOD_DU:
+         vassert(src2->tag == LAri_Reg);
+         *p++ = emit_op_rk_rj_rd(op, iregEnc(src2->LAri.R.reg),
+                                 iregEnc(src1), iregEnc(dst));
+         return p;
+      case LAbin_SLLI_W:
+      case LAbin_SRLI_W:
+      case LAbin_SRAI_W:
+         vassert(src2->tag == LAri_Imm);
+         *p++ = emit_op_ui5_rj_rd(op, src2->LAri.I.imm,
+                                  iregEnc(src1), iregEnc(dst));
+         return p;
+      case LAbin_SLLI_D:
+      case LAbin_SRLI_D:
+      case LAbin_SRAI_D:
+         vassert(src2->tag == LAri_Imm);
+         *p++ = emit_op_ui6_rj_rd(op, src2->LAri.I.imm,
+                                  iregEnc(src1), iregEnc(dst));
+         return p;
+      case LAbin_ADDI_W:
+      case LAbin_ADDI_D:
+         vassert(src2->tag == LAri_Imm);
+         *p++ = emit_op_si12_rj_rd(op, src2->LAri.I.imm,
+                                   iregEnc(src1), iregEnc(dst));
+         return p;
+      case LAbin_ANDI:
+      case LAbin_ORI:
+      case LAbin_XORI:
+         vassert(src2->tag == LAri_Imm);
+         *p++ = emit_op_ui12_rj_rd(op, src2->LAri.I.imm,
+                                   iregEnc(src1), iregEnc(dst));
+         return p;
+      default:
+         return NULL;
+   }
+}
+
+static UInt* mkLoad ( UInt* p, LOONGARCH64LoadOp op,
+                      LOONGARCH64AMode* src, HReg dst )
+{
+   switch (op) {
+      case LAload_LD_W:
+      case LAload_LD_D:
+      case LAload_LD_BU:
+      case LAload_LD_HU:
+      case LAload_LD_WU:
+         vassert(src->tag == LAam_RI);
+         *p++ = emit_op_si12_rj_rd(op, src->LAam.RI.index,
+                                   iregEnc(src->LAam.RI.base), iregEnc(dst));
+         return p;
+      case LAload_LDX_D:
+      case LAload_LDX_BU:
+      case LAload_LDX_HU:
+      case LAload_LDX_WU:
+         vassert(src->tag == LAam_RR);
+         *p++ = emit_op_rk_rj_rd(op, iregEnc(src->LAam.RR.index),
+                                 iregEnc(src->LAam.RR.base), iregEnc(dst));
+         return p;
+      default:
+         return NULL;
+   }
+}
+
+static UInt* mkStore ( UInt* p, LOONGARCH64StoreOp op,
+                       LOONGARCH64AMode* dst, HReg src )
+{
+   switch (op) {
+      case LAstore_ST_B:
+      case LAstore_ST_H:
+      case LAstore_ST_W:
+      case LAstore_ST_D:
+         vassert(dst->tag == LAam_RI);
+         *p++ = emit_op_si12_rj_rd(op, dst->LAam.RI.index,
+                                   iregEnc(dst->LAam.RI.base), iregEnc(src));
+         return p;
+      case LAstore_STX_B:
+      case LAstore_STX_H:
+      case LAstore_STX_W:
+      case LAstore_STX_D:
+         vassert(dst->tag == LAam_RR);
+         *p++ = emit_op_rk_rj_rd(op, iregEnc(dst->LAam.RR.index),
+                                 iregEnc(dst->LAam.RR.base), iregEnc(src));
+         return p;
+      default:
+         return NULL;
+   }
+}
+
+static inline UInt* mkLLSC ( UInt* p, LOONGARCH64LLSCOp op,
+                             LOONGARCH64AMode* addr, HReg val )
+{
+   switch (op) {
+      case LAllsc_LL_W:
+      case LAllsc_SC_W:
+      case LAllsc_LL_D:
+      case LAllsc_SC_D:
+         vassert(addr->tag == LAam_RI);
+         *p++ = emit_op_si14_rj_rd(op, addr->LAam.RI.index,
+                                   iregEnc(addr->LAam.RI.base), iregEnc(val));
+         return p;
+      default:
+         return NULL;
+   }
+}
+
+static inline UInt* mkBar ( UInt* p, LOONGARCH64BarOp op, UShort hint )
+{
+   switch (op) {
+      case LAbar_DBAR:
+      case LAbar_IBAR:
+         *p++ = emit_op_hint15(op, hint);
+         return p;
+      default:
+         return NULL;
+   }
+}
+
+static inline UInt* mkFpUnary ( UInt* p, LOONGARCH64FpUnOp op, HReg src, HReg dst )
+{
+   switch (op) {
+      case LAfpun_FABS_S:
+      case LAfpun_FABS_D:
+      case LAfpun_FNEG_S:
+      case LAfpun_FNEG_D:
+      case LAfpun_FLOGB_S:
+      case LAfpun_FLOGB_D:
+      case LAfpun_FSQRT_S:
+      case LAfpun_FSQRT_D:
+      case LAfpun_FRSQRT_S:
+      case LAfpun_FRSQRT_D:
+      case LAfpun_FCVT_S_D:
+      case LAfpun_FCVT_D_S:
+      case LAfpun_FTINT_W_S:
+      case LAfpun_FTINT_W_D:
+      case LAfpun_FTINT_L_S:
+      case LAfpun_FTINT_L_D:
+      case LAfpun_FFINT_S_W:
+      case LAfpun_FFINT_S_L:
+      case LAfpun_FFINT_D_W:
+      case LAfpun_FFINT_D_L:
+      case LAfpun_FRINT_S:
+      case LAfpun_FRINT_D:
+         *p++ = emit_op_fj_fd(op, fregEnc(src), fregEnc(dst));
+         return p;
+      default:
+         return NULL;
+   }
+}
+
+static inline UInt* mkFpBinary ( UInt* p, LOONGARCH64FpBinOp op, HReg src2,
+                                 HReg src1, HReg dst )
+{
+   switch (op) {
+      case LAfpbin_FADD_S:
+      case LAfpbin_FADD_D:
+      case LAfpbin_FSUB_S:
+      case LAfpbin_FSUB_D:
+      case LAfpbin_FMUL_S:
+      case LAfpbin_FMUL_D:
+      case LAfpbin_FDIV_S:
+      case LAfpbin_FDIV_D:
+      case LAfpbin_FMAX_S:
+      case LAfpbin_FMAX_D:
+      case LAfpbin_FMIN_S:
+      case LAfpbin_FMIN_D:
+      case LAfpbin_FMAXA_S:
+      case LAfpbin_FMAXA_D:
+      case LAfpbin_FMINA_S:
+      case LAfpbin_FMINA_D:
+      case LAfpbin_FSCALEB_S:
+      case LAfpbin_FSCALEB_D:
+         *p++ = emit_op_fk_fj_fd(op, fregEnc(src2), fregEnc(src1), fregEnc(dst));
+         return p;
+      default:
+         return NULL;
+   }
+}
+
+static inline UInt* mkFpTrinary ( UInt* p, LOONGARCH64FpTriOp op, HReg src3,
+                                  HReg src2, HReg src1, HReg dst )
+{
+   switch (op) {
+      case LAfpbin_FMADD_S:
+      case LAfpbin_FMADD_D:
+      case LAfpbin_FMSUB_S:
+      case LAfpbin_FMSUB_D:
+         *p++ = emit_op_fa_fk_fj_fd(op, fregEnc(src3), fregEnc(src2),
+                                    fregEnc(src1), fregEnc(dst));
+         return p;
+      default:
+         return NULL;
+   }
+}
+
+static inline UInt* mkFpLoad ( UInt* p, LOONGARCH64FpLoadOp op,
+                               LOONGARCH64AMode* src, HReg dst )
+{
+   switch (op) {
+      case LAfpload_FLD_S:
+      case LAfpload_FLD_D:
+         vassert(src->tag == LAam_RI);
+         *p++ = emit_op_si12_rj_rd(op, src->LAam.RI.index,
+                                   iregEnc(src->LAam.RI.base), fregEnc(dst));
+         return p;
+      case LAfpload_FLDX_S:
+      case LAfpload_FLDX_D:
+         vassert(src->tag == LAam_RR);
+         *p++ = emit_op_rk_rj_rd(op, iregEnc(src->LAam.RR.index),
+                                 iregEnc(src->LAam.RR.base), fregEnc(dst));
+         return p;
+      default:
+         return NULL;
+   }
+}
+
+static inline UInt* mkFpStore ( UInt* p, LOONGARCH64FpStoreOp op,
+                                LOONGARCH64AMode* dst, HReg src )
+{
+   switch (op) {
+      case LAfpstore_FST_S:
+      case LAfpstore_FST_D:
+         vassert(dst->tag == LAam_RI);
+         *p++ = emit_op_si12_rj_rd(op, dst->LAam.RI.index,
+                                   iregEnc(dst->LAam.RI.base), fregEnc(src));
+         return p;
+      case LAfpstore_FSTX_S:
+      case LAfpstore_FSTX_D:
+         vassert(dst->tag == LAam_RR);
+         *p++ = emit_op_rk_rj_rd(op, iregEnc(dst->LAam.RR.index),
+                                 iregEnc(dst->LAam.RR.base), fregEnc(src));
+         return p;
+      default:
+         return NULL;
+   }
+}
+
+static inline UInt* mkFpMove ( UInt* p, LOONGARCH64FpMoveOp op, HReg src, HReg dst )
+{
+   switch (op) {
+      case LAfpmove_FMOV_S:
+      case LAfpmove_FMOV_D:
+         *p++ = emit_op_fj_fd(op, fregEnc(src), fregEnc(dst));
+         return p;
+      case LAfpmove_MOVGR2FR_W:
+      case LAfpmove_MOVGR2FR_D:
+         *p++ = emit_op_rj_fd(op, iregEnc(src), fregEnc(dst));
+         return p;
+      case LAfpmove_MOVFR2GR_S:
+      case LAfpmove_MOVFR2GR_D:
+         *p++ = emit_op_fj_rd(op, fregEnc(src), iregEnc(dst));
+         return p;
+      case LAfpmove_MOVGR2FCSR:
+         *p++ = emit_op_rj_fcsr(op, iregEnc(src), fcsrEnc(dst));
+         return p;
+      case LAfpmove_MOVFCSR2GR:
+         *p++ = emit_op_fcsr_rd(op, fcsrEnc(src), iregEnc(dst));
+         return p;
+      default:
+         return NULL;
+   }
+}
+
+static inline UInt* mkFpCmp ( UInt* p, LOONGARCH64FpCmpOp op, HReg src2,
+                              HReg src1, HReg dst )
+{
+   /*
+      fcmp.cond.[sd] $fcc0, src1, src2
+      movcf2gr       dst, $fcc0
+    */
+   switch (op) {
+      case LAfpcmp_FCMP_CLT_S:
+      case LAfpcmp_FCMP_CLT_D:
+      case LAfpcmp_FCMP_CEQ_S:
+      case LAfpcmp_FCMP_CEQ_D:
+      case LAfpcmp_FCMP_CUN_S:
+      case LAfpcmp_FCMP_CUN_D:
+         *p++ = emit_op_fk_fj_cd(op, fregEnc(src2), fregEnc(src1), 0);
+         *p++ = emit_op_cj_rd(LAextra_MOVCF2GR, 0, iregEnc(dst));
+         return p;
+      default:
+         return NULL;
+   }
+}
+
+static inline UInt* mkVecUnary ( UInt* p, LOONGARCH64VecUnOp op, HReg src, HReg dst )
+{
+   switch (op) {
+      case LAvecun_VCLO_B:
+      case LAvecun_VCLO_H:
+      case LAvecun_VCLO_W:
+      case LAvecun_VCLZ_B:
+      case LAvecun_VCLZ_H:
+      case LAvecun_VCLZ_W:
+      case LAvecun_VCLZ_D:
+      case LAvecun_VPCNT_B:
+      case LAvecun_VEXTH_H_B:
+      case LAvecun_VEXTH_W_H:
+      case LAvecun_VEXTH_D_W:
+      case LAvecun_VEXTH_Q_D:
+      case LAvecun_VEXTH_HU_BU:
+      case LAvecun_VEXTH_WU_HU:
+      case LAvecun_VEXTH_DU_WU:
+      case LAvecun_VEXTH_QU_DU:
+         *p++ = emit_unop_r(op, vregEnc(src), vregEnc(dst), 5);
+         return p;
+      case LAvecun_VREPLGR2VR_B:
+      case LAvecun_VREPLGR2VR_H:
+      case LAvecun_VREPLGR2VR_W:
+      case LAvecun_VREPLGR2VR_D:
+         *p++ = emit_unop_r(op, iregEnc(src), vregEnc(dst), 5);
+         return p;
+      default:
+         return NULL;
+   }
+}
+
+static inline UInt* mkVecBinary ( UInt* p, LOONGARCH64VecBinOp op,
+                                  LOONGARCH64RI* src2, HReg src1, HReg dst )
+{
+   switch (op) {
+      case LAvecbin_VSEQ_B:
+      case LAvecbin_VSEQ_H:
+      case LAvecbin_VSEQ_W:
+      case LAvecbin_VSEQ_D:
+      case LAvecbin_VSLT_B:
+      case LAvecbin_VSLT_H:
+      case LAvecbin_VSLT_W:
+      case LAvecbin_VSLT_D:
+      case LAvecbin_VSLT_BU:
+      case LAvecbin_VSLT_HU:
+      case LAvecbin_VSLT_WU:
+      case LAvecbin_VSLT_DU:
+      case LAvecbin_VADD_B:
+      case LAvecbin_VADD_H:
+      case LAvecbin_VADD_D:
+      case LAvecbin_VADD_W:
+      case LAvecbin_VSUB_B:
+      case LAvecbin_VSUB_H:
+      case LAvecbin_VSUB_W:
+      case LAvecbin_VSUB_D:
+      case LAvecbin_VSADD_B:
+      case LAvecbin_VSADD_H:
+      case LAvecbin_VSADD_W:
+      case LAvecbin_VSADD_D:
+      case LAvecbin_VSSUB_B:
+      case LAvecbin_VSSUB_H:
+      case LAvecbin_VSSUB_W:
+      case LAvecbin_VSSUB_D:
+      case LAvecbin_VSADD_BU:
+      case LAvecbin_VSADD_HU:
+      case LAvecbin_VSADD_WU:
+      case LAvecbin_VSADD_DU:
+      case LAvecbin_VSSUB_BU:
+      case LAvecbin_VSSUB_HU:
+      case LAvecbin_VSSUB_WU:
+      case LAvecbin_VSSUB_DU:
+      case LAvecbin_VADDA_B:
+      case LAvecbin_VADDA_H:
+      case LAvecbin_VADDA_W:
+      case LAvecbin_VADDA_D:
+      case LAvecbin_VAVGR_B:
+      case LAvecbin_VAVGR_H:
+      case LAvecbin_VAVGR_W:
+      case LAvecbin_VAVGR_D:
+      case LAvecbin_VAVGR_BU:
+      case LAvecbin_VAVGR_HU:
+      case LAvecbin_VAVGR_WU:
+      case LAvecbin_VAVGR_DU:
+      case LAvecbin_VMAX_B:
+      case LAvecbin_VMAX_H:
+      case LAvecbin_VMAX_W:
+      case LAvecbin_VMAX_D:
+      case LAvecbin_VMIN_B:
+      case LAvecbin_VMIN_H:
+      case LAvecbin_VMIN_W:
+      case LAvecbin_VMIN_D:
+      case LAvecbin_VMAX_BU:
+      case LAvecbin_VMAX_HU:
+      case LAvecbin_VMAX_WU:
+      case LAvecbin_VMAX_DU:
+      case LAvecbin_VMIN_BU:
+      case LAvecbin_VMIN_HU:
+      case LAvecbin_VMIN_WU:
+      case LAvecbin_VMIN_DU:
+      case LAvecbin_VMUL_B:
+      case LAvecbin_VMUL_H:
+      case LAvecbin_VMUL_W:
+      case LAvecbin_VMUH_B:
+      case LAvecbin_VMUH_H:
+      case LAvecbin_VMUH_W:
+      case LAvecbin_VMUH_BU:
+      case LAvecbin_VMUH_HU:
+      case LAvecbin_VMUH_WU:
+      case LAvecbin_VSLL_B:
+      case LAvecbin_VSLL_H:
+      case LAvecbin_VSLL_W:
+      case LAvecbin_VSLL_D:
+      case LAvecbin_VSRL_B:
+      case LAvecbin_VSRL_H:
+      case LAvecbin_VSRL_W:
+      case LAvecbin_VSRL_D:
+      case LAvecbin_VSRA_B:
+      case LAvecbin_VSRA_H:
+      case LAvecbin_VSRA_W:
+      case LAvecbin_VSRA_D:
+      case LAvecbin_VILVL_B:
+      case LAvecbin_VILVL_H:
+      case LAvecbin_VILVL_W:
+      case LAvecbin_VILVL_D:
+      case LAvecbin_VILVH_B:
+      case LAvecbin_VILVH_H:
+      case LAvecbin_VILVH_W:
+      case LAvecbin_VILVH_D:
+      case LAvecbin_VPICKEV_B:
+      case LAvecbin_VPICKEV_H:
+      case LAvecbin_VPICKEV_W:
+      case LAvecbin_VPICKOD_B:
+      case LAvecbin_VPICKOD_H:
+      case LAvecbin_VPICKOD_W:
+      case LAvecbin_VAND_V:
+      case LAvecbin_VOR_V:
+      case LAvecbin_VXOR_V:
+      case LAvecbin_VNOR_V:
+      case LAvecbin_VADD_Q:
+      case LAvecbin_VSUB_Q:
+      case LAvecbin_VFADD_S:
+      case LAvecbin_VFADD_D:
+      case LAvecbin_VFSUB_S:
+      case LAvecbin_VFSUB_D:
+      case LAvecbin_VFMUL_S:
+      case LAvecbin_VFMUL_D:
+      case LAvecbin_VFDIV_S:
+      case LAvecbin_VFDIV_D:
+      case LAvecbin_VFMAX_S:
+      case LAvecbin_VFMAX_D:
+      case LAvecbin_VFMIN_S:
+      case LAvecbin_VFMIN_D:
+         vassert(src2->tag == LAri_Reg);
+         *p++ = emit_binop_rr(op, vregEnc(src2->LAri.R.reg), vregEnc(src1), vregEnc(dst));
+         return p;
+      case LAvecbin_VREPLVE_B:
+      case LAvecbin_VREPLVE_H:
+      case LAvecbin_VREPLVE_W:
+      case LAvecbin_VREPLVE_D:
+         vassert(src2->tag == LAri_Reg);
+         *p++ = emit_binop_rr(op, iregEnc(src2->LAri.R.reg), vregEnc(src1), vregEnc(dst));
+         return p;
+      case LAvecbin_VINSGR2VR_D:
+         vassert(src2->tag == LAri_Imm);
+         *p++ = emit_binop_ri(op, src2->LAri.I.imm, 1, iregEnc(src1), vregEnc(dst));
+         return p;
+      case LAvecbin_VPICKVE2GR_D:
+      case LAvecbin_VPICKVE2GR_DU:
+         vassert(src2->tag == LAri_Imm);
+         *p++ = emit_binop_ri(op, src2->LAri.I.imm, 1, vregEnc(src1), iregEnc(dst));
+         return p;
+      case LAvecbin_VINSGR2VR_W:
+         vassert(src2->tag == LAri_Imm);
+         *p++ = emit_binop_ri(op, src2->LAri.I.imm, 2, iregEnc(src1), vregEnc(dst));
+         return p;
+      case LAvecbin_VPICKVE2GR_W:
+      case LAvecbin_VPICKVE2GR_WU:
+         vassert(src2->tag == LAri_Imm);
+         *p++ = emit_binop_ri(op, src2->LAri.I.imm, 2, vregEnc(src1), iregEnc(dst));
+         return p;
+      case LAvecbin_VSLLI_B:
+      case LAvecbin_VSRLI_B:
+      case LAvecbin_VSRAI_B:
+         vassert(src2->tag == LAri_Imm);
+         *p++ = emit_binop_ri(op, src2->LAri.I.imm, 3, vregEnc(src1), vregEnc(dst));
+         return p;
+      case LAvecbin_VINSGR2VR_H:
+         vassert(src2->tag == LAri_Imm);
+         *p++ = emit_binop_ri(op, src2->LAri.I.imm, 3, iregEnc(src1), vregEnc(dst));
+         return p;
+      case LAvecbin_VPICKVE2GR_HU:
+         vassert(src2->tag == LAri_Imm);
+         *p++ = emit_binop_ri(op, src2->LAri.I.imm, 3, vregEnc(src1), iregEnc(dst));
+         return p;
+      case LAvecbin_VSLLI_H:
+      case LAvecbin_VSRLI_H:
+      case LAvecbin_VSRAI_H:
+         vassert(src2->tag == LAri_Imm);
+         *p++ = emit_binop_ri(op, src2->LAri.I.imm, 4, vregEnc(src1), vregEnc(dst));
+         return p;
+      case LAvecbin_VINSGR2VR_B:
+         vassert(src2->tag == LAri_Imm);
+         *p++ = emit_binop_ri(op, src2->LAri.I.imm, 4, iregEnc(src1), vregEnc(dst));
+         return p;
+      case LAvecbin_VPICKVE2GR_BU:
+         vassert(src2->tag == LAri_Imm);
+         *p++ = emit_binop_ri(op, src2->LAri.I.imm, 4, vregEnc(src1), iregEnc(dst));
+         return p;
+      case LAvecbin_VBSLL_V:
+      case LAvecbin_VBSRL_V:
+      case LAvecbin_VSLLI_W:
+      case LAvecbin_VSRLI_W:
+      case LAvecbin_VSRAI_W:
+         vassert(src2->tag == LAri_Imm);
+         *p++ = emit_binop_ri(op, src2->LAri.I.imm, 5, vregEnc(src1), vregEnc(dst));
+         return p;
+      case LAvecbin_VSLLI_D:
+      case LAvecbin_VSRLI_D:
+      case LAvecbin_VSRAI_D:
+         vassert(src2->tag == LAri_Imm);
+         *p++ = emit_binop_ri(op, src2->LAri.I.imm, 6, vregEnc(src1), vregEnc(dst));
+         return p;
+      case LAvecbin_VORI_B:
+         vassert(src2->tag == LAri_Imm);
+         *p++ = emit_binop_ri(op, src2->LAri.I.imm, 8, vregEnc(src1), vregEnc(dst));
+         return p;
+      default:
+         return NULL;
+   }
+}
+
+static inline UInt* mkVecLoad ( UInt* p, LOONGARCH64VecLoadOp op,
+                                LOONGARCH64AMode* src, HReg dst )
+{
+   switch (op) {
+      case LAvecload_VLD:
+         vassert(src->tag == LAam_RI);
+         *p++ = emit_op_si12_rj_vd(op, src->LAam.RI.index,
+                                   iregEnc(src->LAam.RI.base), vregEnc(dst));
+         return p;
+      case LAvecload_VLDX:
+         vassert(src->tag == LAam_RR);
+         *p++ = emit_op_rk_rj_rd(op, iregEnc(src->LAam.RR.index),
+                                 iregEnc(src->LAam.RR.base), vregEnc(dst));
+         return p;
+      default:
+         return NULL;
+   }
+}
+
+static inline UInt* mkVecStore ( UInt* p, LOONGARCH64VecStoreOp op,
+                                 LOONGARCH64AMode* dst, HReg src )
+{
+   switch (op) {
+      case LAvecstore_VST:
+      vassert(dst->tag == LAam_RI);
+         *p++ = emit_op_si12_rj_vd(op, dst->LAam.RI.index,
+                                   iregEnc(dst->LAam.RI.base), vregEnc(src));
+         return p;
+      case LAvecstore_VSTX:
+         vassert(dst->tag == LAam_RR);
+         *p++ = emit_op_rk_rj_vd(op, iregEnc(dst->LAam.RR.index),
+                                 iregEnc(dst->LAam.RR.base), vregEnc(src));
+         return p;
+      default:
+         return NULL;
+   }
+}
+
+static inline UInt* mkCas ( UInt* p, HReg old, HReg addr, HReg expd,
+                            HReg data, Bool size64 )
+{
+   /*
+         ll.[wd] old, addr, 0
+         bne     old, expd, barrier
+         or      $t0, data, $zero
+         sc.[wd] $t0, addr, 0
+         beq     $t0, zero, fail
+         or      old, expd, $zero
+         b       end
+      barrier:
+         dbar    0
+      fail:
+         or      old, data, $zero
+      end:
+    */
+   UInt o = iregEnc(old);
+   UInt a = iregEnc(addr);
+   UInt e = iregEnc(expd);
+   UInt d = iregEnc(data);
+   UInt t = 12;
+   UInt z = 0;
+
+   if (size64) {
+      *p++ = emit_op_si14_rj_rd(LAllsc_LL_D, 0, a, o);
+   } else {
+      *p++ = emit_op_ui6_rj_rd(LAbin_SLLI_W, 0, e, e); // Sign-extend expd
+      *p++ = emit_op_si14_rj_rd(LAllsc_LL_W, 0, a, o);
+   }
+   *p++ = emit_op_offs16_rj_rd(LAextra_BNE, 6, o, e);
+   *p++ = emit_op_rk_rj_rd(LAbin_OR, z, d, t);
+   if (size64) {
+      *p++ = emit_op_si14_rj_rd(LAllsc_SC_D, 0, a, t);
+   } else {
+      *p++ = emit_op_si14_rj_rd(LAllsc_SC_W, 0, a, t);
+   }
+   *p++ = emit_op_offs16_rj_rd(LAextra_BEQ, 4, t, z);
+   *p++ = emit_op_rk_rj_rd(LAbin_OR, z, e, o);
+   *p++ = emit_op_offs26(LAextra_B, 3);
+   *p++ = emit_op_hint15(LAbar_DBAR, 0);
+   *p++ = emit_op_rk_rj_rd(LAbin_OR, z, d, o);
+   return p;
+}
+
+static inline UInt* mkCmp ( UInt* p, LOONGARCH64CondCode cond,
+                            HReg src2, HReg src1, HReg dst )
+{
+   UInt d  = iregEnc(dst);
+   UInt s1 = iregEnc(src1);
+   UInt s2 = iregEnc(src2);
+
+   switch (cond) {
+      case LAcc_EQ:
+         /*
+            xor   dst, src1, src2
+            sltui dst, dst, 1
+          */
+         *p++ = emit_op_rk_rj_rd(LAbin_XOR, s2, s1, d);
+         *p++ = emit_op_si12_rj_rd(LAextra_SLTUI, 1, d, d);
+         return p;
+      case LAcc_NE:
+         /*
+            xor   dst, src1, src2
+            sltu  dst, $zero, dst
+          */
+         *p++ = emit_op_rk_rj_rd(LAbin_XOR, s2, s1, d);
+         *p++ = emit_op_rk_rj_rd(LAextra_SLTU, d, 0, d);
+         return p;
+      case LAcc_LT:
+         /* slt   dst, src1, src2 */
+         *p++ = emit_op_rk_rj_rd(LAextra_SLT, s2, s1, d);
+         return p;
+      case LAcc_GE:
+         /*
+            slt   dst, src1, src2
+            sltui dst, dst, 1
+          */
+         *p++ = emit_op_rk_rj_rd(LAextra_SLT, s2, s1, d);
+         *p++ = emit_op_si12_rj_rd(LAextra_SLTUI, 1, d, d);
+         return p;
+      case LAcc_LTU:
+         /* sltu  dst, src1, src2 */
+         *p++ = emit_op_rk_rj_rd(LAextra_SLTU, s2, s1, d);
+         return p;
+      case LAcc_GEU:
+         /*
+            sltu  dst, src1, src2
+            sltui dst, dst, 1
+          */
+         *p++ = emit_op_rk_rj_rd(LAextra_SLTU, s2, s1, d);
+         *p++ = emit_op_si12_rj_rd(LAextra_SLTUI, 1, d, d);
+         return p;
+      /* No LAcc_AL here.
+         case LAcc_AL:
+            break;
+       */
+      default:
+         return NULL;
+   }
+}
+
+static inline UInt* mkCMove ( UInt* p, HReg cond, HReg r0,
+                              HReg r1, HReg dst, Bool isInt )
+{
+   if (isInt) {
+      /*
+         masknez $t0, r0, cond
+         maskeqz dst, r1, cond
+         or      dst, $t0, dst
+       */
+      UInt c = iregEnc(cond);
+      UInt d = iregEnc(dst);
+      *p++ = emit_op_rk_rj_rd(LAextra_MASKNEZ, c, iregEnc(r0), 12);
+      *p++ = emit_op_rk_rj_rd(LAextra_MASKEQZ, c, iregEnc(r1), d);
+      *p++ = emit_op_rk_rj_rd(LAbin_OR, d, 12, d);
+   } else {
+      /*
+         movgr2cf $fcc0, cond
+         fsel     dst, r0, r1, $fcc0
+       */
+      *p++ = emit_op_rj_cd(LAextra_MOVGR2CF, iregEnc(cond), 0);
+      *p++ = emit_op_ca_fk_fj_fd(LAextra_FSEL, 0, fregEnc(r1),
+                                 fregEnc(r0), fregEnc(dst));
+   }
+   return p;
+}
+
+static inline UInt* mkCall ( UInt* p, HReg cond, Addr64 target, RetLoc rloc )
+{
+   if (!hregIsInvalid(cond) && rloc.pri != RLPri_None) {
+      /* The call might not happen (it isn't unconditional) and
+         it returns a result.  In this case we will need to
+         generate a control flow diamond to put 0x555..555 in
+         the return register(s) in the case where the call
+         doesn't happen.  If this ever becomes necessary, maybe
+         copy code from the 32-bit ARM equivalent.  Until that
+         day, just give up. */
+      return NULL;
+   }
+
+   UInt* ptmp = NULL;
+   if (!hregIsInvalid(cond)) {
+      /* Create a hole to put a conditional branch in.  We'll
+         patch it once we know the branch length. */
+      ptmp = p;
+      p++;
+   }
+
+   /*
+      $t0 = target
+      jirl $ra, $t0, 0
+    */
+   p = mkLoadImm(p, hregT0(), target);
+   *p++ = emit_op_offs16_rj_rd(LAextra_JIRL, 0, 12, 1);
+
+   /* Patch the hole if necessary */
+   if (!hregIsInvalid(cond)) {
+      vassert(ptmp != NULL);
+      UInt offs = (UInt)(p - ptmp);
+      vassert(offs >= 3 && offs <= 6);
+      /* beq cond, $zero, offs */
+      *ptmp++ = emit_op_offs16_rj_rd(LAextra_BEQ, offs, iregEnc(cond), 0);
+   }
+
+   return p;
+}
+
+static inline UInt* mkXDirect ( UInt* p, Addr64 dstGA,
+                                LOONGARCH64AMode* amPC,
+                                HReg cond, Bool toFastEP,
+                                const void* disp_cp_chain_me_to_slowEP,
+                                const void* disp_cp_chain_me_to_fastEP )
+{
+   /* NB: what goes on here has to be very closely coordinated
+      with chainXDirect_LOONGARCH64 and unchainXDirect_LOONGARCH64 below. */
+   /* We're generating chain-me requests here, so we need to be
+      sure this is actually allowed -- no-redir translations
+      can't use chain-me's.  Hence: */
+   vassert(disp_cp_chain_me_to_slowEP != NULL);
+   vassert(disp_cp_chain_me_to_fastEP != NULL);
+
+   /* Use ptmp for backpatching conditional jumps. */
+   UInt* ptmp = NULL;
+
+   /* First off, if this is conditional, create a conditional
+      jump over the rest of it.  Or at least, leave a space for
+      it that we will shortly fill in. */
+   if (!hregIsInvalid(cond)) {
+      ptmp = p;
+      p++;
+   }
+
+   /* Update the guest PC.
+      $t0 = dstGA
+      st.d $t0, amPC
+    */
+   p = mkLoadImm(p, hregT0(), (ULong)dstGA);
+   p = mkStore(p, LAstore_ST_D, amPC, hregT0());
+
+   /* --- FIRST PATCHABLE BYTE follows --- */
+   /* VG_(disp_cp_chain_me_to_{slowEP,fastEP}) (where we're
+      calling to) backs up the return address, so as to find the
+      address of the first patchable byte.  So: don't change the
+      number of instructions (5) below. */
+   /*
+      la   $t0, VG_(disp_cp_chain_me_to_{slowEP,fastEP})
+      jirl $ra, $t0, 0
+    */
+   const void* disp_cp_chain_me = toFastEP ? disp_cp_chain_me_to_fastEP
+                                           : disp_cp_chain_me_to_slowEP;
+   p = mkLoadImm_EXACTLY4(p, hregT0(), (ULong)(Addr)disp_cp_chain_me);
+   *p++ = emit_op_offs16_rj_rd(LAextra_JIRL, 0, 12, 1);
+   /* --- END of PATCHABLE BYTES --- */
+
+   /* Fix up the conditional jump, if there was one. */
+   if (!hregIsInvalid(cond)) {
+      vassert(ptmp != NULL);
+      UInt offs = (UInt)(p - ptmp);
+      vassert(offs >= 8 && offs <= 11);
+      /* beq cond, $zero, offs */
+      *ptmp++ = emit_op_offs16_rj_rd(LAextra_BEQ, offs, iregEnc(cond), 0);
+   }
+
+   return p;
+}
+
+static inline UInt* mkXIndir ( UInt* p, HReg dstGA, LOONGARCH64AMode* amPC,
+                               HReg cond, const void* disp_cp_xindir )
+{
+   /* We're generating transfers that could lead indirectly to a
+      chain-me, so we need to be sure this is actually allowed --
+      no-redir translations are not allowed to reach normal
+      translations without going through the scheduler.  That means
+      no XDirects or XIndirs out from no-redir translations.
+      Hence: */
+   vassert(disp_cp_xindir != NULL);
+
+   /* Use ptmp for backpatching conditional jumps. */
+   UInt* ptmp = NULL;
+
+   /* First off, if this is conditional, create a conditional
+      jump over the rest of it. */
+   if (!hregIsInvalid(cond)) {
+      ptmp = p;
+      p++;
+   }
+
+   /* Update the guest PC.
+      or   $t0, dstGA, $zero
+      st.d $t0, amPC
+    */
+   *p++ = emit_op_rk_rj_rd(LAbin_OR, 0, iregEnc(dstGA), 12);
+   p = mkStore(p, LAstore_ST_D, amPC, hregT0());
+
+   /*
+      la   $t0, VG_(disp_cp_xindir)
+      jirl $ra, $t0, 0
+    */
+   p = mkLoadImm(p, hregT0(), (ULong)(Addr)disp_cp_xindir);
+   *p++ = emit_op_offs16_rj_rd(LAextra_JIRL, 0, 12, 1);
+
+   /* Fix up the conditional jump, if there was one. */
+   if (!hregIsInvalid(cond)) {
+      vassert(ptmp != NULL);
+      UInt offs = (UInt)(p - ptmp);
+      vassert(offs >= 5 && offs <= 8);
+      /* beq cond, $zero, offs */
+      *ptmp++ = emit_op_offs16_rj_rd(LAextra_BEQ, offs, iregEnc(cond), 0);
+   }
+
+   return p;
+}
+
+static inline UInt* mkXAssisted ( UInt* p, HReg dstGA, LOONGARCH64AMode* amPC,
+                                  HReg cond, IRJumpKind jk,
+                                  const void* disp_cp_xassisted )
+{
+   /* First off, if this is conditional, create a conditional jump
+      over the rest of it.  Or at least, leave a space for it that
+      we will shortly fill in. */
+   UInt* ptmp = NULL;
+   if (!hregIsInvalid(cond)) {
+      ptmp = p;
+      p++;
+   }
+
+   /* Update the guest PC.
+      or   $t0, dstGA, $zero
+      st.d $t0, amPC
+    */
+   *p++ = emit_op_rk_rj_rd(LAbin_OR, 0, iregEnc(dstGA), 12);
+   p = mkStore(p, LAstore_ST_D, amPC, hregT0());
+
+   /* li.w $s8, magic_number */
+   UInt trcval = 0;
+   switch (jk) {
+      case Ijk_Boring:
+         trcval = VEX_TRC_JMP_BORING;
+         break;
+      case Ijk_ClientReq:
+         trcval = VEX_TRC_JMP_CLIENTREQ;
+         break;
+      case Ijk_NoDecode:
+         trcval = VEX_TRC_JMP_NODECODE;
+         break;
+      case Ijk_InvalICache:
+         trcval = VEX_TRC_JMP_INVALICACHE;
+         break;
+      case Ijk_NoRedir:
+         trcval = VEX_TRC_JMP_NOREDIR;
+         break;
+      case Ijk_SigTRAP:
+         trcval = VEX_TRC_JMP_SIGTRAP;
+         break;
+      case Ijk_SigSEGV:
+         trcval = VEX_TRC_JMP_SIGSEGV;
+         break;
+      case Ijk_SigBUS:
+         trcval = VEX_TRC_JMP_SIGBUS;
+         break;
+      case Ijk_SigFPE_IntDiv:
+         trcval = VEX_TRC_JMP_SIGFPE_INTDIV;
+         break;
+      case Ijk_SigFPE_IntOvf:
+         trcval = VEX_TRC_JMP_SIGFPE_INTOVF;
+         break;
+      case Ijk_SigSYS:
+         trcval = VEX_TRC_JMP_SIGSYS;
+         break;
+      case Ijk_Sys_syscall:
+         trcval = VEX_TRC_JMP_SYS_SYSCALL;
+         break;
+      /* We don't expect to see the following being assisted.
+         case Ijk_Call:
+         case Ijk_Ret:
+         case Ijk_Yield:
+         case Ijk_EmWarn:
+         case Ijk_EmFail:
+         case Ijk_MapFail:
+         case Ijk_FlushDCache:
+         case Ijk_SigILL:
+         case Ijk_SigFPE:
+         case Ijk_Sys_int32:
+         case Ijk_Sys_int128:
+         case Ijk_Sys_int129:
+         case Ijk_Sys_int130:
+         case Ijk_Sys_int145:
+         case Ijk_Sys_int210:
+         case Ijk_Sys_sysenter:
+       */
+      default:
+         ppIRJumpKind(jk);
+         vpanic("emit_LOONGARCH64Instr.LAin_XAssisted: unexpected jump kind");
+   }
+   vassert(trcval != 0);
+   p = mkLoadImm(p, hregGSP(), trcval);
+
+   /*
+      la   $t0, VG_(disp_cp_xassisted)
+      jirl $ra, $t0, 0
+    */
+   p = mkLoadImm(p, hregT0(), (ULong)(Addr)disp_cp_xassisted);
+   *p++ = emit_op_offs16_rj_rd(LAextra_JIRL, 0, 12, 1);
+
+   /* Fix up the conditional jump, if there was one. */
+   if (!hregIsInvalid(cond)) {
+      vassert(ptmp != NULL);
+      UInt offs = (UInt)(p - ptmp);
+      vassert(offs >= 6 && offs <= 12);
+      /* beq cond, $zero, offs */
+      *ptmp++ = emit_op_offs16_rj_rd(LAextra_BEQ, offs, iregEnc(cond), 0);
+   }
+
+   return p;
+}
+
+static inline UInt* mkEvCheck ( UInt* p, LOONGARCH64AMode* amCounter,
+                                LOONGARCH64AMode* amFailAddr )
+{
+   UInt* p0 = p;
+
+   /*
+         ld.w   $t0, amCounter
+         addi.d $t0, $t0, -1
+         st.w   $t0, amCounter
+         bge    $t0, $zero, nofail
+         ld.d   $t0, amFailAddr
+         jirl   $ra, $t0, 0
+      nofail:
+   */
+   p = mkLoad(p, LAload_LD_W, amCounter, hregT0());
+   *p++ = emit_op_si12_rj_rd(LAbin_ADDI_D, -1 & 0xfff, 12, 12);
+   p = mkStore(p, LAstore_ST_W, amCounter, hregT0());
+   *p++ = emit_op_offs16_rj_rd(LAextra_BGE, 3, 12, 0);
+   p = mkLoad(p, LAload_LD_W, amFailAddr, hregT0());
+   *p++ = emit_op_offs16_rj_rd(LAextra_JIRL, 0, 12, 1);
+
+   /* Crosscheck */
+   vassert(evCheckSzB_LOONGARCH64() == (UChar*)p - (UChar*)p0);
+   return p;
+}
+
+static inline UInt* mkProfInc ( UInt* p )
+{
+   /*
+      li     $t0, 0x6555755585559555UL
+      ld.d   $t1, $t0, 0
+      addi.d $t1, $t1, 1
+      st.d   $t1, $t0, 0
+    */
+   p = mkLoadImm_EXACTLY4(p, hregT0(), 0x6555755585559555UL);
+   *p++ = emit_op_si12_rj_rd(LAload_LD_D, 0, 12, 13);
+   *p++ = emit_op_si12_rj_rd(LAbin_ADDI_D, 1, 13, 13);
+   *p++ = emit_op_si12_rj_rd(LAstore_ST_D, 0, 12, 13);
+   return p;
+}
+
+/* Emit an instruction into buf and return the number of bytes used.
+   Note that buf is not the insn's final place, and therefore it is
+   imperative to emit position-independent code.  If the emitted
+   instruction was a profiler inc, set *is_profInc to True, else
+   leave it unchanged. */
+Int emit_LOONGARCH64Instr ( /*MB_MOD*/Bool* is_profInc,
+                            UChar* buf,
+                            Int nbuf,
+                            const LOONGARCH64Instr* i,
+                            Bool mode64,
+                            VexEndness endness_host,
+                            const void* disp_cp_chain_me_to_slowEP,
+                            const void* disp_cp_chain_me_to_fastEP,
+                            const void* disp_cp_xindir,
+                            const void* disp_cp_xassisted )
+{
+   vassert(mode64 == True);
+
+   UInt* p = (UInt*)buf;
+   vassert(nbuf >= 32);
+   vassert((((HWord)buf) & 3) == 0);
+
+   switch (i->tag) {
+      case LAin_LI:
+         p = mkLoadImm(p, i->LAin.LI.dst, i->LAin.LI.imm);
+         break;
+      case LAin_Un:
+         p = mkUnary(p, i->LAin.Unary.op, i->LAin.Unary.src,
+                     i->LAin.Unary.dst);
+         break;
+      case LAin_Bin:
+         p = mkBinary(p, i->LAin.Binary.op, i->LAin.Binary.src2,
+                      i->LAin.Binary.src1, i->LAin.Binary.dst);
+         break;
+      case LAin_Load:
+         p = mkLoad(p, i->LAin.Load.op, i->LAin.Load.src,
+                    i->LAin.Load.dst);
+         break;
+      case LAin_Store:
+         p = mkStore(p, i->LAin.Store.op, i->LAin.Store.dst,
+                     i->LAin.Store.src);
+         break;
+      case LAin_LLSC:
+         p = mkLLSC(p, i->LAin.LLSC.op, i->LAin.LLSC.addr, i->LAin.LLSC.val);
+         break;
+      case LAin_Bar:
+         p = mkBar(p, i->LAin.Bar.op, i->LAin.Bar.hint);
+         break;
+      case LAin_FpUn:
+         p = mkFpUnary(p, i->LAin.FpUnary.op, i->LAin.FpUnary.src,
+                       i->LAin.FpUnary.dst);
+         break;
+      case LAin_FpBin:
+         p = mkFpBinary(p, i->LAin.FpBinary.op, i->LAin.FpBinary.src2,
+                        i->LAin.FpBinary.src1, i->LAin.FpBinary.dst);
+         break;
+      case LAin_FpTri:
+         p = mkFpTrinary(p, i->LAin.FpTrinary.op, i->LAin.FpTrinary.src3,
+                         i->LAin.FpTrinary.src2, i->LAin.FpTrinary.src1,
+                         i->LAin.FpTrinary.dst);
+         break;
+      case LAin_FpLoad:
+         p = mkFpLoad(p, i->LAin.FpLoad.op, i->LAin.FpLoad.src,
+                      i->LAin.FpLoad.dst);
+         break;
+      case LAin_FpStore:
+         p = mkFpStore(p, i->LAin.FpStore.op, i->LAin.FpStore.dst,
+                       i->LAin.FpStore.src);
+         break;
+      case LAin_FpMove:
+         p = mkFpMove(p, i->LAin.FpMove.op, i->LAin.FpMove.src,
+                      i->LAin.FpMove.dst);
+         break;
+      case LAin_FpCmp:
+         p = mkFpCmp(p, i->LAin.FpCmp.op, i->LAin.FpCmp.src2,
+                     i->LAin.FpCmp.src1, i->LAin.FpCmp.dst);
+         break;
+      case LAin_VecUn:
+         p = mkVecUnary(p, i->LAin.VecUnary.op, i->LAin.VecUnary.src,
+                        i->LAin.VecUnary.dst);
+         break;
+      case LAin_VecBin:
+         p = mkVecBinary(p, i->LAin.VecBinary.op, i->LAin.VecBinary.src2,
+                         i->LAin.VecBinary.src1, i->LAin.VecBinary.dst);
+         break;
+      case LAin_VecLoad:
+         p = mkVecLoad(p, i->LAin.VecLoad.op, i->LAin.VecLoad.src,
+                       i->LAin.VecLoad.dst);
+         break;
+      case LAin_VecStore:
+         p = mkVecStore(p, i->LAin.VecStore.op, i->LAin.VecStore.dst,
+                        i->LAin.VecStore.src);
+         break;
+      case LAin_Cas:
+         p = mkCas(p, i->LAin.Cas.old, i->LAin.Cas.addr, i->LAin.Cas.expd,
+                   i->LAin.Cas.data, i->LAin.Cas.size64);
+         break;
+      case LAin_Cmp:
+         p = mkCmp(p, i->LAin.Cmp.cond, i->LAin.Cmp.src2,
+                   i->LAin.Cmp.src1, i->LAin.Cmp.dst);
+         break;
+      case LAin_CMove:
+         p = mkCMove(p, i->LAin.CMove.cond, i->LAin.CMove.r0,
+                     i->LAin.CMove.r1, i->LAin.CMove.dst,
+                     i->LAin.CMove.isInt);
+         break;
+      case LAin_Call:
+         p = mkCall(p, i->LAin.Call.cond, i->LAin.Call.target,
+                    i->LAin.Call.rloc);
+         break;
+      case LAin_XDirect:
+         p = mkXDirect(p, i->LAin.XDirect.dstGA, i->LAin.XDirect.amPC,
+                       i->LAin.XDirect.cond, i->LAin.XDirect.toFastEP,
+                       disp_cp_chain_me_to_slowEP,
+                       disp_cp_chain_me_to_fastEP);
+         break;
+      case LAin_XIndir:
+         p = mkXIndir(p, i->LAin.XIndir.dstGA, i->LAin.XIndir.amPC,
+                      i->LAin.XIndir.cond, disp_cp_xindir);
+         break;
+      case LAin_XAssisted:
+         p = mkXAssisted(p, i->LAin.XAssisted.dstGA, i->LAin.XAssisted.amPC,
+                         i->LAin.XAssisted.cond, i->LAin.XAssisted.jk,
+                         disp_cp_xassisted);
+         break;
+      case LAin_EvCheck:
+         p = mkEvCheck(p, i->LAin.EvCheck.amCounter,
+                       i->LAin.EvCheck.amFailAddr);
+         break;
+      case LAin_ProfInc:
+         p = mkProfInc(p);
+         break;
+      default:
+         p = NULL;
+         break;
+   }
+
+   if (p == NULL) {
+      ppLOONGARCH64Instr(i, True);
+      vpanic("emit_LOONGARCH64Instr");
+      /*NOTREACHED*/
+   }
+
+   vassert(((UChar*)p) - &buf[0] <= 48);
+   return ((UChar*)p) - &buf[0];
+}
+
+/* How big is an event check?  See case for mkEvCheck just above.  That
+   crosschecks what this returns, so we can tell if we're inconsistent. */
+Int evCheckSzB_LOONGARCH64 ( void )
+{
+   return 6 * 4; // 6 insns
+}
+
+/* NB: what goes on here has to be very closely coordinated with the
+   emitInstr case for XDirect, above. */
+VexInvalRange chainXDirect_LOONGARCH64 ( VexEndness endness_host,
+                                         void* place_to_chain,
+                                         const void* disp_cp_chain_me_EXPECTED,
+                                         const void* place_to_jump_to )
+{
+   vassert(endness_host == VexEndnessLE);
+
+   /* What we're expecting to see is:
+    *  la $t0, disp_cp_chain_me_to_EXPECTED
+    *  jirl $ra, $t0, 0
+    * viz
+    *  <16 bytes generated by mkLoadImm_EXACTLY4>
+    *  jirl $ra, $t0, 0
+    */
+   UInt* p = (UInt*)place_to_chain;
+   vassert(((HWord)p & 3) == 0);
+   vassert(is_LoadImm_EXACTLY4(p, hregT0(), (ULong)(Addr)disp_cp_chain_me_EXPECTED));
+   vassert(p[4] == emit_op_offs16_rj_rd(LAextra_JIRL, 0, 12, 1));
+
+   /* And what we want to change it to is:
+    *  la $t0, place_to_jump_to
+    *  jirl $ra, $t0, 0
+    * viz
+    *  <16 bytes generated by mkLoadImm_EXACTLY4>
+    *  jirl $ra, $t0, 0
+    *
+    * The replacement has the same length as the original.
+    */
+   p = mkLoadImm_EXACTLY4(p, hregT0(), (ULong)(Addr)place_to_jump_to);
+   *p++ = emit_op_offs16_rj_rd(LAextra_JIRL, 0, 12, 1);
+
+   VexInvalRange vir = { (HWord)place_to_chain, 4 * 4 + 4 };
+   return vir;
+}
+
+/* NB: what goes on here has to be very closely coordinated with the
+   emitInstr case for XDirect, above. */
+VexInvalRange unchainXDirect_LOONGARCH64 ( VexEndness endness_host,
+                                           void* place_to_unchain,
+                                           const void* place_to_jump_to_EXPECTED,
+                                           const void* disp_cp_chain_me )
+{
+   vassert(endness_host == VexEndnessLE);
+
+   /* What we're expecting to see is:
+    *  la $t0, place_to_jump_to_EXPECTED
+    *  jirl $ra, $t0, 0
+    * viz
+    *  <16 bytes generated by mkLoadImm_EXACTLY4>
+    *  jirl $ra, $t0, 0
+    */
+   UInt* p = (UInt*)place_to_unchain;
+   vassert(((HWord)p & 3) == 0);
+   vassert(is_LoadImm_EXACTLY4(p, hregT0(), (ULong)(Addr)place_to_jump_to_EXPECTED));
+   vassert(p[4] == emit_op_offs16_rj_rd(LAextra_JIRL, 0, 12, 1));
+
+   /* And what we want to change it to is:
+    *  la $t0, disp_cp_chain_me
+    *  jirl $ra, $t0, 0
+    * viz
+    *  <16 bytes generated by mkLoadImm_EXACTLY4>
+    *  jirl $ra, $t0, 0
+    *
+    * The replacement has the same length as the original.
+    */
+   p = mkLoadImm_EXACTLY4(p, hregT0(), (ULong)(Addr)disp_cp_chain_me);
+   *p++ = emit_op_offs16_rj_rd(LAextra_JIRL, 0, 12, 1);
+
+   VexInvalRange vir = { (HWord)place_to_unchain, 4 * 4 + 4 };
+   return vir;
+}
+
+/* Patch the counter address into a profile inc point, as previously
+   created by the mkProfInc. */
+VexInvalRange patchProfInc_LOONGARCH64 ( VexEndness endness_host,
+                                         void*  place_to_patch,
+                                         const ULong* location_of_counter )
+{
+   vassert(endness_host == VexEndnessLE);
+   vassert(sizeof(ULong*) == 8);
+
+   /*
+      $t0 = NotKnownYet
+      ld.d   $t1, $t0, 0
+      addi.d $t1, $t1, 1
+      st.d   $t1, $t0, 0
+    */
+   UInt* p = (UInt*)place_to_patch;
+   vassert(((HWord)p & 3) == 0);
+   vassert(is_LoadImm_EXACTLY4(p, hregT0(), 0x6555755585559555UL));
+   vassert(p[4] == emit_op_si12_rj_rd(LAload_LD_D, 0, 12, 13));
+   vassert(p[5] == emit_op_si12_rj_rd(LAbin_ADDI_D, 1, 13, 13));
+   vassert(p[6] == emit_op_si12_rj_rd(LAstore_ST_D, 0, 12, 13));
+
+   p = mkLoadImm_EXACTLY4(p, hregT0(), (ULong)(Addr)location_of_counter);
+
+   VexInvalRange vir = { (HWord)place_to_patch, 4 * 4 };
+   return vir;
+}
+
+
+/*---------------------------------------------------------------*/
+/*--- end                             host_loongarch64_defs.c ---*/
+/*---------------------------------------------------------------*/
diff --git a/VEX/priv/host_loongarch64_defs.h b/VEX/priv/host_loongarch64_defs.h
new file mode 100644
index 0000000..acf38b1
--- /dev/null
+++ b/VEX/priv/host_loongarch64_defs.h
@@ -0,0 +1,919 @@
+
+/*---------------------------------------------------------------*/
+/*--- begin                           host_loongarch64_defs.h ---*/
+/*---------------------------------------------------------------*/
+
+/*
+   This file is part of Valgrind, a dynamic binary instrumentation
+   framework.
+
+   Copyright (C) 2021-2022 Loongson Technology Corporation Limited
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, see <http://www.gnu.org/licenses/>.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#ifndef __VEX_HOST_LOONGARCH64_DEFS_H
+#define __VEX_HOST_LOONGARCH64_DEFS_H
+
+#include "libvex_basictypes.h"
+#include "libvex.h"             /* VexArch */
+#include "host_generic_regs.h"  /* HReg */
+
+
+/* --------- Registers. --------- */
+
+#define ST_IN static inline
+
+/* Integer static registers */
+ST_IN HReg hregLOONGARCH64_R23 ( void ) { return mkHReg(False, HRcInt64, 23,  0); }
+ST_IN HReg hregLOONGARCH64_R24 ( void ) { return mkHReg(False, HRcInt64, 24,  1); }
+ST_IN HReg hregLOONGARCH64_R25 ( void ) { return mkHReg(False, HRcInt64, 25,  2); }
+ST_IN HReg hregLOONGARCH64_R26 ( void ) { return mkHReg(False, HRcInt64, 26,  3); }
+ST_IN HReg hregLOONGARCH64_R27 ( void ) { return mkHReg(False, HRcInt64, 27,  4); }
+ST_IN HReg hregLOONGARCH64_R28 ( void ) { return mkHReg(False, HRcInt64, 28,  5); }
+ST_IN HReg hregLOONGARCH64_R29 ( void ) { return mkHReg(False, HRcInt64, 29,  6); }
+ST_IN HReg hregLOONGARCH64_R30 ( void ) { return mkHReg(False, HRcInt64, 30,  7); }
+/* $r31 is used as guest stack pointer */
+
+/* Integer temporary registers */
+/* $r12 is used as a chaining/ProfInc/Cmove/genSpill/genReload temporary */
+/* $r13 is used as a ProfInc temporary */
+ST_IN HReg hregLOONGARCH64_R14 ( void ) { return mkHReg(False, HRcInt64, 14,  8); }
+ST_IN HReg hregLOONGARCH64_R15 ( void ) { return mkHReg(False, HRcInt64, 15,  9); }
+ST_IN HReg hregLOONGARCH64_R16 ( void ) { return mkHReg(False, HRcInt64, 16, 10); }
+ST_IN HReg hregLOONGARCH64_R17 ( void ) { return mkHReg(False, HRcInt64, 17, 11); }
+ST_IN HReg hregLOONGARCH64_R18 ( void ) { return mkHReg(False, HRcInt64, 18, 12); }
+ST_IN HReg hregLOONGARCH64_R19 ( void ) { return mkHReg(False, HRcInt64, 19, 13); }
+ST_IN HReg hregLOONGARCH64_R20 ( void ) { return mkHReg(False, HRcInt64, 20, 14); }
+
+/* Floating point static registers */
+ST_IN HReg hregLOONGARCH64_F24 ( void ) { return mkHReg(False, HRcFlt64, 24, 15); }
+ST_IN HReg hregLOONGARCH64_F25 ( void ) { return mkHReg(False, HRcFlt64, 25, 16); }
+ST_IN HReg hregLOONGARCH64_F26 ( void ) { return mkHReg(False, HRcFlt64, 26, 17); }
+ST_IN HReg hregLOONGARCH64_F27 ( void ) { return mkHReg(False, HRcFlt64, 27, 18); }
+ST_IN HReg hregLOONGARCH64_F28 ( void ) { return mkHReg(False, HRcFlt64, 28, 19); }
+ST_IN HReg hregLOONGARCH64_F29 ( void ) { return mkHReg(False, HRcFlt64, 29, 20); }
+ST_IN HReg hregLOONGARCH64_F30 ( void ) { return mkHReg(False, HRcFlt64, 30, 21); }
+ST_IN HReg hregLOONGARCH64_F31 ( void ) { return mkHReg(False, HRcFlt64, 31, 22); }
+
+/* Vector static registers */
+ST_IN HReg hregLOONGARCH64_V24 ( void ) { return mkHReg(False, HRcVec128, 24, 23); }
+ST_IN HReg hregLOONGARCH64_V25 ( void ) { return mkHReg(False, HRcVec128, 25, 24); }
+ST_IN HReg hregLOONGARCH64_V26 ( void ) { return mkHReg(False, HRcVec128, 26, 25); }
+ST_IN HReg hregLOONGARCH64_V27 ( void ) { return mkHReg(False, HRcVec128, 27, 26); }
+ST_IN HReg hregLOONGARCH64_V28 ( void ) { return mkHReg(False, HRcVec128, 28, 27); }
+ST_IN HReg hregLOONGARCH64_V29 ( void ) { return mkHReg(False, HRcVec128, 29, 28); }
+ST_IN HReg hregLOONGARCH64_V30 ( void ) { return mkHReg(False, HRcVec128, 30, 29); }
+ST_IN HReg hregLOONGARCH64_V31 ( void ) { return mkHReg(False, HRcVec128, 31, 30); }
+
+/* Other Integer registers */
+ST_IN HReg hregLOONGARCH64_R0  ( void ) { return mkHReg(False, HRcInt64,  0, 31); }
+ST_IN HReg hregLOONGARCH64_R1  ( void ) { return mkHReg(False, HRcInt64,  1, 32); }
+ST_IN HReg hregLOONGARCH64_R2  ( void ) { return mkHReg(False, HRcInt64,  2, 33); }
+ST_IN HReg hregLOONGARCH64_R3  ( void ) { return mkHReg(False, HRcInt64,  3, 34); }
+ST_IN HReg hregLOONGARCH64_R4  ( void ) { return mkHReg(False, HRcInt64,  4, 35); }
+ST_IN HReg hregLOONGARCH64_R5  ( void ) { return mkHReg(False, HRcInt64,  5, 36); }
+ST_IN HReg hregLOONGARCH64_R6  ( void ) { return mkHReg(False, HRcInt64,  6, 37); }
+ST_IN HReg hregLOONGARCH64_R7  ( void ) { return mkHReg(False, HRcInt64,  7, 38); }
+ST_IN HReg hregLOONGARCH64_R8  ( void ) { return mkHReg(False, HRcInt64,  8, 39); }
+ST_IN HReg hregLOONGARCH64_R9  ( void ) { return mkHReg(False, HRcInt64,  9, 40); }
+ST_IN HReg hregLOONGARCH64_R10 ( void ) { return mkHReg(False, HRcInt64, 10, 41); }
+ST_IN HReg hregLOONGARCH64_R11 ( void ) { return mkHReg(False, HRcInt64, 11, 42); }
+ST_IN HReg hregLOONGARCH64_R12 ( void ) { return mkHReg(False, HRcInt64, 12, 43); }
+ST_IN HReg hregLOONGARCH64_R13 ( void ) { return mkHReg(False, HRcInt64, 13, 44); }
+ST_IN HReg hregLOONGARCH64_R21 ( void ) { return mkHReg(False, HRcInt64, 21, 45); }
+ST_IN HReg hregLOONGARCH64_R22 ( void ) { return mkHReg(False, HRcInt64, 22, 46); }
+ST_IN HReg hregLOONGARCH64_R31 ( void ) { return mkHReg(False, HRcInt64, 31, 47); }
+
+/* Special registers */
+ST_IN HReg hregLOONGARCH64_FCSR3 ( void ) { return mkHReg(False, HRcInt32, 3, 48); }
+
+#undef ST_IN
+
+#define hregZERO() hregLOONGARCH64_R0()
+#define hregSP()   hregLOONGARCH64_R3()
+#define hregT0()   hregLOONGARCH64_R12()
+#define hregT1()   hregLOONGARCH64_R13()
+#define hregGSP()  hregLOONGARCH64_R31()
+
+extern UInt ppHRegLOONGARCH64 ( HReg reg );
+
+/* Number of registers used arg passing in function calls */
+#define LOONGARCH64_N_ARGREGS 8 /* a0 ... a7 */
+
+
+/* --------- Condition codes, LOONGARCH64 encoding. --------- */
+typedef enum {
+   LAcc_EQ  = 0, /* equal */
+   LAcc_NE  = 1, /* not equal */
+
+   LAcc_LT  = 2, /* less than (signed) */
+   LAcc_GE  = 3, /* great equal (signed) */
+
+   LAcc_LTU = 4, /* less than (unsigned) */
+   LAcc_GEU = 5, /* great equal (unsigned) */
+
+   LAcc_AL  = 6  /* always (unconditional) */
+} LOONGARCH64CondCode;
+
+
+/* --------- Memory address expressions (amodes). --------- */
+
+typedef enum {
+   LAam_RI, /* Reg + Imm (signed 12-bit) */
+   LAam_RR  /* Reg1 + Reg2 */
+} LOONGARCH64AModeTag;
+
+typedef struct {
+   LOONGARCH64AModeTag tag;
+   union {
+      struct {
+         HReg   base;
+         UShort index;
+      } RI;
+      struct {
+         HReg base;
+         HReg index;
+      } RR;
+   } LAam;
+} LOONGARCH64AMode;
+
+extern LOONGARCH64AMode* LOONGARCH64AMode_RI ( HReg reg, UShort imm );
+extern LOONGARCH64AMode* LOONGARCH64AMode_RR ( HReg base, HReg index );
+
+
+/* --------- Operand, which can be reg or imm. --------- */
+
+typedef enum {
+   LAri_Reg,
+   LAri_Imm
+} LOONGARCH64RITag;
+
+typedef struct {
+   LOONGARCH64RITag tag;
+   union {
+      struct {
+         HReg reg;
+      } R;
+      struct {
+         UShort imm;
+         UChar  size; // size == 5 || size == 6 || size == 12
+         Bool   isSigned;
+      } I;
+   } LAri;
+} LOONGARCH64RI;
+
+extern LOONGARCH64RI* LOONGARCH64RI_R ( HReg reg );
+extern LOONGARCH64RI* LOONGARCH64RI_I ( UShort imm, UChar size, Bool isSigned );
+
+
+/* --------- Instructions. --------- */
+
+/* Tags for unary operations */
+typedef enum {
+   LAun_CLZ_W     = 0x00001400,
+   LAun_CTZ_W     = 0x00001c00,
+   LAun_CLZ_D     = 0x00002400,
+   LAun_CTZ_D     = 0x00002c00,
+   LAun_EXT_W_H   = 0x00005800,
+   LAun_EXT_W_B   = 0x00005c00
+} LOONGARCH64UnOp;
+
+/* Tags for binary operations */
+typedef enum {
+   LAbin_ADD_W     = 0x00100000,
+   LAbin_ADD_D     = 0x00108000,
+   LAbin_SUB_W     = 0x00110000,
+   LAbin_SUB_D     = 0x00118000,
+   LAbin_NOR       = 0x00140000,
+   LAbin_AND       = 0x00148000,
+   LAbin_OR        = 0x00150000,
+   LAbin_XOR       = 0x00158000,
+   LAbin_SLL_W     = 0x00170000,
+   LAbin_SRL_W     = 0x00178000,
+   LAbin_SRA_W     = 0x00180000,
+   LAbin_SLL_D     = 0x00188000,
+   LAbin_SRL_D     = 0x00190000,
+   LAbin_SRA_D     = 0x00198000,
+   LAbin_MUL_W     = 0x001c0000,
+   LAbin_MUL_D     = 0x001d8000,
+   LAbin_MULH_W    = 0x001c8000,
+   LAbin_MULH_WU   = 0x001d0000,
+   LAbin_MULH_D    = 0x001e0000,
+   LAbin_MULH_DU   = 0x001e8000,
+   LAbin_MULW_D_W  = 0x001f0000,
+   LAbin_MULW_D_WU = 0x001f8000,
+   LAbin_DIV_W     = 0x00200000,
+   LAbin_MOD_W     = 0x00208000,
+   LAbin_DIV_WU    = 0x00210000,
+   LAbin_MOD_WU    = 0x00218000,
+   LAbin_DIV_D     = 0x00220000,
+   LAbin_MOD_D     = 0x00228000,
+   LAbin_DIV_DU    = 0x00230000,
+   LAbin_MOD_DU    = 0x00238000,
+   LAbin_SLLI_W    = 0x00408000,
+   LAbin_SLLI_D    = 0x00410000,
+   LAbin_SRLI_W    = 0x00448000,
+   LAbin_SRLI_D    = 0x00450000,
+   LAbin_SRAI_W    = 0x00488000,
+   LAbin_SRAI_D    = 0x00490000,
+   LAbin_ADDI_W    = 0x02800000,
+   LAbin_ADDI_D    = 0x02c00000,
+   LAbin_ANDI      = 0x03400000,
+   LAbin_ORI       = 0x03800000,
+   LAbin_XORI      = 0x03c00000
+} LOONGARCH64BinOp;
+
+/* Tags for load operations */
+typedef enum {
+   LAload_LD_W   = 0x28800000,
+   LAload_LD_D   = 0x28c00000,
+   LAload_LD_BU  = 0x2a000000,
+   LAload_LD_HU  = 0x2a400000,
+   LAload_LD_WU  = 0x2a800000,
+   LAload_LDX_D  = 0x380c0000,
+   LAload_LDX_BU = 0x38200000,
+   LAload_LDX_HU = 0x38240000,
+   LAload_LDX_WU = 0x38280000
+} LOONGARCH64LoadOp;
+
+/* Tags for store operations */
+typedef enum {
+   LAstore_ST_B  = 0x29000000,
+   LAstore_ST_H  = 0x29400000,
+   LAstore_ST_W  = 0x29800000,
+   LAstore_ST_D  = 0x29c00000,
+   LAstore_STX_B = 0x38100000,
+   LAstore_STX_H = 0x38140000,
+   LAstore_STX_W = 0x38180000,
+   LAstore_STX_D = 0x381c0000
+} LOONGARCH64StoreOp;
+
+/* Tags for ll/sc operations */
+typedef enum {
+   LAllsc_LL_W = 0x20000000,
+   LAllsc_SC_W = 0x21000000,
+   LAllsc_LL_D = 0x22000000,
+   LAllsc_SC_D = 0x23000000
+} LOONGARCH64LLSCOp;
+
+/* Tags for barrier operations */
+typedef enum {
+   LAbar_DBAR = 0x38720000,
+   LAbar_IBAR = 0x38728000
+} LOONGARCH64BarOp;
+
+/* Tags for floating point unary operations */
+typedef enum {
+   LAfpun_FABS_S    = 0x01140400,
+   LAfpun_FABS_D    = 0x01140800,
+   LAfpun_FNEG_S    = 0x01141400,
+   LAfpun_FNEG_D    = 0x01141800,
+   LAfpun_FLOGB_S   = 0x01142400,
+   LAfpun_FLOGB_D   = 0x01142800,
+   LAfpun_FSQRT_S   = 0x01144400,
+   LAfpun_FSQRT_D   = 0x01144800,
+   LAfpun_FRSQRT_S  = 0x01146400,
+   LAfpun_FRSQRT_D  = 0x01146800,
+   LAfpun_FCVT_S_D  = 0x01191800,
+   LAfpun_FCVT_D_S  = 0x01192400,
+   LAfpun_FTINT_W_S = 0x011b0400,
+   LAfpun_FTINT_W_D = 0x011b0800,
+   LAfpun_FTINT_L_S = 0x011b2400,
+   LAfpun_FTINT_L_D = 0x011b2800,
+   LAfpun_FFINT_S_W = 0x011d1000,
+   LAfpun_FFINT_S_L = 0x011d1800,
+   LAfpun_FFINT_D_W = 0x011d2000,
+   LAfpun_FFINT_D_L = 0x011d2800,
+   LAfpun_FRINT_S   = 0x011e4400,
+   LAfpun_FRINT_D   = 0x011e4800
+} LOONGARCH64FpUnOp;
+
+/* Tags for floating point binary operations */
+typedef enum {
+   LAfpbin_FADD_S    = 0x01008000,
+   LAfpbin_FADD_D    = 0x01010000,
+   LAfpbin_FSUB_S    = 0x01028000,
+   LAfpbin_FSUB_D    = 0x01030000,
+   LAfpbin_FMUL_S    = 0x01048000,
+   LAfpbin_FMUL_D    = 0x01050000,
+   LAfpbin_FDIV_S    = 0x01068000,
+   LAfpbin_FDIV_D    = 0x01070000,
+   LAfpbin_FMAX_S    = 0x01088000,
+   LAfpbin_FMAX_D    = 0x01090000,
+   LAfpbin_FMIN_S    = 0x010a8000,
+   LAfpbin_FMIN_D    = 0x010b0000,
+   LAfpbin_FMAXA_S   = 0x010c8000,
+   LAfpbin_FMAXA_D   = 0x010d0000,
+   LAfpbin_FMINA_S   = 0x010e8000,
+   LAfpbin_FMINA_D   = 0x010f0000,
+   LAfpbin_FSCALEB_S = 0x01108000,
+   LAfpbin_FSCALEB_D = 0x01110000
+} LOONGARCH64FpBinOp;
+
+/* Tags for floating point trinary operations */
+typedef enum {
+   LAfpbin_FMADD_S = 0x08100000,
+   LAfpbin_FMADD_D = 0x08200000,
+   LAfpbin_FMSUB_S = 0x08500000,
+   LAfpbin_FMSUB_D = 0x08600000
+} LOONGARCH64FpTriOp;
+
+/* Tags for floating point load operations */
+typedef enum {
+   LAfpload_FLD_S  = 0x2b000000,
+   LAfpload_FLD_D  = 0x2b800000,
+   LAfpload_FLDX_S = 0x38300000,
+   LAfpload_FLDX_D = 0x38340000
+} LOONGARCH64FpLoadOp;
+
+/* Tags for floating point store operations */
+typedef enum {
+   LAfpstore_FST_S  = 0x2b400000,
+   LAfpstore_FST_D  = 0x2bc00000,
+   LAfpstore_FSTX_S = 0x38380000,
+   LAfpstore_FSTX_D = 0x383c0000
+} LOONGARCH64FpStoreOp;
+
+/* Tags for floating point move operations */
+typedef enum {
+   LAfpmove_FMOV_S     = 0x01149400,
+   LAfpmove_FMOV_D     = 0x01149800,
+   LAfpmove_MOVGR2FR_W = 0x0114a400,
+   LAfpmove_MOVGR2FR_D = 0x0114a800,
+   LAfpmove_MOVFR2GR_S = 0x0114b400,
+   LAfpmove_MOVFR2GR_D = 0x0114b800,
+   LAfpmove_MOVGR2FCSR = 0x0114c000,
+   LAfpmove_MOVFCSR2GR = 0x0114c800
+} LOONGARCH64FpMoveOp;
+
+/* Tags for floating point compare operations */
+typedef enum {
+   LAfpcmp_FCMP_CLT_S = 0x0c110000,
+   LAfpcmp_FCMP_CLT_D = 0x0c210000,
+   LAfpcmp_FCMP_CEQ_S = 0x0c120000,
+   LAfpcmp_FCMP_CEQ_D = 0x0c220000,
+   LAfpcmp_FCMP_CUN_S = 0x0c140000,
+   LAfpcmp_FCMP_CUN_D = 0x0c240000
+} LOONGARCH64FpCmpOp;
+
+/* Tags for vector unary operations */
+typedef enum {
+   LAvecun_VCLO_B       = 0x729c0000,
+   LAvecun_VCLO_H       = 0x729c0400,
+   LAvecun_VCLO_W       = 0x729c0800,
+   LAvecun_VCLZ_B       = 0x729c1000,
+   LAvecun_VCLZ_H       = 0x729c1400,
+   LAvecun_VCLZ_W       = 0x729c1800,
+   LAvecun_VCLZ_D       = 0x729c1c00,
+   LAvecun_VPCNT_B      = 0x729c2000,
+   LAvecun_VEXTH_H_B    = 0x729ee000,
+   LAvecun_VEXTH_W_H    = 0x729ee400,
+   LAvecun_VEXTH_D_W    = 0x729ee800,
+   LAvecun_VEXTH_Q_D    = 0x729eec00,
+   LAvecun_VEXTH_HU_BU  = 0x729ef000,
+   LAvecun_VEXTH_WU_HU  = 0x729ef400,
+   LAvecun_VEXTH_DU_WU  = 0x729ef800,
+   LAvecun_VEXTH_QU_DU  = 0x729efc00,
+   LAvecun_VREPLGR2VR_B = 0x729f0000,
+   LAvecun_VREPLGR2VR_H = 0x729f0400,
+   LAvecun_VREPLGR2VR_W = 0x729f0800,
+   LAvecun_VREPLGR2VR_D = 0x729f0c00
+} LOONGARCH64VecUnOp;
+
+/* Tags for vector binary operations */
+typedef enum {
+   LAvecbin_VSEQ_B        = 0x70000000,
+   LAvecbin_VSEQ_H        = 0x70008000,
+   LAvecbin_VSEQ_W        = 0x70010000,
+   LAvecbin_VSEQ_D        = 0x70018000,
+   LAvecbin_VSLT_B        = 0x70060000,
+   LAvecbin_VSLT_H        = 0x70068000,
+   LAvecbin_VSLT_W        = 0x70070000,
+   LAvecbin_VSLT_D        = 0x70078000,
+   LAvecbin_VSLT_BU       = 0x70080000,
+   LAvecbin_VSLT_HU       = 0x70088000,
+   LAvecbin_VSLT_WU       = 0x70090000,
+   LAvecbin_VSLT_DU       = 0x70098000,
+   LAvecbin_VADD_B        = 0x700a0000,
+   LAvecbin_VADD_H        = 0x700a8000,
+   LAvecbin_VADD_W        = 0x700b0000,
+   LAvecbin_VADD_D        = 0x700b8000,
+   LAvecbin_VSUB_B        = 0x700c0000,
+   LAvecbin_VSUB_H        = 0x700c8000,
+   LAvecbin_VSUB_W        = 0x700d0000,
+   LAvecbin_VSUB_D        = 0x700d8000,
+   LAvecbin_VSADD_B       = 0x70460000,
+   LAvecbin_VSADD_H       = 0x70468000,
+   LAvecbin_VSADD_W       = 0x70470000,
+   LAvecbin_VSADD_D       = 0x70478000,
+   LAvecbin_VSSUB_B       = 0x70480000,
+   LAvecbin_VSSUB_H       = 0x70488000,
+   LAvecbin_VSSUB_W       = 0x70490000,
+   LAvecbin_VSSUB_D       = 0x70498000,
+   LAvecbin_VSADD_BU      = 0x704a0000,
+   LAvecbin_VSADD_HU      = 0x704a8000,
+   LAvecbin_VSADD_WU      = 0x704b0000,
+   LAvecbin_VSADD_DU      = 0x704b8000,
+   LAvecbin_VSSUB_BU      = 0x704c0000,
+   LAvecbin_VSSUB_HU      = 0x704c8000,
+   LAvecbin_VSSUB_WU      = 0x704d0000,
+   LAvecbin_VSSUB_DU      = 0x704d8000,
+   LAvecbin_VADDA_B       = 0x705c0000,
+   LAvecbin_VADDA_H       = 0x705c8000,
+   LAvecbin_VADDA_W       = 0x705d0000,
+   LAvecbin_VADDA_D       = 0x705d8000,
+   LAvecbin_VAVGR_B       = 0x70680000,
+   LAvecbin_VAVGR_H       = 0x70688000,
+   LAvecbin_VAVGR_W       = 0x70690000,
+   LAvecbin_VAVGR_D       = 0x70698000,
+   LAvecbin_VAVGR_BU      = 0x706a0000,
+   LAvecbin_VAVGR_HU      = 0x706a8000,
+   LAvecbin_VAVGR_WU      = 0x706b0000,
+   LAvecbin_VAVGR_DU      = 0x706b8000,
+   LAvecbin_VMAX_B        = 0x70700000,
+   LAvecbin_VMAX_H        = 0x70708000,
+   LAvecbin_VMAX_W        = 0x70710000,
+   LAvecbin_VMAX_D        = 0x70718000,
+   LAvecbin_VMIN_B        = 0x70720000,
+   LAvecbin_VMIN_H        = 0x70728000,
+   LAvecbin_VMIN_W        = 0x70730000,
+   LAvecbin_VMIN_D        = 0x70738000,
+   LAvecbin_VMAX_BU       = 0x70740000,
+   LAvecbin_VMAX_HU       = 0x70748000,
+   LAvecbin_VMAX_WU       = 0x70750000,
+   LAvecbin_VMAX_DU       = 0x70758000,
+   LAvecbin_VMIN_BU       = 0x70760000,
+   LAvecbin_VMIN_HU       = 0x70768000,
+   LAvecbin_VMIN_WU       = 0x70770000,
+   LAvecbin_VMIN_DU       = 0x70778000,
+   LAvecbin_VMUL_B        = 0x70840000,
+   LAvecbin_VMUL_H        = 0x70848000,
+   LAvecbin_VMUL_W        = 0x70850000,
+   LAvecbin_VMUH_B        = 0x70860000,
+   LAvecbin_VMUH_H        = 0x70868000,
+   LAvecbin_VMUH_W        = 0x70870000,
+   LAvecbin_VMUH_BU       = 0x70880000,
+   LAvecbin_VMUH_HU       = 0x70888000,
+   LAvecbin_VMUH_WU       = 0x70890000,
+   LAvecbin_VSLL_B        = 0x70e80000,
+   LAvecbin_VSLL_H        = 0x70e88000,
+   LAvecbin_VSLL_W        = 0x70e90000,
+   LAvecbin_VSLL_D        = 0x70e98000,
+   LAvecbin_VSRL_B        = 0x70ea0000,
+   LAvecbin_VSRL_H        = 0x70ea8000,
+   LAvecbin_VSRL_W        = 0x70eb0000,
+   LAvecbin_VSRL_D        = 0x70eb8000,
+   LAvecbin_VSRA_B        = 0x70ec0000,
+   LAvecbin_VSRA_H        = 0x70ec8000,
+   LAvecbin_VSRA_W        = 0x70ed0000,
+   LAvecbin_VSRA_D        = 0x70ed8000,
+   LAvecbin_VILVL_B       = 0x711a0000,
+   LAvecbin_VILVL_H       = 0x711a8000,
+   LAvecbin_VILVL_W       = 0x711b0000,
+   LAvecbin_VILVL_D       = 0x711b8000,
+   LAvecbin_VILVH_B       = 0x711c0000,
+   LAvecbin_VILVH_H       = 0x711c8000,
+   LAvecbin_VILVH_W       = 0x711d0000,
+   LAvecbin_VILVH_D       = 0x711d8000,
+   LAvecbin_VPICKEV_B     = 0x711e0000,
+   LAvecbin_VPICKEV_H     = 0x711e8000,
+   LAvecbin_VPICKEV_W     = 0x711f0000,
+   LAvecbin_VPICKOD_B     = 0x71200000,
+   LAvecbin_VPICKOD_H     = 0x71208000,
+   LAvecbin_VPICKOD_W     = 0x71210000,
+   LAvecbin_VREPLVE_B     = 0x71220000,
+   LAvecbin_VREPLVE_H     = 0x71228000,
+   LAvecbin_VREPLVE_W     = 0x71230000,
+   LAvecbin_VREPLVE_D     = 0x71238000,
+   LAvecbin_VAND_V        = 0x71260000,
+   LAvecbin_VOR_V         = 0x71268000,
+   LAvecbin_VXOR_V        = 0x71270000,
+   LAvecbin_VNOR_V        = 0x71278000,
+   LAvecbin_VADD_Q        = 0x712d0000,
+   LAvecbin_VSUB_Q        = 0x712d8000,
+   LAvecbin_VFADD_S       = 0x71308000,
+   LAvecbin_VFADD_D       = 0x71310000,
+   LAvecbin_VFSUB_S       = 0x71328000,
+   LAvecbin_VFSUB_D       = 0x71330000,
+   LAvecbin_VFMUL_S       = 0x71388000,
+   LAvecbin_VFMUL_D       = 0x71390000,
+   LAvecbin_VFDIV_S       = 0x713a8000,
+   LAvecbin_VFDIV_D       = 0x713b0000,
+   LAvecbin_VFMAX_S       = 0x713c8000,
+   LAvecbin_VFMAX_D       = 0x713d0000,
+   LAvecbin_VFMIN_S       = 0x713e8000,
+   LAvecbin_VFMIN_D       = 0x713f0000,
+   LAvecbin_VBSLL_V       = 0x728e0000,
+   LAvecbin_VBSRL_V       = 0x728e8000,
+   LAvecbin_VINSGR2VR_B   = 0x72eb8000,
+   LAvecbin_VINSGR2VR_H   = 0x72ebc000,
+   LAvecbin_VINSGR2VR_W   = 0x72ebe000,
+   LAvecbin_VINSGR2VR_D   = 0x72ebf000,
+   LAvecbin_VPICKVE2GR_W  = 0x72efe000,
+   LAvecbin_VPICKVE2GR_D  = 0x72eff000,
+   LAvecbin_VPICKVE2GR_BU = 0x72f38000,
+   LAvecbin_VPICKVE2GR_HU = 0x72f3c000,
+   LAvecbin_VPICKVE2GR_WU = 0x72f3e000,
+   LAvecbin_VPICKVE2GR_DU = 0x72f3f000,
+   LAvecbin_VSLLI_B       = 0x732c2000,
+   LAvecbin_VSLLI_H       = 0x732c4000,
+   LAvecbin_VSLLI_W       = 0x732c8000,
+   LAvecbin_VSLLI_D       = 0x732d0000,
+   LAvecbin_VSRLI_B       = 0x73302000,
+   LAvecbin_VSRLI_H       = 0x73304000,
+   LAvecbin_VSRLI_W       = 0x73308000,
+   LAvecbin_VSRLI_D       = 0x73310000,
+   LAvecbin_VSRAI_B       = 0x73342000,
+   LAvecbin_VSRAI_H       = 0x73344000,
+   LAvecbin_VSRAI_W       = 0x73348000,
+   LAvecbin_VSRAI_D       = 0x73350000,
+   LAvecbin_VORI_B        = 0x73d40000
+} LOONGARCH64VecBinOp;
+
+/* Tags for vector load operations */
+typedef enum {
+   LAvecload_VLD  = 0x2c000000,
+   LAvecload_VLDX = 0x38400000
+} LOONGARCH64VecLoadOp;
+
+/* Tags for vector store operations */
+typedef enum {
+   LAvecstore_VST  = 0x2c400000,
+   LAvecstore_VSTX = 0x38440000
+} LOONGARCH64VecStoreOp;
+
+/* Tags for extra operations, we only use them when emiting code directly */
+typedef enum {
+   LAextra_MOVGR2CF = 0x0114d800,
+   LAextra_MOVCF2GR = 0x0114dc00,
+   LAextra_SLT      = 0x00120000,
+   LAextra_SLTU     = 0x00128000,
+   LAextra_MASKEQZ  = 0x00130000,
+   LAextra_MASKNEZ  = 0x00138000,
+   LAextra_SLTI     = 0x02000000,
+   LAextra_SLTUI    = 0x02400000,
+   LAextra_LU52I_D  = 0x03000000,
+   LAextra_FSEL     = 0x0d000000,
+   LAextra_LU12I_W  = 0x14000000,
+   LAextra_LU32I_D  = 0x16000000,
+   LAextra_JIRL     = 0x4c000000,
+   LAextra_B        = 0x50000000,
+   LAextra_BEQ      = 0x58000000,
+   LAextra_BNE      = 0x5c000000,
+   LAextra_BGE      = 0x64000000
+} LOONGARCH64ExtraOp;
+
+/* Tags for instructions */
+typedef enum {
+   /* Pseudo-insn, used for generating a 64-bit
+      literal to register */
+   LAin_LI,         /* load imm */
+
+   /* Integer insns */
+   LAin_Un,         /* unary */
+   LAin_Bin,        /* binary */
+   LAin_Load,       /* load */
+   LAin_Store,      /* store */
+   LAin_LLSC,       /* ll/sc */
+   LAin_Bar,        /* barrier */
+
+   /* Floating point insns */
+   LAin_FpUn,       /* floating point unary */
+   LAin_FpBin,      /* floating point binary */
+   LAin_FpTri,      /* floating point trinary */
+   LAin_FpLoad,     /* floating point load */
+   LAin_FpStore,    /* floating point store */
+   LAin_FpMove,     /* floating point move */
+   LAin_FpCmp,      /* floating point compare */
+
+   /* Vector insns */
+   LAin_VecUn,       /* vector unary */
+   LAin_VecBin,      /* vector binary */
+   LAin_VecLoad,     /* vector load */
+   LAin_VecStore,    /* vector store */
+
+   /* Pseudo-insn */
+   LAin_Cas,        /* compare and swap */
+   LAin_Cmp,        /* word compare */
+   LAin_CMove,      /* condition move */
+
+   /* Call target (an absolute address), on given
+      condition (which could be LAcc_AL). */
+   LAin_Call,       /* call */
+
+   /* The following 5 insns are mandated by translation chaining */
+   LAin_XDirect,    /* direct transfer to GA */
+   LAin_XIndir,     /* indirect transfer to GA */
+   LAin_XAssisted,  /* assisted transfer to GA */
+   LAin_EvCheck,    /* Event check */
+   LAin_ProfInc     /* 64-bit profile counter increment */
+} LOONGARCH64InstrTag;
+
+typedef struct {
+   LOONGARCH64InstrTag tag;
+   union {
+      struct {
+         ULong                imm;
+         HReg                 dst;
+      } LI;
+      struct {
+         LOONGARCH64UnOp      op;
+         HReg                 src;
+         HReg                 dst;
+      } Unary;
+      struct {
+         LOONGARCH64BinOp     op;
+         LOONGARCH64RI*       src2;
+         HReg                 src1;
+         HReg                 dst;
+      } Binary;
+      struct {
+         LOONGARCH64LoadOp    op;
+         LOONGARCH64AMode*    src;
+         HReg                 dst;
+      } Load;
+      struct {
+         LOONGARCH64StoreOp   op;
+         LOONGARCH64AMode*    dst;
+         HReg                 src;
+      } Store;
+      struct {
+         LOONGARCH64LLSCOp    op;
+         Bool                 isLoad;
+         LOONGARCH64AMode*    addr;
+         HReg                 val;
+      } LLSC;
+      struct {
+         LOONGARCH64BarOp     op;
+         UShort               hint;
+      } Bar;
+      struct {
+         LOONGARCH64FpUnOp    op;
+         HReg                 src;
+         HReg                 dst;
+      } FpUnary;
+      struct {
+         LOONGARCH64FpBinOp   op;
+         HReg                 src2;
+         HReg                 src1;
+         HReg                 dst;
+      } FpBinary;
+      struct {
+         LOONGARCH64FpTriOp   op;
+         HReg                 src3;
+         HReg                 src2;
+         HReg                 src1;
+         HReg                 dst;
+      } FpTrinary;
+      struct {
+         LOONGARCH64FpLoadOp  op;
+         LOONGARCH64AMode*    src;
+         HReg                 dst;
+      } FpLoad;
+      struct {
+         LOONGARCH64FpStoreOp op;
+         LOONGARCH64AMode*    dst;
+         HReg                 src;
+      } FpStore;
+      struct {
+         LOONGARCH64FpMoveOp  op;
+         HReg                 src;
+         HReg                 dst;
+      } FpMove;
+      struct {
+         LOONGARCH64FpCmpOp   op;
+         HReg                 src2;
+         HReg                 src1;
+         HReg                 dst;
+      } FpCmp;
+      struct {
+         LOONGARCH64VecUnOp   op;
+         HReg                 src;
+         HReg                 dst;
+      } VecUnary;
+      struct {
+         LOONGARCH64VecBinOp  op;
+         LOONGARCH64RI*       src2;
+         HReg                 src1;
+         HReg                 dst;
+      } VecBinary;
+      struct {
+         LOONGARCH64VecLoadOp op;
+         LOONGARCH64AMode*    src;
+         HReg                 dst;
+      } VecLoad;
+      struct {
+         LOONGARCH64VecStoreOp op;
+         LOONGARCH64AMode*     dst;
+         HReg                  src;
+      } VecStore;
+      struct {
+         HReg                 old;
+         HReg                 addr;
+         HReg                 expd;
+         HReg                 data;
+         Bool                 size64;
+      } Cas;
+      struct {
+         LOONGARCH64CondCode  cond;
+         HReg                 dst;
+         HReg                 src1;
+         HReg                 src2;
+      } Cmp;
+      struct {
+         HReg                 cond;
+         HReg                 r0;
+         HReg                 r1;
+         HReg                 dst;
+         Bool                 isInt;
+      } CMove;
+      struct {
+         HReg                 cond;
+         Addr64               target;
+         UInt                 nArgRegs;
+         RetLoc               rloc;
+      } Call;
+      struct {
+         Addr64               dstGA;
+         LOONGARCH64AMode*    amPC;
+         HReg                 cond;
+         Bool                 toFastEP;
+      } XDirect;
+      struct {
+         HReg                 dstGA;
+         LOONGARCH64AMode*    amPC;
+         HReg                 cond;
+      } XIndir;
+      struct {
+         HReg                 dstGA;
+         LOONGARCH64AMode*    amPC;
+         HReg                 cond;
+         IRJumpKind           jk;
+      } XAssisted;
+      struct {
+         LOONGARCH64AMode*    amCounter;
+         LOONGARCH64AMode*    amFailAddr;
+      } EvCheck;
+      struct {
+         /* No fields.  The address of the counter to inc is
+            installed later, post-translation, by patching it in,
+            as it is not known at translation time. */
+      } ProfInc;
+   } LAin;
+} LOONGARCH64Instr;
+
+extern LOONGARCH64Instr* LOONGARCH64Instr_LI        ( ULong imm, HReg dst );
+extern LOONGARCH64Instr* LOONGARCH64Instr_Unary     ( LOONGARCH64UnOp op,
+                                                      HReg src, HReg dst );
+extern LOONGARCH64Instr* LOONGARCH64Instr_Binary    ( LOONGARCH64BinOp op,
+                                                      LOONGARCH64RI* src2,
+                                                      HReg src1, HReg dst );
+extern LOONGARCH64Instr* LOONGARCH64Instr_Load      ( LOONGARCH64LoadOp op,
+                                                      LOONGARCH64AMode* src,
+                                                      HReg dst );
+extern LOONGARCH64Instr* LOONGARCH64Instr_Store     ( LOONGARCH64StoreOp op,
+                                                      LOONGARCH64AMode* dst,
+                                                      HReg src );
+extern LOONGARCH64Instr* LOONGARCH64Instr_LLSC      ( LOONGARCH64LLSCOp op,
+                                                      Bool isLoad,
+                                                      LOONGARCH64AMode* addr,
+                                                      HReg val );
+extern LOONGARCH64Instr* LOONGARCH64Instr_Bar       ( LOONGARCH64BarOp op,
+                                                      UShort hint );
+extern LOONGARCH64Instr* LOONGARCH64Instr_FpUnary   ( LOONGARCH64FpUnOp op,
+                                                      HReg src, HReg dst );
+extern LOONGARCH64Instr* LOONGARCH64Instr_FpBinary  ( LOONGARCH64FpBinOp op,
+                                                      HReg src2, HReg src1,
+                                                      HReg dst );
+extern LOONGARCH64Instr* LOONGARCH64Instr_FpTrinary ( LOONGARCH64FpTriOp op,
+                                                      HReg src3, HReg src2,
+                                                      HReg src1, HReg dst );
+extern LOONGARCH64Instr* LOONGARCH64Instr_FpLoad    ( LOONGARCH64FpLoadOp op,
+                                                      LOONGARCH64AMode* src,
+                                                      HReg dst );
+extern LOONGARCH64Instr* LOONGARCH64Instr_FpStore   ( LOONGARCH64FpStoreOp op,
+                                                      LOONGARCH64AMode* dst,
+                                                      HReg src );
+extern LOONGARCH64Instr* LOONGARCH64Instr_FpMove    ( LOONGARCH64FpMoveOp op,
+                                                      HReg src, HReg dst );
+extern LOONGARCH64Instr* LOONGARCH64Instr_FpCmp     ( LOONGARCH64FpCmpOp op,
+                                                      HReg src2, HReg src1,
+                                                      HReg dst );
+extern LOONGARCH64Instr* LOONGARCH64Instr_VecUnary  ( LOONGARCH64VecUnOp op,
+                                                      HReg src, HReg dst );
+extern LOONGARCH64Instr* LOONGARCH64Instr_VecBinary ( LOONGARCH64VecBinOp op,
+                                                      LOONGARCH64RI* src2,
+                                                      HReg src1, HReg dst );
+extern LOONGARCH64Instr* LOONGARCH64Instr_VecLoad   ( LOONGARCH64VecLoadOp op,
+                                                      LOONGARCH64AMode* src,
+                                                      HReg dst );
+extern LOONGARCH64Instr* LOONGARCH64Instr_VecStore  ( LOONGARCH64VecStoreOp op,
+                                                      LOONGARCH64AMode* dst,
+                                                      HReg src );
+extern LOONGARCH64Instr* LOONGARCH64Instr_Cas       ( HReg old, HReg addr,
+                                                      HReg expd, HReg data,
+                                                      Bool size64 );
+extern LOONGARCH64Instr* LOONGARCH64Instr_Cmp       ( LOONGARCH64CondCode cond,
+                                                      HReg src2, HReg src1,
+                                                      HReg dst );
+extern LOONGARCH64Instr* LOONGARCH64Instr_CMove     ( HReg cond, HReg r0, HReg r1,
+                                                      HReg dst, Bool isInt );
+extern LOONGARCH64Instr* LOONGARCH64Instr_Call      ( HReg cond, Addr64 target,
+                                                      UInt nArgRegs, RetLoc rloc );
+extern LOONGARCH64Instr* LOONGARCH64Instr_XDirect   ( Addr64 dstGA,
+                                                      LOONGARCH64AMode* amPC,
+                                                      HReg cond, Bool toFastEP );
+extern LOONGARCH64Instr* LOONGARCH64Instr_XIndir    ( HReg dstGA,
+                                                      LOONGARCH64AMode* amPC,
+                                                      HReg cond );
+extern LOONGARCH64Instr* LOONGARCH64Instr_XAssisted ( HReg dstGA,
+                                                      LOONGARCH64AMode* amPC,
+                                                      HReg cond, IRJumpKind jk );
+extern LOONGARCH64Instr* LOONGARCH64Instr_EvCheck   ( LOONGARCH64AMode* amCounter,
+                                                      LOONGARCH64AMode* amFailAddr );
+extern LOONGARCH64Instr* LOONGARCH64Instr_ProfInc   ( void );
+
+extern void ppLOONGARCH64Instr ( const LOONGARCH64Instr* i, Bool mode64 );
+
+/* Some functions that insulate the register allocator from details
+   of the underlying instruction set. */
+extern void getRegUsage_LOONGARCH64Instr ( HRegUsage* u,
+                                           const LOONGARCH64Instr* i,
+                                           Bool mode64 );
+extern void mapRegs_LOONGARCH64Instr ( HRegRemap* m, LOONGARCH64Instr* i,
+                                       Bool mode64 );
+extern Int emit_LOONGARCH64Instr (/*MB_MOD*/Bool* is_profInc,
+                                  UChar* buf,
+                                  Int nbuf,
+                                  const LOONGARCH64Instr* i,
+                                  Bool mode64,
+                                  VexEndness endness_host,
+                                  const void* disp_cp_chain_me_to_slowEP,
+                                  const void* disp_cp_chain_me_to_fastEP,
+                                  const void* disp_cp_xindir,
+                                  const void* disp_cp_xassisted );
+
+extern void genSpill_LOONGARCH64 ( /*OUT*/ HInstr** i1, /*OUT*/ HInstr** i2,
+                                   HReg rreg, Int offsetB, Bool mode64);
+extern void genReload_LOONGARCH64 ( /*OUT*/ HInstr** i1, /*OUT*/ HInstr** i2,
+                                    HReg rreg, Int offsetB, Bool mode64);
+extern LOONGARCH64Instr* genMove_LOONGARCH64 ( HReg from, HReg to,
+                                               Bool mode64 );
+
+extern const RRegUniverse* getRRegUniverse_LOONGARCH64 ( void );
+
+extern HInstrArray* iselSB_LOONGARCH64 ( const IRSB*,
+                                         VexArch,
+                                         const VexArchInfo*,
+                                         const VexAbiInfo*,
+                                         Int offs_Host_EvC_Counter,
+                                         Int offs_Host_EvC_FailAddr,
+                                         Bool chainingAllowed,
+                                         Bool addProfInc,
+                                         Addr max_ga );
+
+/* How big is an event check?  See case for Min_EvCheck in
+   emit_LOONGARCH64Instr just above.  That crosschecks what this returns,
+   so we can tell if we're inconsistent. */
+extern Int evCheckSzB_LOONGARCH64 ( void );
+
+/* NB: what goes on here has to be very closely coordinated with the
+   emitInstr case for XDirect, above. */
+extern VexInvalRange chainXDirect_LOONGARCH64 ( VexEndness endness_host,
+                                                void* place_to_chain,
+                                                const void* disp_cp_chain_me_EXPECTED,
+                                                const void* place_to_jump_to );
+
+/* NB: what goes on here has to be very closely coordinated with the
+   emitInstr case for XDirect, above. */
+extern VexInvalRange unchainXDirect_LOONGARCH64 ( VexEndness endness_host,
+                                                  void* place_to_unchain,
+                                                  const void* place_to_jump_to_EXPECTED,
+                                                  const void* disp_cp_chain_me );
+
+/* Patch the counter address into a profile inc point, as previously
+   created by the Min_ProfInc case for emit_LOONGARCH64Instr. */
+extern VexInvalRange patchProfInc_LOONGARCH64 ( VexEndness endness_host,
+                                                void*  place_to_patch,
+                                                const ULong* location_of_counter );
+
+#endif /* ndef __VEX_HOST_LOONGARCH64_DEFS_H */
+
+
+/*---------------------------------------------------------------*/
+/*--- end                             host-loongarch64_defs.h ---*/
+/*---------------------------------------------------------------*/
diff --git a/VEX/priv/host_loongarch64_isel.c b/VEX/priv/host_loongarch64_isel.c
new file mode 100644
index 0000000..d60a2b3
--- /dev/null
+++ b/VEX/priv/host_loongarch64_isel.c
@@ -0,0 +1,3748 @@
+
+/*---------------------------------------------------------------*/
+/*--- begin                           host_loongarch64_isel.c ---*/
+/*---------------------------------------------------------------*/
+
+/*
+   This file is part of Valgrind, a dynamic binary instrumentation
+   framework.
+
+   Copyright (C) 2021-2022 Loongson Technology Corporation Limited
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details->
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, see <http://www.gnu.org/licenses/>.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "libvex_basictypes.h"
+#include "libvex_ir.h"
+#include "libvex.h"
+
+#include "main_util.h"
+#include "main_globals.h"
+#include "host_generic_regs.h"
+#include "host_loongarch64_defs.h"
+
+
+/*---------------------------------------------------------*/
+/*--- ISelEnv                                           ---*/
+/*---------------------------------------------------------*/
+
+/* This carries around:
+
+   - A mapping from IRTemp to IRType, giving the type of any IRTemp we
+     might encounter.  This is computed before insn selection starts,
+     and does not change.
+
+   - A mapping from IRTemp to HReg.  This tells the insn selector
+     which virtual register is associated with each IRTemp temporary.
+     This is computed before insn selection starts, and does not
+     change.  We expect this mapping to map precisely the same set of
+     IRTemps as the type mapping does.
+
+     |vregmap|   holds the primary register for the IRTemp.
+     |vregmapHI| is only used for 128-bit integer-typed
+                 IRTemps.  It holds the identity of a second
+                 64-bit virtual HReg, which holds the high half
+                 of the value.
+
+   - The code array, that is, the insns selected so far.
+
+   - A counter, for generating new virtual registers.
+
+   - The host hardware capabilities word.  This is set at the start
+     and does not change.
+
+   - A Bool for indicating whether we may generate chain-me
+     instructions for control flow transfers, or whether we must use
+     XAssisted.
+
+   - The maximum guest address of any guest insn in this block.
+     Actually, the address of the highest-addressed byte from any insn
+     in this block.  Is set at the start and does not change.  This is
+     used for detecting jumps which are definitely forward-edges from
+     this block, and therefore can be made (chained) to the fast entry
+     point of the destination, thereby avoiding the destination's
+     event check.
+
+    - An IRExpr*, which may be NULL, holding the IR expression (an
+      IRRoundingMode-encoded value) to which the FPU's rounding mode
+      was most recently set.  Setting to NULL is always safe.  Used to
+      avoid redundant settings of the FPU's rounding mode, as
+      described in set_FPCR_rounding_mode below.
+
+   Note, this is all (well, mostly) host-independent.
+*/
+
+typedef
+   struct {
+      /* Constant -- are set at the start and do not change. */
+      IRTypeEnv*   type_env;
+
+      HReg*        vregmap;
+      HReg*        vregmapHI;
+      Int          n_vregmap;
+
+      UInt         hwcaps;
+
+      Bool         chainingAllowed;
+      Addr64       max_ga;
+
+      /* These are modified as we go along. */
+      HInstrArray* code;
+      Int          vreg_ctr;
+   }
+   ISelEnv;
+
+
+static HReg lookupIRTemp ( ISelEnv* env, IRTemp tmp )
+{
+   vassert(tmp < env->n_vregmap);
+   return env->vregmap[tmp];
+}
+
+static void lookupIRTempPair ( HReg* vrHI, HReg* vrLO,
+                               ISelEnv* env, IRTemp tmp )
+{
+   vassert(tmp < env->n_vregmap);
+   vassert(!hregIsInvalid(env->vregmapHI[tmp]));
+   *vrLO = env->vregmap[tmp];
+   *vrHI = env->vregmapHI[tmp];
+}
+
+static void addInstr ( ISelEnv* env, LOONGARCH64Instr* instr )
+{
+   addHInstr(env->code, instr);
+   if (vex_traceflags & VEX_TRACE_VCODE) {
+      ppLOONGARCH64Instr(instr, True);
+      vex_printf("\n");
+   }
+}
+
+static HReg newVRegI ( ISelEnv* env )
+{
+   HReg reg = mkHReg(True/*virtual reg*/, HRcInt64, 0, env->vreg_ctr);
+   env->vreg_ctr++;
+   return reg;
+}
+
+static HReg newVRegF ( ISelEnv* env )
+{
+   HReg reg = mkHReg(True/*virtual reg*/, HRcFlt64, 0, env->vreg_ctr);
+   env->vreg_ctr++;
+   return reg;
+}
+
+static HReg newVRegV ( ISelEnv* env )
+{
+   HReg reg = mkHReg(True/*virtual reg*/, HRcVec128, 0, env->vreg_ctr);
+   env->vreg_ctr++;
+   return reg;
+}
+
+
+/*---------------------------------------------------------*/
+/*--- ISEL: Forward declarations                        ---*/
+/*---------------------------------------------------------*/
+
+/* These are organised as iselXXX and iselXXX_wrk pairs.  The
+   iselXXX_wrk do the real work, but are not to be called directly.
+   For each XXX, iselXXX calls its iselXXX_wrk counterpart, then
+   checks that all returned registers are virtual.  You should not
+   call the _wrk version directly.
+*/
+
+static LOONGARCH64AMode*   iselIntExpr_AMode_wrk ( ISelEnv* env,
+                                                   IRExpr* e, IRType dty );
+static LOONGARCH64AMode*   iselIntExpr_AMode     ( ISelEnv* env,
+                                                   IRExpr* e, IRType dty );
+
+static LOONGARCH64RI*      iselIntExpr_RI_wrk    ( ISelEnv* env, IRExpr* e,
+                                                   UChar size, Bool isSigned );
+static LOONGARCH64RI*      iselIntExpr_RI        ( ISelEnv* env, IRExpr* e,
+                                                   UChar size, Bool isSigned );
+
+static HReg                iselIntExpr_R_wrk     ( ISelEnv* env, IRExpr* e );
+static HReg                iselIntExpr_R         ( ISelEnv* env, IRExpr* e );
+
+static HReg                iselCondCode_R_wrk    ( ISelEnv* env, IRExpr* e );
+static HReg                iselCondCode_R        ( ISelEnv* env, IRExpr* e );
+
+static void                iselInt128Expr_wrk    ( HReg* hi, HReg* lo,
+                                                   ISelEnv* env, IRExpr* e );
+static void                iselInt128Expr        ( HReg* hi, HReg* lo,
+                                                   ISelEnv* env, IRExpr* e );
+
+static HReg                iselFltExpr_wrk        ( ISelEnv* env, IRExpr* e );
+static HReg                iselFltExpr            ( ISelEnv* env, IRExpr* e );
+
+static HReg                iselV128Expr_wrk       ( ISelEnv* env, IRExpr* e );
+static HReg                iselV128Expr           ( ISelEnv* env, IRExpr* e );
+
+static void                iselV256Expr_wrk       ( HReg* hi, HReg* lo,
+                                                    ISelEnv* env, IRExpr* e );
+static void                iselV256Expr           ( HReg* hi, HReg* lo,
+                                                    ISelEnv* env, IRExpr* e );
+
+
+/*---------------------------------------------------------*/
+/*--- ISEL: Misc helpers                                ---*/
+/*---------------------------------------------------------*/
+
+/* Generate move insn */
+static LOONGARCH64Instr* LOONGARCH64Instr_Move ( HReg to, HReg from )
+{
+   LOONGARCH64RI *ri = LOONGARCH64RI_R(hregZERO());
+   return LOONGARCH64Instr_Binary(LAbin_OR, ri, from, to);
+}
+
+/* Generate vector move insn */
+static LOONGARCH64Instr* LOONGARCH64Instr_VecMove ( HReg to, HReg from )
+{
+   LOONGARCH64RI *ri = LOONGARCH64RI_I(0, 8, False);
+   return LOONGARCH64Instr_VecBinary(LAvecbin_VORI_B, ri, from, to);
+}
+
+/* Generate LOONGARCH64AMode from HReg and UInt */
+static LOONGARCH64AMode* mkLOONGARCH64AMode_RI ( HReg reg, UInt imm )
+{
+   vassert(imm < (1 << 12));
+   return LOONGARCH64AMode_RI(reg, (UShort)imm);
+}
+
+/* Set floating point rounding mode */
+static void set_rounding_mode ( ISelEnv* env, IRExpr* mode )
+{
+   /*
+      rounding mode | LOONGARCH | IR
+      ------------------------------
+      to nearest    | 00        | 00
+      to zero       | 01        | 11
+      to +infinity  | 10        | 10
+      to -infinity  | 11        | 01
+   */
+
+   /* rm = XOR(rm, (rm << 1)) & 3 */
+   HReg            rm = iselIntExpr_R(env, mode);
+   HReg           tmp = newVRegI(env);
+   LOONGARCH64RI*  ri = LOONGARCH64RI_I(1, 5, False);
+   LOONGARCH64RI* ri2 = LOONGARCH64RI_R(rm);
+   LOONGARCH64RI* ri3 = LOONGARCH64RI_I(3, 12, False);
+   addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_W, ri, rm, tmp));
+   addInstr(env, LOONGARCH64Instr_Binary(LAbin_XOR, ri2, tmp, rm));
+   addInstr(env, LOONGARCH64Instr_Binary(LAbin_ANDI, ri3, rm, rm));
+
+   /* Save old value of FCSR3 */
+   HReg fcsr = newVRegI(env);
+   addInstr(env, LOONGARCH64Instr_FpMove(LAfpmove_MOVFCSR2GR,
+                                         hregLOONGARCH64_FCSR3(), fcsr));
+
+   /* Store old FCSR3 to stack */
+   LOONGARCH64RI* ri4 = LOONGARCH64RI_I(-4 & 0xfff, 12, True);
+   addInstr(env, LOONGARCH64Instr_Binary(LAbin_ADDI_D, ri4, hregSP(), hregSP()));
+   LOONGARCH64AMode* am = LOONGARCH64AMode_RI(hregSP(), 0);
+   addInstr(env, LOONGARCH64Instr_Store(LAstore_ST_W, am, fcsr));
+
+   /* Set new value of FCSR3 */
+   LOONGARCH64RI* ri5 = LOONGARCH64RI_I(8, 5, False);
+   addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_W, ri5, rm, rm));
+   addInstr(env, LOONGARCH64Instr_FpMove(LAfpmove_MOVGR2FCSR,
+                                         rm, hregLOONGARCH64_FCSR3()));
+}
+
+static void set_rounding_mode_default ( ISelEnv* env )
+{
+   /* Load old FCSR3 from stack */
+   HReg fcsr = newVRegI(env);
+   LOONGARCH64AMode* am = LOONGARCH64AMode_RI(hregSP(), 0);
+   addInstr(env, LOONGARCH64Instr_Load(LAload_LD_WU, am, fcsr));
+
+   /* Restore SP */
+   LOONGARCH64RI* ri = LOONGARCH64RI_I(4, 12, True);
+   addInstr(env, LOONGARCH64Instr_Binary(LAbin_ADDI_D, ri, hregSP(), hregSP()));
+
+   /* Set new value of FCSR3 */
+   addInstr(env, LOONGARCH64Instr_FpMove(LAfpmove_MOVGR2FCSR,
+                                         fcsr, hregLOONGARCH64_FCSR3()));
+}
+
+/* Convert LOONGARCH FCMP cond to IR result */
+static HReg convert_cond_to_IR ( ISelEnv* env, HReg src2, HReg src1, Bool size64 )
+{
+   HReg tmp = newVRegI(env);
+   HReg dst = newVRegI(env);
+
+   LOONGARCH64RI* ri1 = LOONGARCH64RI_I(63, 6, False);
+   LOONGARCH64RI* ri2 = LOONGARCH64RI_I(0x45, 12, False);
+   if (size64)
+      addInstr(env, LOONGARCH64Instr_FpCmp(LAfpcmp_FCMP_CUN_D, src2, src1, tmp));
+   else
+      addInstr(env, LOONGARCH64Instr_FpCmp(LAfpcmp_FCMP_CUN_S, src2, src1, tmp));
+   addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_D, ri1, tmp, tmp));
+   addInstr(env, LOONGARCH64Instr_Binary(LAbin_SRAI_D, ri1, tmp, tmp));
+   addInstr(env, LOONGARCH64Instr_Binary(LAbin_ANDI, ri2, tmp, dst));
+
+   LOONGARCH64RI* ri3 = LOONGARCH64RI_I(0x1, 12, False);
+   LOONGARCH64RI* ri4 = LOONGARCH64RI_R(tmp);
+   if (size64)
+      addInstr(env, LOONGARCH64Instr_FpCmp(LAfpcmp_FCMP_CLT_D, src2, src1, tmp));
+   else
+      addInstr(env, LOONGARCH64Instr_FpCmp(LAfpcmp_FCMP_CLT_S, src2, src1, tmp));
+   addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_D, ri1, tmp, tmp));
+   addInstr(env, LOONGARCH64Instr_Binary(LAbin_SRAI_D, ri1, tmp, tmp));
+   addInstr(env, LOONGARCH64Instr_Binary(LAbin_ANDI, ri3, tmp, tmp));
+   addInstr(env, LOONGARCH64Instr_Binary(LAbin_OR, ri4, dst, dst));
+
+   LOONGARCH64RI* ri5 = LOONGARCH64RI_I(0x40, 12, False);
+   if (size64)
+      addInstr(env, LOONGARCH64Instr_FpCmp(LAfpcmp_FCMP_CEQ_D, src2, src1, tmp));
+   else
+      addInstr(env, LOONGARCH64Instr_FpCmp(LAfpcmp_FCMP_CEQ_S, src2, src1, tmp));
+   addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_D, ri1, tmp, tmp));
+   addInstr(env, LOONGARCH64Instr_Binary(LAbin_SRAI_D, ri1, tmp, tmp));
+   addInstr(env, LOONGARCH64Instr_Binary(LAbin_ANDI, ri5, tmp, tmp));
+   addInstr(env, LOONGARCH64Instr_Binary(LAbin_OR, ri4, dst, dst));
+
+   return dst;
+}
+
+
+/*---------------------------------------------------------*/
+/*--- ISEL: Function call helpers                       ---*/
+/*---------------------------------------------------------*/
+
+/* Used only in doHelperCall.  See big comment in doHelperCall re
+   handling of register-parameter args.  This function figures out
+   whether evaluation of an expression might require use of a fixed
+   register.  If in doubt return True (safe but suboptimal).
+*/
+static Bool mightRequireFixedRegs ( IRExpr* e )
+{
+   if (UNLIKELY(is_IRExpr_VECRET_or_GSPTR(e))) {
+      // These are always "safe" -- either a copy of SP in some
+      // arbitrary vreg, or a copy of $r31, respectively.
+      return False;
+   }
+   /* Else it's a "normal" expression. */
+   switch (e->tag) {
+      case Iex_RdTmp: case Iex_Const: case Iex_Get:
+         return False;
+      default:
+         return True;
+   }
+}
+
+/* Do a complete function call.  |guard| is a Ity_Bit expression
+   indicating whether or not the call happens.  If guard==NULL, the
+   call is unconditional.  |retloc| is set to indicate where the
+   return value is after the call.  The caller (of this fn) must
+   generate code to add |stackAdjustAfterCall| to the stack pointer
+   after the call is done.  Returns True iff it managed to handle this
+   combination of arg/return types, else returns False. */
+static Bool doHelperCall( /*OUT*/UInt* stackAdjustAfterCall,
+                          /*OUT*/RetLoc* retloc,
+                          ISelEnv* env,
+                          IRExpr* guard,
+                          IRCallee* cee, IRType retTy, IRExpr** args )
+{
+   HReg          cond;
+   HReg          argregs[LOONGARCH64_N_ARGREGS];
+   HReg          tmpregs[LOONGARCH64_N_ARGREGS];
+   Bool          go_fast;
+   Int           n_args, i, nextArgReg;
+   Addr64        target;
+
+   vassert(LOONGARCH64_N_ARGREGS == 8);
+
+   /* Set default returns.  We'll update them later if needed. */
+   *stackAdjustAfterCall = 0;
+   *retloc               = mk_RetLoc_INVALID();
+
+   /* These are used for cross-checking that IR-level constraints on
+      the use of IRExpr_VECRET() and IRExpr_GSPTR() are observed. */
+   UInt nVECRETs = 0;
+   UInt nGSPTRs  = 0;
+
+   /* Marshal args for a call and do the call.
+
+      This function only deals with a tiny set of possibilities, which
+      cover all helpers in practice.  The restrictions are that only
+      arguments in registers are supported, hence only
+      LOONGARCH64_N_ARGREGS x 64 integer bits in total can be passed.
+      In fact the only supported arg type is I64.
+
+      The return type can be I{64,32} or V{128,256}.  In the latter two
+      cases, it is expected that |args| will contain the special node
+      IRExpr_VECRET(), in which case this routine generates code to
+      allocate space on the stack for the vector return value.  Since
+      we are not passing any scalars on the stack, it is enough to
+      preallocate the return space before marshalling any arguments,
+      in this case.
+
+      |args| may also contain IRExpr_GSPTR(), in which case the
+      value in $r31 is passed as the corresponding argument.
+
+      Generating code which is both efficient and correct when
+      parameters are to be passed in registers is difficult, for the
+      reasons elaborated in detail in comments attached to
+      doHelperCall() in priv/host_x86_isel.c.  Here, we use a variant
+      of the method described in those comments.
+
+      The problem is split into two cases: the fast scheme and the
+      slow scheme.  In the fast scheme, arguments are computed
+      directly into the target (real) registers.  This is only safe
+      when we can be sure that computation of each argument will not
+      trash any real registers set by computation of any other
+      argument.
+
+      In the slow scheme, all args are first computed into vregs, and
+      once they are all done, they are moved to the relevant real
+      regs.  This always gives correct code, but it also gives a bunch
+      of vreg-to-rreg moves which are usually redundant but are hard
+      for the register allocator to get rid of.
+
+      To decide which scheme to use, all argument expressions are
+      first examined.  If they are all so simple that it is clear they
+      will be evaluated without use of any fixed registers, use the
+      fast scheme, else use the slow scheme.  Note also that only
+      unconditional calls may use the fast scheme, since having to
+      compute a condition expression could itself trash real
+      registers.
+
+      Note this requires being able to examine an expression and
+      determine whether or not evaluation of it might use a fixed
+      register.  That requires knowledge of how the rest of this insn
+      selector works.  Currently just the following 3 are regarded as
+      safe -- hopefully they cover the majority of arguments in
+      practice: IRExpr_Tmp IRExpr_Const IRExpr_Get.
+   */
+
+   /* LOONGARCH64 calling convention: up to eight registers ($a0 ... $a7)
+      are allowed to be used for passing integer arguments.  They correspond
+      to regs $r4 ... $r11.  Note that the cee->regparms field is meaningless
+      on LOONGARCH64 host (since we only implement one calling convention)
+      and so we always ignore it. */
+
+   n_args = 0;
+   for (i = 0; args[i]; i++) {
+      IRExpr* arg = args[i];
+      if (UNLIKELY(arg->tag == Iex_VECRET)) {
+         nVECRETs++;
+      } else if (UNLIKELY(arg->tag == Iex_GSPTR)) {
+         nGSPTRs++;
+      }
+      n_args++;
+   }
+
+   /* If this fails, the IR is ill-formed */
+   vassert(nGSPTRs == 0 || nGSPTRs == 1);
+
+   /* If we have a VECRET, allocate space on the stack for the return
+      value, and record the stack pointer after that. */
+   HReg r_vecRetAddr = INVALID_HREG;
+   LOONGARCH64RI* ri;
+   if (nVECRETs == 1) {
+      vassert(retTy == Ity_V128 || retTy == Ity_V256);
+      r_vecRetAddr = newVRegI(env);
+      if (retTy == Ity_V128)
+         ri = LOONGARCH64RI_I(-16 & 0xfff, 12, True);
+      else // retTy == Ity_V256
+         ri = LOONGARCH64RI_I(-32 & 0xfff, 12, True);
+      addInstr(env, LOONGARCH64Instr_Binary(LAbin_ADDI_D, ri, hregSP(), hregSP()));
+      addInstr(env, LOONGARCH64Instr_Move(r_vecRetAddr, hregSP()));
+   } else {
+      // If either of these fail, the IR is ill-formed
+      vassert(retTy != Ity_V128 && retTy != Ity_V256);
+      vassert(nVECRETs == 0);
+   }
+
+   if (n_args > LOONGARCH64_N_ARGREGS) {
+      vpanic("doHelperCall(loongarch64): cannot currently handle > 8 args");
+   }
+
+   argregs[0] = hregLOONGARCH64_R4();
+   argregs[1] = hregLOONGARCH64_R5();
+   argregs[2] = hregLOONGARCH64_R6();
+   argregs[3] = hregLOONGARCH64_R7();
+   argregs[4] = hregLOONGARCH64_R8();
+   argregs[5] = hregLOONGARCH64_R9();
+   argregs[6] = hregLOONGARCH64_R10();
+   argregs[7] = hregLOONGARCH64_R11();
+
+   tmpregs[0] = tmpregs[1] = tmpregs[2] = tmpregs[3] = INVALID_HREG;
+   tmpregs[4] = tmpregs[5] = tmpregs[6] = tmpregs[7] = INVALID_HREG;
+
+   /* First decide which scheme (slow or fast) is to be used. First assume the
+      fast scheme, and select slow if any contraindications (wow) appear. */
+
+   go_fast = True;
+
+   if (guard) {
+      if (guard->tag == Iex_Const
+          && guard->Iex.Const.con->tag == Ico_U1
+          && guard->Iex.Const.con->Ico.U1 == True) {
+         /* unconditional */
+      } else {
+         /* Not manifestly unconditional -- be conservative. */
+         go_fast = False;
+      }
+   }
+
+   if (go_fast) {
+      for (i = 0; i < n_args; i++) {
+         if (mightRequireFixedRegs(args[i])) {
+            go_fast = False;
+            break;
+         }
+      }
+   }
+
+   if (go_fast) {
+      if (retTy == Ity_V128 || retTy == Ity_V256)
+         go_fast = False;
+   }
+
+   /* At this point the scheme to use has been established.  Generate
+      code to get the arg values into the argument rregs.  If we run
+      out of arg regs, give up. */
+
+   if (go_fast) {
+      /* FAST SCHEME */
+      nextArgReg = 0;
+
+      for (i = 0; i < n_args; i++) {
+         IRExpr* arg = args[i];
+
+         IRType aTy = Ity_INVALID;
+         if (LIKELY(!is_IRExpr_VECRET_or_GSPTR(arg)))
+            aTy = typeOfIRExpr(env->type_env, args[i]);
+
+         if (nextArgReg >= LOONGARCH64_N_ARGREGS)
+            return False; /* out of argregs */
+
+         if (aTy == Ity_I64) {
+            addInstr(env, LOONGARCH64Instr_Move(argregs[nextArgReg],
+                                                iselIntExpr_R(env, args[i])));
+            nextArgReg++;
+         } else if (arg->tag == Iex_GSPTR) {
+            addInstr(env, LOONGARCH64Instr_Move(argregs[nextArgReg], hregGSP()));
+            nextArgReg++;
+         } else if (arg->tag == Iex_VECRET) {
+            // because of the go_fast logic above, we can't get here,
+            // since vector return values makes us use the slow path
+            // instead.
+            vassert(0);
+         } else
+            return False; /* unhandled arg type */
+      }
+
+      /* Fast scheme only applies for unconditional calls.  Hence: */
+      cond = INVALID_HREG;
+   } else {
+      /* SLOW SCHEME; move via temporaries */
+      nextArgReg = 0;
+
+      for (i = 0; i < n_args; i++) {
+         IRExpr* arg = args[i];
+
+         IRType  aTy = Ity_INVALID;
+         if (LIKELY(!is_IRExpr_VECRET_or_GSPTR(arg)))
+            aTy = typeOfIRExpr(env->type_env, args[i]);
+
+         if (nextArgReg >= LOONGARCH64_N_ARGREGS)
+            return False; /* out of argregs */
+
+         if (aTy == Ity_I64) {
+            tmpregs[nextArgReg] = iselIntExpr_R(env, args[i]);
+            nextArgReg++;
+         } else if (arg->tag == Iex_GSPTR) {
+            tmpregs[nextArgReg] = hregGSP();
+            nextArgReg++;
+         } else if (arg->tag == Iex_VECRET) {
+            vassert(!hregIsInvalid(r_vecRetAddr));
+            tmpregs[nextArgReg] = r_vecRetAddr;
+            nextArgReg++;
+         } else
+            return False; /* unhandled arg type */
+      }
+
+      /* Now we can compute the condition.  We can't do it earlier
+         because the argument computations could trash the condition
+         codes.  Be a bit clever to handle the common case where the
+         guard is 1:Bit. */
+      cond = INVALID_HREG;
+      if (guard) {
+         if (guard->tag == Iex_Const
+             && guard->Iex.Const.con->tag == Ico_U1
+             && guard->Iex.Const.con->Ico.U1 == True) {
+            /* unconditional -- do nothing */
+         } else {
+            cond = iselCondCode_R(env, guard);
+         }
+      }
+
+      /* Move the args to their final destinations. */
+      for (i = 0; i < nextArgReg; i++) {
+         vassert(!(hregIsInvalid(tmpregs[i])));
+         /* None of these insns, including any spill code that might
+            be generated, may alter the condition codes. */
+         addInstr(env, LOONGARCH64Instr_Move(argregs[i], tmpregs[i]));
+      }
+   }
+
+   /* Should be assured by checks above */
+   vassert(nextArgReg <= LOONGARCH64_N_ARGREGS);
+
+   /* Do final checks, set the return values, and generate the call
+      instruction proper. */
+   vassert(nGSPTRs == 0 || nGSPTRs == 1);
+   vassert(nVECRETs == ((retTy == Ity_V128 || retTy == Ity_V256) ? 1 : 0));
+   vassert(*stackAdjustAfterCall == 0);
+   vassert(is_RetLoc_INVALID(*retloc));
+   switch (retTy) {
+      case Ity_INVALID:
+         /* Function doesn't return a value. */
+         *retloc = mk_RetLoc_simple(RLPri_None);
+         break;
+      case Ity_I8: case Ity_I16: case Ity_I32: case Ity_I64:
+         *retloc = mk_RetLoc_simple(RLPri_Int);
+         break;
+      case Ity_V128:
+         *retloc = mk_RetLoc_spRel(RLPri_V128SpRel, 0);
+         *stackAdjustAfterCall = 16;
+         break;
+      case Ity_V256:
+         *retloc = mk_RetLoc_spRel(RLPri_V256SpRel, 0);
+         *stackAdjustAfterCall = 32;
+         break;
+      default:
+         /* IR can denote other possible return types, but we don't
+            handle those here. */
+         vassert(0);
+         break;
+   }
+
+   /* Finally, generate the call itself.  This needs the *retloc value
+      set in the switch above, which is why it's at the end. */
+
+   /* nextArgReg doles out argument registers.  Since these are
+      assigned in the order $a0 .. $a7, its numeric value at this point,
+      which must be between 0 and 8 inclusive, is going to be equal to
+      the number of arg regs in use for the call.  Hence bake that
+      number into the call (we'll need to know it when doing register
+      allocation, to know what regs the call reads.) */
+
+   target = (Addr)cee->addr;
+   addInstr(env, LOONGARCH64Instr_Call(cond, target, nextArgReg, *retloc));
+
+   return True; /* success */
+}
+
+
+/*---------------------------------------------------------*/
+/*--- ISEL: Integer expressions (64/32/16/8 bit)        ---*/
+/*---------------------------------------------------------*/
+
+/* Select insns for an integer-typed expression, and add them to the
+   code list.  Return a reg holding the result.  This reg will be a
+   virtual register.  THE RETURNED REG MUST NOT BE MODIFIED.  If you
+   want to modify it, ask for a new vreg, copy it in there, and modify
+   the copy.  The register allocator will do its best to map both
+   vregs to the same real register, so the copies will often disappear
+   later in the game.
+
+   This should handle expressions of 64, 32, 16 and 8-bit type.
+   All results are returned in a (mode64 ? 64bit : 32bit) register.
+   For 16- and 8-bit expressions, the upper (32/48/56 : 16/24) bits
+   are arbitrary, so you should mask or sign extend partial values
+   if necessary.
+*/
+
+/* --------------------- AMode --------------------- */
+
+static LOONGARCH64AMode* iselIntExpr_AMode ( ISelEnv* env,
+                                             IRExpr* e, IRType dty )
+{
+   LOONGARCH64AMode* am = iselIntExpr_AMode_wrk(env, e, dty);
+
+   /* sanity checks ... */
+   switch (am->tag) {
+      case LAam_RI:
+         vassert(am->LAam.RI.index < (1 << 12));
+         vassert(hregClass(am->LAam.RI.base) == HRcInt64);
+         vassert(hregIsVirtual(am->LAam.RI.base));
+         break;
+      case LAam_RR:
+         vassert(hregClass(am->LAam.RR.base) == HRcInt64);
+         vassert(hregIsVirtual(am->LAam.RR.base));
+         vassert(hregClass(am->LAam.RR.index) == HRcInt64);
+         vassert(hregIsVirtual(am->LAam.RR.index));
+         break;
+      default:
+         vpanic("iselIntExpr_AMode: unknown LOONGARCH64 AMode tag");
+         break;
+   }
+
+   return am;
+}
+
+/* DO NOT CALL THIS DIRECTLY ! */
+static LOONGARCH64AMode* iselIntExpr_AMode_wrk ( ISelEnv* env,
+                                                 IRExpr* e, IRType dty )
+{
+   IRType ty = typeOfIRExpr(env->type_env, e);
+   vassert(e);
+   vassert(ty == Ity_I64);
+
+   /* Add64(expr, i), where i <= 0x7ff */
+   if (e->tag == Iex_Binop && e->Iex.Binop.op == Iop_Add64
+       && e->Iex.Binop.arg2->tag == Iex_Const
+       && e->Iex.Binop.arg2->Iex.Const.con->tag == Ico_U64
+       && e->Iex.Binop.arg2->Iex.Const.con->Ico.U64 <= 0x7ff) {
+      return LOONGARCH64AMode_RI(iselIntExpr_R(env, e->Iex.Binop.arg1),
+                                 (UShort)e->Iex.Binop.arg2->Iex.Const.con->Ico.U64);
+   }
+
+   /* Add64(expr, expr) */
+   if (e->tag == Iex_Binop && e->Iex.Binop.op == Iop_Add64) {
+      HReg base = iselIntExpr_R(env, e->Iex.Binop.arg1);
+      HReg index = iselIntExpr_R(env, e->Iex.Binop.arg2);
+      return LOONGARCH64AMode_RR(base, index);
+   }
+
+   /* Doesn't match anything in particular.  Generate it into
+      a register and use that. */
+   return LOONGARCH64AMode_RI(iselIntExpr_R(env, e), 0);
+}
+
+/* --------------------- RI --------------------- */
+
+static LOONGARCH64RI* iselIntExpr_RI ( ISelEnv* env, IRExpr* e,
+                                       UChar size, Bool isSigned )
+{
+   LOONGARCH64RI* ri = iselIntExpr_RI_wrk(env, e, size, isSigned);
+
+   /* sanity checks ... */
+   switch (ri->tag) {
+      case LAri_Imm:
+         switch (ri->LAri.I.size) {
+            case 0 ... 4:
+            case 6 ... 7:
+               vassert(ri->LAri.I.isSigned == False);
+               break;
+            case 9 ... 11:
+               vassert(ri->LAri.I.isSigned == True);
+               break;
+            case 5: case 8: case 12:
+               break;
+            default:
+               break;
+               vassert(0);
+         }
+         vassert(ri->LAri.I.imm < (1 << ri->LAri.I.size));
+         break;
+      case LAri_Reg:
+         vassert(hregClass(ri->LAri.R.reg) == HRcInt64);
+         vassert(hregIsVirtual(ri->LAri.R.reg));
+         break;
+      default:
+         vpanic("iselIntExpr_RI: unknown LOONGARCH64 RI tag");
+         break;
+   }
+
+   return ri;
+}
+
+/* DO NOT CALL THIS DIRECTLY ! */
+static LOONGARCH64RI* iselIntExpr_RI_wrk ( ISelEnv* env, IRExpr* e,
+                                           UChar size, Bool isSigned )
+{
+   IRType ty = typeOfIRExpr(env->type_env, e);
+   vassert(e);
+   vassert(ty == Ity_I8 || ty == Ity_I16 || ty == Ity_I32 || ty == Ity_I64);
+
+   LOONGARCH64RI *ri = NULL;
+
+   /* special case: immediate */
+   if (e->tag == Iex_Const) {
+      switch (e->Iex.Const.con->tag) {
+         case Ico_U8:
+            if (!isSigned && e->Iex.Const.con->Ico.U8 < (1 << size)) {
+               UShort imm = e->Iex.Const.con->Ico.U8;
+               ri = LOONGARCH64RI_I(imm, size, isSigned);
+            }
+            break;
+         case Ico_U32:
+            if (!isSigned && e->Iex.Const.con->Ico.U32 < (1 << size)) {
+               UShort imm = e->Iex.Const.con->Ico.U32;
+               ri = LOONGARCH64RI_I(imm, size, isSigned);
+            }
+            break;
+         case Ico_U64:
+            if (!isSigned && e->Iex.Const.con->Ico.U64 < (1 << size)) {
+               UShort imm = e->Iex.Const.con->Ico.U64;
+               ri = LOONGARCH64RI_I(imm, size, isSigned);
+            }
+            break;
+         default:
+            break;
+      }
+      /* else fail, fall through to default case */
+   }
+
+   if (ri == NULL) {
+      /* default case: calculate into a register and return that */
+      HReg reg = iselIntExpr_R(env, e);
+      ri = LOONGARCH64RI_R(reg);
+   }
+
+   return ri;
+}
+
+/* --------------------- Reg --------------------- */
+
+static HReg iselIntExpr_R ( ISelEnv* env, IRExpr* e )
+{
+   HReg r = iselIntExpr_R_wrk(env, e);
+
+   /* sanity checks ... */
+   vassert(hregClass(r) == HRcInt64);
+   vassert(hregIsVirtual(r));
+
+   return r;
+}
+
+/* DO NOT CALL THIS DIRECTLY ! */
+static HReg iselIntExpr_R_wrk ( ISelEnv* env, IRExpr* e )
+{
+   IRType ty = typeOfIRExpr(env->type_env, e);
+   vassert(e);
+   vassert(ty == Ity_I8 || ty == Ity_I16 || ty == Ity_I32 || ty == Ity_I64);
+
+   switch (e->tag) {
+      /* --------- TEMP --------- */
+      case Iex_RdTmp:
+         return lookupIRTemp(env, e->Iex.RdTmp.tmp);
+
+      /* --------- LOAD --------- */
+      case Iex_Load: {
+         if (e->Iex.Load.end != Iend_LE)
+            goto irreducible;
+
+         LOONGARCH64AMode* am = iselIntExpr_AMode(env, e->Iex.Load.addr, ty);
+         HReg             dst = newVRegI(env);
+         LOONGARCH64LoadOp op;
+         switch (ty) {
+            case Ity_I8:
+               op = (am->tag == LAam_RI) ? LAload_LD_BU : LAload_LDX_BU;
+               break;
+            case Ity_I16:
+               op = (am->tag == LAam_RI) ? LAload_LD_HU : LAload_LDX_HU;
+               break;
+            case Ity_I32:
+               op = (am->tag == LAam_RI) ? LAload_LD_WU : LAload_LDX_WU;
+               break;
+            case Ity_I64:
+               op = (am->tag == LAam_RI) ? LAload_LD_D : LAload_LDX_D;
+               break;
+            default:
+               goto irreducible;
+         }
+         addInstr(env, LOONGARCH64Instr_Load(op, am, dst));
+         return dst;
+      }
+
+      /* --------- BINARY OP --------- */
+      case Iex_Binop: {
+         switch (e->Iex.Binop.op) {
+            case Iop_8HLto16: {
+               HReg          dst  = newVRegI(env);
+               HReg          tHi  = newVRegI(env);
+               HReg          tLow = newVRegI(env);
+               HReg          sHi  = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               HReg          sLow = iselIntExpr_R(env, e->Iex.Binop.arg2);
+               LOONGARCH64RI* ui5 = LOONGARCH64RI_I(8, 5, False);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_W, ui5, sHi, tHi));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_W, ui5, sLow, tLow));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SRLI_W, ui5, tLow, tLow));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_OR, LOONGARCH64RI_R(tHi), tLow, dst));
+               return dst;
+            }
+            case Iop_16HLto32: {
+               HReg          dst  = newVRegI(env);
+               HReg          tHi  = newVRegI(env);
+               HReg          tLow = newVRegI(env);
+               HReg          sHi  = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               HReg          sLow = iselIntExpr_R(env, e->Iex.Binop.arg2);
+               LOONGARCH64RI* ui5 = LOONGARCH64RI_I(16, 5, False);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_W, ui5, sHi, tHi));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_W, ui5, sLow, tLow));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SRLI_W, ui5, tLow, tLow));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_OR, LOONGARCH64RI_R(tHi), tLow, dst));
+               return dst;
+            }
+            case Iop_32HLto64: {
+               HReg          dst = newVRegI(env);
+               HReg           hi = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               LOONGARCH64RI* lo = iselIntExpr_RI(env, e->Iex.Binop.arg2, 0, False);
+               LOONGARCH64RI* ri = LOONGARCH64RI_I(32, 6, False);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_D, ri, hi, dst));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_OR, lo, dst, dst));
+               return dst;
+            }
+            case Iop_Add32: {
+               HReg            dst = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 12, True);
+               LOONGARCH64BinOp op = (src2->tag == LAri_Reg) ? LAbin_ADD_W : LAbin_ADDI_W;
+               addInstr(env, LOONGARCH64Instr_Binary(op, src2, src1, dst));
+               return dst;
+            }
+            case Iop_Add64: {
+               HReg            dst = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 12, True);
+               LOONGARCH64BinOp op = (src2->tag == LAri_Reg) ? LAbin_ADD_D : LAbin_ADDI_D;
+               addInstr(env, LOONGARCH64Instr_Binary(op, src2, src1, dst));
+               return dst;
+            }
+            case Iop_And8: {
+               HReg            dst = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 12, False);
+               LOONGARCH64BinOp op = (src2->tag == LAri_Reg) ? LAbin_AND : LAbin_ANDI;
+               addInstr(env, LOONGARCH64Instr_Binary(op, src2, src1, dst));
+               return dst;
+            }
+            case Iop_And32: {
+               HReg            dst = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 12, False);
+               LOONGARCH64BinOp op = (src2->tag == LAri_Reg) ? LAbin_AND : LAbin_ANDI;
+               addInstr(env, LOONGARCH64Instr_Binary(op, src2, src1, dst));
+               return dst;
+            }
+            case Iop_And64: {
+               HReg            dst = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 12, False);
+               LOONGARCH64BinOp op = (src2->tag == LAri_Reg) ? LAbin_AND : LAbin_ANDI;
+               addInstr(env, LOONGARCH64Instr_Binary(op, src2, src1, dst));
+               return dst;
+            }
+            case Iop_DivModS32to32: {
+               HReg            dst = newVRegI(env);
+               HReg            tmp = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               HReg           src2 = iselIntExpr_R(env, e->Iex.Binop.arg2);
+               LOONGARCH64RI*  ri1 = LOONGARCH64RI_I(0, 5, False);
+               LOONGARCH64RI*  ri2 = LOONGARCH64RI_R(src2);
+               LOONGARCH64RI*  ri3 = LOONGARCH64RI_I(32, 6, False);
+               LOONGARCH64RI*  ri4 = LOONGARCH64RI_R(tmp);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_W, ri1, src1, src1));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_W, ri1, src2, src2));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_DIV_W, ri2, src1, dst));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_MOD_W, ri2, src1, tmp));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_D, ri3, tmp, tmp));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_D, ri3, dst, dst));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SRLI_D, ri3, dst, dst));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_OR, ri4, dst, dst));
+               return dst;
+            }
+            case Iop_DivModU32to32: {
+               HReg            dst = newVRegI(env);
+               HReg            tmp = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               HReg           src2 = iselIntExpr_R(env, e->Iex.Binop.arg2);
+               LOONGARCH64RI*  ri1 = LOONGARCH64RI_I(0, 5, False);
+               LOONGARCH64RI*  ri2 = LOONGARCH64RI_R(src2);
+               LOONGARCH64RI*  ri3 = LOONGARCH64RI_I(32, 6, False);
+               LOONGARCH64RI*  ri4 = LOONGARCH64RI_R(tmp);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_W, ri1, src1, src1));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_W, ri1, src2, src2));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_DIV_WU, ri2, src1, dst));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_MOD_WU, ri2, src1, tmp));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_D, ri3, tmp, tmp));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_D, ri3, dst, dst));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SRLI_D, ri3, dst, dst));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_OR, ri4, dst, dst));
+               return dst;
+            }
+            case Iop_DivS32: {
+               HReg            dst = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               HReg           src2 = iselIntExpr_R(env, e->Iex.Binop.arg2);
+               LOONGARCH64RI*  ri1 = LOONGARCH64RI_I(0, 5, False);
+               LOONGARCH64RI*  ri2 = LOONGARCH64RI_R(src2);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_W, ri1, src1, src1));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_W, ri1, src2, src2));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_DIV_W, ri2, src1, dst));
+               return dst;
+            }
+            case Iop_DivS64: {
+               HReg            dst = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 0, False);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_DIV_D, src2, src1, dst));
+               return dst;
+            }
+            case Iop_DivU32: {
+               HReg            dst = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               HReg           src2 = iselIntExpr_R(env, e->Iex.Binop.arg2);
+               LOONGARCH64RI*  ri1 = LOONGARCH64RI_I(0, 5, False);
+               LOONGARCH64RI*  ri2 = LOONGARCH64RI_R(src2);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_W, ri1, src1, src1));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_W, ri1, src2, src2));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_DIV_WU, ri2, src1, dst));
+               return dst;
+            }
+            case Iop_DivU64: {
+               HReg            dst = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 0, False);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_DIV_DU, src2, src1, dst));
+               return dst;
+            }
+            case Iop_CmpF32: {
+               HReg src1 = iselFltExpr(env, e->Iex.Binop.arg1);
+               HReg src2 = iselFltExpr(env, e->Iex.Binop.arg2);
+               return convert_cond_to_IR(env, src2, src1, False);
+            }
+            case Iop_CmpF64: {
+               HReg src1 = iselFltExpr(env, e->Iex.Binop.arg1);
+               HReg src2 = iselFltExpr(env, e->Iex.Binop.arg2);
+               return convert_cond_to_IR(env, src2, src1, True);
+            }
+            case Iop_F32toI32S: {
+               HReg tmp = newVRegF(env);
+               HReg dst = newVRegI(env);
+               HReg src = iselFltExpr(env, e->Iex.Binop.arg2);
+               set_rounding_mode(env, e->Iex.Binop.arg1);
+               addInstr(env, LOONGARCH64Instr_FpUnary(LAfpun_FTINT_W_S, src, tmp));
+               set_rounding_mode_default(env);
+               addInstr(env, LOONGARCH64Instr_FpMove(LAfpmove_MOVFR2GR_S, tmp, dst));
+               return dst;
+            }
+            case Iop_F32toI64S: {
+               HReg tmp = newVRegF(env);
+               HReg dst = newVRegI(env);
+               HReg src = iselFltExpr(env, e->Iex.Binop.arg2);
+               set_rounding_mode(env, e->Iex.Binop.arg1);
+               addInstr(env, LOONGARCH64Instr_FpUnary(LAfpun_FTINT_L_S, src, tmp));
+               set_rounding_mode_default(env);
+               addInstr(env, LOONGARCH64Instr_FpMove(LAfpmove_MOVFR2GR_D, tmp, dst));
+               return dst;
+            }
+            case Iop_F64toI32S: {
+               HReg tmp = newVRegF(env);
+               HReg dst = newVRegI(env);
+               HReg src = iselFltExpr(env, e->Iex.Binop.arg2);
+               set_rounding_mode(env, e->Iex.Binop.arg1);
+               addInstr(env, LOONGARCH64Instr_FpUnary(LAfpun_FTINT_W_D, src, tmp));
+               set_rounding_mode_default(env);
+               addInstr(env, LOONGARCH64Instr_FpMove(LAfpmove_MOVFR2GR_S, tmp, dst));
+               return dst;
+            }
+            case Iop_F64toI64S: {
+               HReg tmp = newVRegF(env);
+               HReg dst = newVRegI(env);
+               HReg src = iselFltExpr(env, e->Iex.Binop.arg2);
+               set_rounding_mode(env, e->Iex.Binop.arg1);
+               addInstr(env, LOONGARCH64Instr_FpUnary(LAfpun_FTINT_L_D, src, tmp));
+               set_rounding_mode_default(env);
+               addInstr(env, LOONGARCH64Instr_FpMove(LAfpmove_MOVFR2GR_D, tmp, dst));
+               return dst;
+            }
+            case Iop_GetElem16x8:
+            case Iop_GetElem32x4:
+            case Iop_GetElem64x2:
+            case Iop_GetElem8x16: {
+               UChar                size;
+               LOONGARCH64VecBinOp  pickOp, veplOp;
+               switch (e->Iex.Binop.op) {
+                  case Iop_GetElem8x16:
+                     size = 4;
+                     pickOp = LAvecbin_VPICKVE2GR_BU;
+                     veplOp = LAvecbin_VREPLVE_B;
+                     break;
+                  case Iop_GetElem16x8:
+                     size = 3;
+                     pickOp = LAvecbin_VPICKVE2GR_HU;
+                     veplOp = LAvecbin_VREPLVE_H;
+                     break;
+                  case Iop_GetElem32x4:
+                     size = 2;
+                     pickOp = LAvecbin_VPICKVE2GR_WU;
+                     veplOp = LAvecbin_VREPLVE_W;
+                     break;
+                  case Iop_GetElem64x2:
+                     size = 1;
+                     pickOp = LAvecbin_VPICKVE2GR_DU;
+                     veplOp = LAvecbin_VREPLVE_D;
+                     break;
+                  default:
+                     vassert(0);
+                     break;
+               }
+               HReg           dst  = newVRegI(env);
+               HReg           src1 = iselV128Expr(env, e->Iex.Binop.arg1);
+               LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, size, False);
+               if (src2->tag == LAri_Imm) {
+                  addInstr(env, LOONGARCH64Instr_VecBinary(pickOp, src2, src1, dst));
+               } else {
+                  HReg v_tmp = newVRegV(env);
+                  addInstr(env, LOONGARCH64Instr_VecBinary(veplOp, src2, src1, v_tmp));
+                  addInstr(env, LOONGARCH64Instr_VecBinary(pickOp, LOONGARCH64RI_I(0, size, False), v_tmp, dst));
+               }
+
+               return dst;
+            }
+            case Iop_Max32U: {
+               HReg          cond = newVRegI(env);
+               HReg           dst = newVRegI(env);
+               HReg          src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               HReg          src2 = iselIntExpr_R(env, e->Iex.Binop.arg2);
+               addInstr(env, LOONGARCH64Instr_Cmp(LAcc_LTU, src2, src1, cond));
+               addInstr(env, LOONGARCH64Instr_CMove(cond, src1, src2, dst, True));
+               return dst;
+            }
+            case Iop_MullS32: {
+               HReg            dst = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 0, False);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_MULW_D_W, src2, src1, dst));
+               return dst;
+            }
+            case Iop_MullU32: {
+               HReg            dst = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 0, False);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_MULW_D_WU, src2, src1, dst));
+               return dst;
+            }
+            case Iop_Or32: {
+               HReg            dst = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 12, False);
+               LOONGARCH64BinOp op = (src2->tag == LAri_Reg) ? LAbin_OR : LAbin_ORI;
+               addInstr(env, LOONGARCH64Instr_Binary(op, src2, src1, dst));
+               return dst;
+            }
+            case Iop_Or64: {
+               HReg            dst = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 12, False);
+               LOONGARCH64BinOp op = (src2->tag == LAri_Reg) ? LAbin_OR : LAbin_ORI;
+               addInstr(env, LOONGARCH64Instr_Binary(op, src2, src1, dst));
+               return dst;
+            }
+            case Iop_Sar32: {
+               HReg            dst = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 5, False);
+               LOONGARCH64BinOp op = (src2->tag == LAri_Reg) ? LAbin_SRA_W : LAbin_SRAI_W;
+               addInstr(env, LOONGARCH64Instr_Binary(op, src2, src1, dst));
+               return dst;
+            }
+            case Iop_Sar64: {
+               HReg            dst = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 6, False);
+               LOONGARCH64BinOp op = (src2->tag == LAri_Reg) ? LAbin_SRA_D : LAbin_SRAI_D;
+               addInstr(env, LOONGARCH64Instr_Binary(op, src2, src1, dst));
+               return dst;
+            }
+            case Iop_Shl32: {
+               HReg            dst = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 5, False);
+               LOONGARCH64BinOp op = (src2->tag == LAri_Reg) ? LAbin_SLL_W : LAbin_SLLI_W;
+               addInstr(env, LOONGARCH64Instr_Binary(op, src2, src1, dst));
+               return dst;
+            }
+            case Iop_Shl64: {
+               HReg            dst = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 6, False);
+               LOONGARCH64BinOp op = (src2->tag == LAri_Reg) ? LAbin_SLL_D : LAbin_SLLI_D;
+               addInstr(env, LOONGARCH64Instr_Binary(op, src2, src1, dst));
+               return dst;
+            }
+            case Iop_Shr32: {
+               HReg            dst = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 5, False);
+               LOONGARCH64BinOp op = (src2->tag == LAri_Reg) ? LAbin_SRL_W : LAbin_SRLI_W;
+               addInstr(env, LOONGARCH64Instr_Binary(op, src2, src1, dst));
+               return dst;
+            }
+            case Iop_Shr64: {
+               HReg            dst = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 6, False);
+               LOONGARCH64BinOp op = (src2->tag == LAri_Reg) ? LAbin_SRL_D : LAbin_SRLI_D;
+               addInstr(env, LOONGARCH64Instr_Binary(op, src2, src1, dst));
+               return dst;
+            }
+            case Iop_Sub32: {
+               HReg            dst = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 0, False);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SUB_W, src2, src1, dst));
+               return dst;
+            }
+            case Iop_Sub64: {
+               HReg            dst = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 0, False);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SUB_D, src2, src1, dst));
+               return dst;
+            }
+            case Iop_Xor32: {
+               HReg            dst = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 12, False);
+               LOONGARCH64BinOp op = (src2->tag == LAri_Reg) ? LAbin_XOR : LAbin_XORI;
+               addInstr(env, LOONGARCH64Instr_Binary(op, src2, src1, dst));
+               return dst;
+            }
+            case Iop_Xor64: {
+               HReg            dst = newVRegI(env);
+               HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 12, False);
+               LOONGARCH64BinOp op = (src2->tag == LAri_Reg) ? LAbin_XOR : LAbin_XORI;
+               addInstr(env, LOONGARCH64Instr_Binary(op, src2, src1, dst));
+               return dst;
+            }
+            case Iop_64HLtoV128: {
+               HReg dst  = newVRegV(env);
+               HReg sHi  = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               HReg sLow = iselIntExpr_R(env, e->Iex.Binop.arg2);
+               addInstr(env, LOONGARCH64Instr_VecBinary(LAvecbin_VINSGR2VR_D,
+                                                        LOONGARCH64RI_I(0, 1, False), sLow, dst));
+               addInstr(env, LOONGARCH64Instr_VecBinary(LAvecbin_VINSGR2VR_D,
+                                                        LOONGARCH64RI_I(1, 1, False), sHi, dst));
+               return dst;
+            }
+            default:
+               goto irreducible;
+         }
+      }
+
+      /* --------- UNARY OP --------- */
+      case Iex_Unop: {
+         switch (e->Iex.Unop.op) {
+            case Iop_128HIto64: {
+               HReg hi, lo;
+               iselInt128Expr(&hi, &lo, env, e->Iex.Unop.arg);
+               return hi;
+            }
+            case Iop_128to64: {
+               HReg hi, lo;
+               iselInt128Expr(&hi, &lo, env, e->Iex.Unop.arg);
+               return lo;
+            }
+            case Iop_16Sto64: {
+               HReg dst = newVRegI(env);
+               HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               addInstr(env, LOONGARCH64Instr_Unary(LAun_EXT_W_H, src, dst));
+               return dst;
+            }
+            case Iop_16Uto32: {
+               HReg          dst = newVRegI(env);
+               HReg          src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               LOONGARCH64RI* ri = LOONGARCH64RI_I(48, 6, False);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_D, ri, src, dst));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SRLI_D, ri, dst, dst));
+               return dst;
+            }
+            case Iop_16Uto64: {
+               HReg          dst = newVRegI(env);
+               HReg          src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               LOONGARCH64RI* ri = LOONGARCH64RI_I(48, 6, False);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_D, ri, src, dst));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SRLI_D, ri, dst, dst));
+               return dst;
+            }
+            case Iop_1Sto32: {
+               HReg           dst = newVRegI(env);
+               HReg           src = iselCondCode_R(env, e->Iex.Unop.arg);
+               LOONGARCH64RI* ri = LOONGARCH64RI_I(63, 6, False);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_D, ri, src, dst));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SRAI_D, ri, dst, dst));
+               return dst;
+            }
+            case Iop_1Sto64: {
+               HReg          dst = newVRegI(env);
+               HReg          src = iselCondCode_R(env, e->Iex.Unop.arg);
+               LOONGARCH64RI* ri = LOONGARCH64RI_I(63, 6, False);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_D, ri, src, dst));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SRAI_D, ri, dst, dst));
+               return dst;
+            }
+            case Iop_1Uto64: {
+               HReg          dst = newVRegI(env);
+               HReg          src = iselCondCode_R(env, e->Iex.Unop.arg);
+               LOONGARCH64RI* ri = LOONGARCH64RI_I(0x1, 12, False);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_ANDI, ri, src, dst));
+               return dst;
+            }
+            case Iop_1Uto8: {
+               HReg          dst = newVRegI(env);
+               HReg          src = iselCondCode_R(env, e->Iex.Unop.arg);
+               LOONGARCH64RI* ri = LOONGARCH64RI_I(0x1, 12, False);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_ANDI, ri, src, dst));
+               return dst;
+            }
+            case Iop_32Sto64: {
+               HReg          dst = newVRegI(env);
+               HReg          src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               LOONGARCH64RI* ri = LOONGARCH64RI_I(0, 5, False);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_W, ri, src, dst));
+               return dst;
+            }
+            case Iop_32Uto64: {
+               HReg          dst = newVRegI(env);
+               HReg          src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               LOONGARCH64RI* ri = LOONGARCH64RI_I(32, 6, False);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_D, ri, src, dst));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SRLI_D, ri, dst, dst));
+               return dst;
+            }
+            case Iop_32to8: {
+               HReg dst = newVRegI(env);
+               HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               LOONGARCH64RI* ri = LOONGARCH64RI_I(0xff, 12, False);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_ANDI, ri, src, dst));
+               return dst;
+            }
+            case Iop_64HIto32: {
+               HReg          dst = newVRegI(env);
+               HReg          src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               LOONGARCH64RI* ri = LOONGARCH64RI_I(32, 6, False);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SRLI_D, ri, src, dst));
+               return dst;
+            }
+            case Iop_64to32: {
+               HReg          dst = newVRegI(env);
+               HReg          src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               LOONGARCH64RI* ri = LOONGARCH64RI_I(32, 6, False);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_D, ri, src, dst));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SRLI_D, ri, dst, dst));
+               return dst;
+            }
+            case Iop_64to8: {
+               HReg          dst = newVRegI(env);
+               HReg          src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               LOONGARCH64RI* ri = LOONGARCH64RI_I(0xff, 12, False);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_ANDI, ri, src, dst));
+               return dst;
+            }
+            case Iop_8Sto64: {
+               HReg dst = newVRegI(env);
+               HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               addInstr(env, LOONGARCH64Instr_Unary(LAun_EXT_W_B, src, dst));
+               return dst;
+            }
+            case Iop_8Uto32: {
+               HReg          dst = newVRegI(env);
+               HReg          src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               LOONGARCH64RI* ri = LOONGARCH64RI_I(0xff, 12, False);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_ANDI, ri, src, dst));
+               return dst;
+            }
+            case Iop_8Uto64: {
+               HReg dst = newVRegI(env);
+               HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               LOONGARCH64RI* ri = LOONGARCH64RI_I(0xff, 12, False);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_ANDI, ri, src, dst));
+               return dst;
+            }
+            case Iop_CmpwNEZ32: {
+               HReg          dst = newVRegI(env);
+               HReg          src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               LOONGARCH64RI* ri = LOONGARCH64RI_I(63, 6, False);
+               addInstr(env, LOONGARCH64Instr_Cmp(LAcc_NE, hregZERO(), src, dst));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_D, ri, dst, dst));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SRAI_D, ri, dst, dst));
+               return dst;
+            }
+            case Iop_CmpwNEZ64: {
+               HReg          dst = newVRegI(env);
+               HReg          src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               LOONGARCH64RI* ri = LOONGARCH64RI_I(63, 6, False);
+               addInstr(env, LOONGARCH64Instr_Cmp(LAcc_NE, hregZERO(), src, dst));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_D, ri, dst, dst));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SRAI_D, ri, dst, dst));
+               return dst;
+            }
+            case Iop_Clz32: {
+               HReg dst = newVRegI(env);
+               HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               addInstr(env, LOONGARCH64Instr_Unary(LAun_CLZ_W, src, dst));
+               return dst;
+            }
+            case Iop_Clz64: {
+               HReg dst = newVRegI(env);
+               HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               addInstr(env, LOONGARCH64Instr_Unary(LAun_CLZ_D, src, dst));
+               return dst;
+            }
+            case Iop_Ctz32: {
+               HReg dst = newVRegI(env);
+               HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               addInstr(env, LOONGARCH64Instr_Unary(LAun_CTZ_W, src, dst));
+               return dst;
+            }
+            case Iop_Ctz64: {
+               HReg dst = newVRegI(env);
+               HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               addInstr(env, LOONGARCH64Instr_Unary(LAun_CTZ_D, src, dst));
+               return dst;
+            }
+            case Iop_Left16: {
+               HReg           tmp = newVRegI(env);
+               HReg           dst = newVRegI(env);
+               HReg           src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               LOONGARCH64RI* ri = LOONGARCH64RI_R(src);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SUB_D, ri, hregZERO(), tmp));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_OR, ri, tmp, dst));
+               return dst;
+            }
+            case Iop_Left32: {
+               HReg           tmp = newVRegI(env);
+               HReg           dst = newVRegI(env);
+               HReg           src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               LOONGARCH64RI* ri = LOONGARCH64RI_R(src);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SUB_D, ri, hregZERO(), tmp));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_OR, ri, tmp, dst));
+               return dst;
+            }
+            case Iop_Left64: {
+               HReg           tmp = newVRegI(env);
+               HReg           dst = newVRegI(env);
+               HReg           src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               LOONGARCH64RI* ri = LOONGARCH64RI_R(src);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SUB_D, ri, hregZERO(), tmp));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_OR, ri, tmp, dst));
+               return dst;
+            }
+            case Iop_Left8: {
+               HReg           tmp = newVRegI(env);
+               HReg           dst = newVRegI(env);
+               HReg           src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               LOONGARCH64RI* ri = LOONGARCH64RI_R(src);
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_SUB_D, ri, hregZERO(), tmp));
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_OR, ri, tmp, dst));
+               return dst;
+            }
+            case Iop_ReinterpF32asI32: {
+               HReg          dst = newVRegI(env);
+               HReg          src = iselFltExpr(env, e->Iex.Unop.arg);
+               addInstr(env, LOONGARCH64Instr_FpMove(LAfpmove_MOVFR2GR_S, src, dst));
+               return dst;
+            }
+            case Iop_ReinterpF64asI64: {
+               HReg          dst = newVRegI(env);
+               HReg          src = iselFltExpr(env, e->Iex.Unop.arg);
+               addInstr(env, LOONGARCH64Instr_FpMove(LAfpmove_MOVFR2GR_D, src, dst));
+               return dst;
+            }
+            case Iop_Not32: {
+               HReg          dst = newVRegI(env);
+               HReg          src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               LOONGARCH64RI* ri = LOONGARCH64RI_R(hregZERO());
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_NOR, ri, src, dst));
+               return dst;
+            }
+            case Iop_Not64: {
+               HReg          dst = newVRegI(env);
+               HReg          src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               LOONGARCH64RI* ri = LOONGARCH64RI_R(hregZERO());
+               addInstr(env, LOONGARCH64Instr_Binary(LAbin_NOR, ri, src, dst));
+               return dst;
+            }
+            case Iop_V128to32: {
+               HReg           dst = newVRegI(env);
+               HReg           src = iselV128Expr(env, e->Iex.Unop.arg);
+               addInstr(env, LOONGARCH64Instr_VecBinary(LAvecbin_VPICKVE2GR_W,
+                                                        LOONGARCH64RI_I(0, 2, False), src, dst));
+               return dst;
+            }
+            case Iop_V128to64: {
+               HReg           dst = newVRegI(env);
+               HReg           src = iselV128Expr(env, e->Iex.Unop.arg);
+               addInstr(env, LOONGARCH64Instr_VecBinary(LAvecbin_VPICKVE2GR_D,
+                                                        LOONGARCH64RI_I(0, 1, False), src, dst));
+               return dst;
+            }
+            case Iop_V128HIto64: {
+               HReg           dst = newVRegI(env);
+               HReg           src = iselV128Expr(env, e->Iex.Unop.arg);
+               addInstr(env, LOONGARCH64Instr_VecBinary(LAvecbin_VPICKVE2GR_D,
+                                                        LOONGARCH64RI_I(1, 1, False), src, dst));
+               return dst;
+            }
+            case Iop_V256to64_0: case Iop_V256to64_1:
+            case Iop_V256to64_2: case Iop_V256to64_3: {
+               UShort id;
+               HReg vHi, vLo, vec;
+               iselV256Expr(&vHi, &vLo, env, e->Iex.Unop.arg);
+               switch (e->Iex.Unop.op) {
+                  case Iop_V256to64_0: vec = vLo; id = 0; break;
+                  case Iop_V256to64_1: vec = vLo; id = 1; break;
+                  case Iop_V256to64_2: vec = vHi; id = 0; break;
+                  case Iop_V256to64_3: vec = vHi; id = 1; break;
+                  default: vassert(0);                    break;
+               }
+               HReg dst = newVRegI(env);
+               addInstr(env, LOONGARCH64Instr_VecBinary(LAvecbin_VPICKVE2GR_D,
+                                                        LOONGARCH64RI_I(id, 1, False), vec, dst));
+               return dst;
+            }
+            default:
+               goto irreducible;
+         }
+      }
+
+      /* --------- GET --------- */
+      case Iex_Get: {
+         Bool  ri = e->Iex.Get.offset < 1024;
+         HReg dst = newVRegI(env);
+         HReg tmp;
+         LOONGARCH64AMode* am;
+         LOONGARCH64LoadOp op;
+         switch (ty) {
+            case Ity_I8:
+               op = ri ? LAload_LD_BU : LAload_LDX_BU;
+               break;
+            case Ity_I16:
+               op = ri ? LAload_LD_HU : LAload_LDX_HU;
+               break;
+            case Ity_I32:
+               op = ri ? LAload_LD_WU : LAload_LDX_WU;
+               break;
+            case Ity_I64:
+               op = ri ? LAload_LD_D : LAload_LDX_D;
+               break;
+            default:
+               goto irreducible;
+         }
+         if (ri) {
+            am = LOONGARCH64AMode_RI(hregGSP(), e->Iex.Get.offset);
+         } else {
+            tmp = newVRegI(env);
+            addInstr(env, LOONGARCH64Instr_LI(e->Iex.Get.offset, tmp));
+            am = LOONGARCH64AMode_RR(hregGSP(), tmp);
+         }
+         addInstr(env, LOONGARCH64Instr_Load(op, am, dst));
+         return dst;
+      }
+
+      /* --------- CCALL --------- */
+      case Iex_CCall: {
+         HReg    dst = newVRegI(env);
+         vassert(ty == e->Iex.CCall.retty);
+
+         /* be very restrictive for now.  Only 64-bit ints allowed for
+            args, and 64 bits for return type.  Don't forget to change
+            the RetLoc if more types are allowed in future. */
+         if (e->Iex.CCall.retty != Ity_I64)
+            goto irreducible;
+
+         /* Marshal args, do the call, clear stack. */
+         UInt   addToSp = 0;
+         RetLoc rloc    = mk_RetLoc_INVALID();
+         Bool   ok      = doHelperCall(&addToSp, &rloc, env, NULL,
+                                       e->Iex.CCall.cee, e->Iex.CCall.retty,
+                                       e->Iex.CCall.args);
+
+         if (ok) {
+            vassert(is_sane_RetLoc(rloc));
+            vassert(rloc.pri == RLPri_Int);
+            vassert(addToSp == 0);
+            addInstr(env, LOONGARCH64Instr_Move(dst, hregLOONGARCH64_R4()));
+            return dst;
+         }
+         goto irreducible;
+      }
+
+      /* --------- LITERAL --------- */
+      /* 64-bit literals */
+      case Iex_Const: {
+         ULong imm = 0;
+         HReg  dst = newVRegI(env);
+         switch (e->Iex.Const.con->tag) {
+            case Ico_U64:
+               imm = e->Iex.Const.con->Ico.U64;
+               break;
+            case Ico_U32:
+               imm = e->Iex.Const.con->Ico.U32;
+               break;
+            case Ico_U16:
+               imm = e->Iex.Const.con->Ico.U16;
+               break;
+            case Ico_U8:
+               imm = e->Iex.Const.con->Ico.U8;
+               break;
+            default:
+               ppIRExpr(e);
+               vpanic("iselIntExpr_R.Iex_Const(loongarch64)");
+         }
+         addInstr(env, LOONGARCH64Instr_LI(imm, dst));
+         return dst;
+      }
+
+      case Iex_ITE: {
+         HReg   r0 = iselIntExpr_R(env, e->Iex.ITE.iffalse);
+         HReg   r1 = iselIntExpr_R(env, e->Iex.ITE.iftrue);
+         HReg cond = iselCondCode_R(env, e->Iex.ITE.cond);
+         HReg  dst = newVRegI(env);
+         addInstr(env, LOONGARCH64Instr_CMove(cond, r0, r1, dst, True));
+         return dst;
+      }
+
+      default:
+         break;
+   }
+
+   /* We get here if no pattern matched. */
+irreducible:
+   ppIRExpr(e);
+   vpanic("iselIntExpr_R(loongarch64): cannot reduce tree");
+}
+
+/* ------------------- CondCode ------------------- */
+
+/* Generate code to evaluated a bit-typed expression, returning the
+   condition code which would correspond when the expression would
+   notionally have returned 1. */
+
+static HReg iselCondCode_R ( ISelEnv* env, IRExpr* e )
+{
+   HReg r = iselCondCode_R_wrk(env, e);
+
+   /* sanity checks ... */
+   vassert(hregClass(r) == HRcInt64);
+   vassert(hregIsVirtual(r));
+
+   return r;
+}
+
+/* DO NOT CALL THIS DIRECTLY ! */
+static HReg iselCondCode_R_wrk ( ISelEnv* env, IRExpr* e )
+{
+   vassert(e);
+   vassert(typeOfIRExpr(env->type_env, e) == Ity_I1);
+
+   HReg dst = newVRegI(env);
+
+   /* var */
+   if (e->tag == Iex_RdTmp) {
+      HReg tmp = newVRegI(env);
+      dst = lookupIRTemp(env, e->Iex.RdTmp.tmp);
+      addInstr(env, LOONGARCH64Instr_LI(1, tmp));
+      addInstr(env, LOONGARCH64Instr_Cmp(LAcc_EQ, dst, tmp, dst));
+      return dst;
+   }
+
+   /* const */
+   if (e->tag == Iex_Const && e->Iex.Const.con->tag == Ico_U1) {
+      UInt imm = e->Iex.Const.con->Ico.U1;
+      addInstr(env, LOONGARCH64Instr_LI(imm, dst));
+      return dst;
+   }
+
+   if (e->tag == Iex_Unop) {
+      if (e->Iex.Unop.op == Iop_Not1) {
+         HReg          src = iselCondCode_R(env, e->Iex.Unop.arg);
+         LOONGARCH64RI* ri = LOONGARCH64RI_R(hregZERO());
+         addInstr(env, LOONGARCH64Instr_Binary(LAbin_NOR, ri, src, dst));
+         return dst;
+      }
+
+      LOONGARCH64CondCode cc;
+      switch (e->Iex.Unop.op) {
+         case Iop_CmpNEZ16:
+            cc = LAcc_NE;
+            break;
+         case Iop_CmpNEZ32:
+            cc = LAcc_NE;
+            break;
+         case Iop_CmpNEZ64:
+            cc = LAcc_NE;
+            break;
+         case Iop_CmpNEZ8:
+            cc = LAcc_NE;
+            break;
+         default:
+            goto irreducible;
+      }
+      HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+      addInstr(env, LOONGARCH64Instr_Cmp(cc, hregZERO(), src, dst));
+      return dst;
+   }
+
+   if (e->tag == Iex_Binop) {
+      if (e->Iex.Binop.op == Iop_And1) {
+         HReg           src1 = iselCondCode_R(env, e->Iex.Binop.arg1);
+         HReg           src2 = iselCondCode_R(env, e->Iex.Binop.arg2);
+         LOONGARCH64RI*   ri = LOONGARCH64RI_R(src2);
+         addInstr(env, LOONGARCH64Instr_Binary(LAbin_AND, ri, src1, dst));
+         return dst;
+      } else if (e->Iex.Binop.op == Iop_Or1) {
+         HReg           src1 = iselCondCode_R(env, e->Iex.Binop.arg1);
+         HReg           src2 = iselCondCode_R(env, e->Iex.Binop.arg2);
+         LOONGARCH64RI*   ri = LOONGARCH64RI_R(src2);
+         addInstr(env, LOONGARCH64Instr_Binary(LAbin_OR, ri, src1, dst));
+         return dst;
+      }
+
+      Bool extend  = False;
+      Bool reverse = False;
+      LOONGARCH64CondCode cc;
+      switch (e->Iex.Binop.op) {
+         case Iop_CasCmpEQ32:
+            cc = LAcc_EQ;
+            break;
+         case Iop_CasCmpEQ64:
+            cc = LAcc_EQ;
+            break;
+         case Iop_CasCmpNE32:
+            cc = LAcc_NE;
+            break;
+         case Iop_CasCmpNE64:
+            cc = LAcc_NE;
+            break;
+         case Iop_CmpEQ32:
+            cc = LAcc_EQ;
+            break;
+         case Iop_CmpEQ64:
+            cc = LAcc_EQ;
+            break;
+         case Iop_CmpLE32S:
+            cc = LAcc_GE;
+            reverse = True;
+            break;
+         case Iop_CmpLE32U:
+            cc = LAcc_GEU;
+            reverse = True;
+            break;
+         case Iop_CmpLE64S:
+            cc = LAcc_GE;
+            reverse = True;
+            break;
+         case Iop_CmpLE64U:
+            cc = LAcc_GEU;
+            reverse = True;
+            break;
+         case Iop_CmpLT32S:
+            cc = LAcc_LT;
+            extend = True;
+            break;
+         case Iop_CmpLT32U:
+            cc = LAcc_LTU;
+            extend = True;
+            break;
+         case Iop_CmpLT64S:
+            cc = LAcc_LT;
+            break;
+         case Iop_CmpLT64U:
+            cc = LAcc_LTU;
+            break;
+         case Iop_CmpNE32:
+            cc = LAcc_NE;
+            break;
+         case Iop_CmpNE64:
+            cc = LAcc_NE;
+            break;
+         default:
+            goto irreducible;
+      }
+      HReg src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+      HReg src2 = iselIntExpr_R(env, e->Iex.Binop.arg2);
+      if (extend) {
+         /* Sign-extend */
+         LOONGARCH64RI* ri = LOONGARCH64RI_I(0, 5, False);
+         addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_W, ri, src1, src1));
+         addInstr(env, LOONGARCH64Instr_Binary(LAbin_SLLI_W, ri, src2, src2));
+      }
+      if (reverse) {
+         addInstr(env, LOONGARCH64Instr_Cmp(cc, src1, src2, dst));
+      } else {
+         addInstr(env, LOONGARCH64Instr_Cmp(cc, src2, src1, dst));
+      }
+      return dst;
+   }
+
+   /* We get here if no pattern matched. */
+irreducible:
+   ppIRExpr(e);
+   vpanic("iselCondCode(loongarch64): cannot reduce tree");
+}
+
+
+/*---------------------------------------------------------*/
+/*--- ISEL: Integer expressions (128 bit)               ---*/
+/*---------------------------------------------------------*/
+
+/* Compute a 128-bit value into a register pair, which is returned as
+   the first two parameters.  As with iselIntExpr_R, these may be
+   either real or virtual regs; in any case they must not be changed
+   by subsequent code emitted by the caller.  */
+
+static void iselInt128Expr (HReg* hi, HReg* lo, ISelEnv* env, IRExpr* e)
+{
+   iselInt128Expr_wrk(hi, lo, env, e);
+
+   /* sanity checks ... */
+   vassert(hregClass(*hi) == HRcInt64);
+   vassert(hregIsVirtual(*hi));
+   vassert(hregClass(*lo) == HRcInt64);
+   vassert(hregIsVirtual(*lo));
+}
+
+/* DO NOT CALL THIS DIRECTLY ! */
+static void iselInt128Expr_wrk (HReg* hi, HReg* lo, ISelEnv* env, IRExpr* e)
+{
+   vassert(e);
+   vassert(typeOfIRExpr(env->type_env, e) == Ity_I128);
+
+   /* --------- TEMP --------- */
+   if (e->tag == Iex_RdTmp) {
+      lookupIRTempPair(hi, lo, env, e->Iex.RdTmp.tmp);
+      return;
+   }
+
+   /* --------- BINARY OP --------- */
+   if (e->tag == Iex_Binop) {
+      switch (e->Iex.Binop.op) {
+         case Iop_64HLto128: {
+            *hi = iselIntExpr_R(env, e->Iex.Binop.arg1);
+            *lo = iselIntExpr_R(env, e->Iex.Binop.arg2);
+            return;
+         }
+         case Iop_DivModS64to64: {
+            HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+            LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 0, False);
+            HReg          dstLo = newVRegI(env);
+            HReg          dstHi = newVRegI(env);
+            addInstr(env, LOONGARCH64Instr_Binary(LAbin_DIV_D, src2, src1, dstLo));
+            addInstr(env, LOONGARCH64Instr_Binary(LAbin_MOD_D, src2, src1, dstHi));
+            *hi = dstHi;
+            *lo = dstLo;
+            return;
+         }
+         case Iop_DivModU64to64: {
+            HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+            LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 0, False);
+            HReg          dstLo = newVRegI(env);
+            HReg          dstHi = newVRegI(env);
+            addInstr(env, LOONGARCH64Instr_Binary(LAbin_DIV_DU, src2, src1, dstLo));
+            addInstr(env, LOONGARCH64Instr_Binary(LAbin_MOD_DU, src2, src1, dstHi));
+            *hi = dstHi;
+            *lo = dstLo;
+            return;
+         }
+         case Iop_MullS64: {
+            HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+            LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 0, False);
+            HReg          dstLo = newVRegI(env);
+            HReg          dstHi = newVRegI(env);
+            addInstr(env, LOONGARCH64Instr_Binary(LAbin_MUL_D, src2, src1, dstLo));
+            addInstr(env, LOONGARCH64Instr_Binary(LAbin_MULH_D, src2, src1, dstHi));
+            *hi = dstHi;
+            *lo = dstLo;
+            return;
+         }
+         case Iop_MullU64: {
+            HReg           src1 = iselIntExpr_R(env, e->Iex.Binop.arg1);
+            LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, 0, False);
+            HReg          dstLo = newVRegI(env);
+            HReg          dstHi = newVRegI(env);
+            addInstr(env, LOONGARCH64Instr_Binary(LAbin_MUL_D, src2, src1, dstLo));
+            addInstr(env, LOONGARCH64Instr_Binary(LAbin_MULH_DU, src2, src1, dstHi));
+            *hi = dstHi;
+            *lo = dstLo;
+            return;
+         }
+         default:
+            goto irreducible;
+      }
+   }
+
+   /* We get here if no pattern matched. */
+irreducible:
+   ppIRExpr(e);
+   vpanic("iselInt128Expr(loongarch64): cannot reduce tree");
+}
+
+
+/*---------------------------------------------------------*/
+/*--- ISEL: Floating point expressions (64/32 bit)      ---*/
+/*---------------------------------------------------------*/
+
+/* Compute a floating point value into a register, the identity of
+   which is returned.  As with iselIntExpr_R, the reg may be either
+   real or virtual; in any case it must not be changed by subsequent
+   code emitted by the caller.  */
+
+static HReg iselFltExpr ( ISelEnv* env, IRExpr* e )
+{
+   HReg r = iselFltExpr_wrk(env, e);
+
+   /* sanity checks ... */
+   vassert(hregClass(r) == HRcFlt64);
+   vassert(hregIsVirtual(r));
+
+   return r;
+}
+
+/* DO NOT CALL THIS DIRECTLY */
+static HReg iselFltExpr_wrk ( ISelEnv* env, IRExpr* e )
+{
+   IRType ty = typeOfIRExpr(env->type_env, e);
+   vassert(e);
+   vassert(ty == Ity_F32 || ty == Ity_F64);
+
+   switch (e->tag) {
+      /* --------- TEMP --------- */
+      case Iex_RdTmp:
+         return lookupIRTemp(env, e->Iex.RdTmp.tmp);
+
+      /* --------- LOAD --------- */
+      case Iex_Load: {
+         if (e->Iex.Load.end != Iend_LE)
+            goto irreducible;
+
+         LOONGARCH64AMode* am = iselIntExpr_AMode(env, e->Iex.Load.addr, ty);
+         HReg             dst = newVRegF(env);
+         LOONGARCH64FpLoadOp op;
+         switch (ty) {
+            case Ity_F32:
+               op = (am->tag == LAam_RI) ? LAfpload_FLD_S : LAfpload_FLDX_S;
+               break;
+            case Ity_F64:
+               op = (am->tag == LAam_RI) ? LAfpload_FLD_D : LAfpload_FLDX_D;
+               break;
+            default:
+               goto irreducible;
+         }
+         addInstr(env, LOONGARCH64Instr_FpLoad(op, am, dst));
+         return dst;
+      }
+
+      /* --------- GET --------- */
+      case Iex_Get: {
+         Bool ri  = e->Iex.Get.offset < 1024;
+         HReg dst = newVRegF(env);
+         HReg tmp;
+         LOONGARCH64AMode* am;
+         LOONGARCH64FpLoadOp op;
+         switch (ty) {
+            case Ity_F32:
+               op = ri ? LAfpload_FLD_S : LAfpload_FLDX_S;
+               break;
+            case Ity_F64:
+               op = ri ? LAfpload_FLD_D : LAfpload_FLDX_D;
+               break;
+            default:
+               goto irreducible;
+         }
+         if (ri) {
+            am = LOONGARCH64AMode_RI(hregGSP(), e->Iex.Get.offset);
+         } else {
+            tmp = newVRegI(env);
+            addInstr(env, LOONGARCH64Instr_LI(e->Iex.Get.offset, tmp));
+            am = LOONGARCH64AMode_RR(hregGSP(), tmp);
+         }
+         addInstr(env, LOONGARCH64Instr_FpLoad(op, am, dst));
+         return dst;
+      }
+
+      /* --------- QUATERNARY OP --------- */
+      case Iex_Qop: {
+         switch (e->Iex.Qop.details->op) {
+            case Iop_MAddF32: {
+               HReg  dst = newVRegF(env);
+               HReg src1 = iselFltExpr(env, e->Iex.Qop.details->arg2);
+               HReg src2 = iselFltExpr(env, e->Iex.Qop.details->arg3);
+               HReg src3 = iselFltExpr(env, e->Iex.Qop.details->arg4);
+               set_rounding_mode(env, e->Iex.Qop.details->arg1);
+               addInstr(env, LOONGARCH64Instr_FpTrinary(LAfpbin_FMADD_S, src3, src2, src1, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            case Iop_MAddF64: {
+               HReg  dst = newVRegF(env);
+               HReg src1 = iselFltExpr(env, e->Iex.Qop.details->arg2);
+               HReg src2 = iselFltExpr(env, e->Iex.Qop.details->arg3);
+               HReg src3 = iselFltExpr(env, e->Iex.Qop.details->arg4);
+               set_rounding_mode(env, e->Iex.Qop.details->arg1);
+               addInstr(env, LOONGARCH64Instr_FpTrinary(LAfpbin_FMADD_D, src3, src2, src1, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            case Iop_MSubF32: {
+               HReg  dst = newVRegF(env);
+               HReg src1 = iselFltExpr(env, e->Iex.Qop.details->arg2);
+               HReg src2 = iselFltExpr(env, e->Iex.Qop.details->arg3);
+               HReg src3 = iselFltExpr(env, e->Iex.Qop.details->arg4);
+               set_rounding_mode(env, e->Iex.Qop.details->arg1);
+               addInstr(env, LOONGARCH64Instr_FpTrinary(LAfpbin_FMSUB_S, src3, src2, src1, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            case Iop_MSubF64: {
+               HReg  dst = newVRegF(env);
+               HReg src1 = iselFltExpr(env, e->Iex.Qop.details->arg2);
+               HReg src2 = iselFltExpr(env, e->Iex.Qop.details->arg3);
+               HReg src3 = iselFltExpr(env, e->Iex.Qop.details->arg4);
+               set_rounding_mode(env, e->Iex.Qop.details->arg1);
+               addInstr(env, LOONGARCH64Instr_FpTrinary(LAfpbin_FMSUB_D, src3, src2, src1, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            default:
+               goto irreducible;
+         }
+      }
+
+      /* --------- TERNARY OP --------- */
+      case Iex_Triop: {
+         switch (e->Iex.Triop.details->op) {
+            case Iop_AddF32: {
+               HReg  dst = newVRegF(env);
+               HReg src1 = iselFltExpr(env, e->Iex.Triop.details->arg2);
+               HReg src2 = iselFltExpr(env, e->Iex.Triop.details->arg3);
+               set_rounding_mode(env, e->Iex.Triop.details->arg1);
+               addInstr(env, LOONGARCH64Instr_FpBinary(LAfpbin_FADD_S, src2, src1, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            case Iop_AddF64: {
+               HReg  dst = newVRegF(env);
+               HReg src1 = iselFltExpr(env, e->Iex.Triop.details->arg2);
+               HReg src2 = iselFltExpr(env, e->Iex.Triop.details->arg3);
+               set_rounding_mode(env, e->Iex.Triop.details->arg1);
+               addInstr(env, LOONGARCH64Instr_FpBinary(LAfpbin_FADD_D, src2, src1, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            case Iop_DivF32: {
+               HReg  dst = newVRegF(env);
+               HReg src1 = iselFltExpr(env, e->Iex.Triop.details->arg2);
+               HReg src2 = iselFltExpr(env, e->Iex.Triop.details->arg3);
+               set_rounding_mode(env, e->Iex.Triop.details->arg1);
+               addInstr(env, LOONGARCH64Instr_FpBinary(LAfpbin_FDIV_S, src2, src1, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            case Iop_DivF64: {
+               HReg  dst = newVRegF(env);
+               HReg src1 = iselFltExpr(env, e->Iex.Triop.details->arg2);
+               HReg src2 = iselFltExpr(env, e->Iex.Triop.details->arg3);
+               set_rounding_mode(env, e->Iex.Triop.details->arg1);
+               addInstr(env, LOONGARCH64Instr_FpBinary(LAfpbin_FDIV_D, src2, src1, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            case Iop_MulF32: {
+               HReg  dst = newVRegF(env);
+               HReg src1 = iselFltExpr(env, e->Iex.Triop.details->arg2);
+               HReg src2 = iselFltExpr(env, e->Iex.Triop.details->arg3);
+               set_rounding_mode(env, e->Iex.Triop.details->arg1);
+               addInstr(env, LOONGARCH64Instr_FpBinary(LAfpbin_FMUL_S, src2, src1, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            case Iop_MulF64: {
+               HReg  dst = newVRegF(env);
+               HReg src1 = iselFltExpr(env, e->Iex.Triop.details->arg2);
+               HReg src2 = iselFltExpr(env, e->Iex.Triop.details->arg3);
+               set_rounding_mode(env, e->Iex.Triop.details->arg1);
+               addInstr(env, LOONGARCH64Instr_FpBinary(LAfpbin_FMUL_D, src2, src1, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            case Iop_ScaleBF32: {
+               HReg  dst = newVRegF(env);
+               HReg src1 = iselFltExpr(env, e->Iex.Triop.details->arg2);
+               HReg src2 = iselFltExpr(env, e->Iex.Triop.details->arg3);
+               set_rounding_mode(env, e->Iex.Triop.details->arg1);
+               addInstr(env, LOONGARCH64Instr_FpBinary(LAfpbin_FSCALEB_S, src2, src1, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            case Iop_ScaleBF64: {
+               HReg  dst = newVRegF(env);
+               HReg src1 = iselFltExpr(env, e->Iex.Triop.details->arg2);
+               HReg src2 = iselFltExpr(env, e->Iex.Triop.details->arg3);
+               set_rounding_mode(env, e->Iex.Triop.details->arg1);
+               addInstr(env, LOONGARCH64Instr_FpBinary(LAfpbin_FSCALEB_D, src2, src1, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            case Iop_SubF32: {
+               HReg  dst = newVRegF(env);
+               HReg src1 = iselFltExpr(env, e->Iex.Triop.details->arg2);
+               HReg src2 = iselFltExpr(env, e->Iex.Triop.details->arg3);
+               set_rounding_mode(env, e->Iex.Triop.details->arg1);
+               addInstr(env, LOONGARCH64Instr_FpBinary(LAfpbin_FSUB_S, src2, src1, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            case Iop_SubF64: {
+               HReg  dst = newVRegF(env);
+               HReg src1 = iselFltExpr(env, e->Iex.Triop.details->arg2);
+               HReg src2 = iselFltExpr(env, e->Iex.Triop.details->arg3);
+               set_rounding_mode(env, e->Iex.Triop.details->arg1);
+               addInstr(env, LOONGARCH64Instr_FpBinary(LAfpbin_FSUB_D, src2, src1, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            default:
+               goto irreducible;
+         }
+      }
+
+      /* --------- BINARY OP --------- */
+      case Iex_Binop: {
+         switch (e->Iex.Binop.op) {
+            case Iop_F64toF32: {
+               HReg dst = newVRegF(env);
+               HReg src = iselFltExpr(env, e->Iex.Binop.arg2);
+               set_rounding_mode(env, e->Iex.Binop.arg1);
+               addInstr(env, LOONGARCH64Instr_FpUnary(LAfpun_FCVT_S_D, src, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            case Iop_I32StoF32: {
+               HReg tmp = newVRegF(env);
+               HReg dst = newVRegF(env);
+               HReg src = iselIntExpr_R(env, e->Iex.Binop.arg2);
+               addInstr(env, LOONGARCH64Instr_FpMove(LAfpmove_MOVGR2FR_D, src, tmp));
+               set_rounding_mode(env, e->Iex.Binop.arg1);
+               addInstr(env, LOONGARCH64Instr_FpUnary(LAfpun_FFINT_S_W, tmp, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            case Iop_I64StoF32: {
+               HReg tmp = newVRegF(env);
+               HReg dst = newVRegF(env);
+               HReg src = iselIntExpr_R(env, e->Iex.Binop.arg2);
+               addInstr(env, LOONGARCH64Instr_FpMove(LAfpmove_MOVGR2FR_D, src, tmp));
+               set_rounding_mode(env, e->Iex.Binop.arg1);
+               addInstr(env, LOONGARCH64Instr_FpUnary(LAfpun_FFINT_S_L, tmp, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            case Iop_I64StoF64: {
+               HReg tmp = newVRegF(env);
+               HReg dst = newVRegF(env);
+               HReg src = iselIntExpr_R(env, e->Iex.Binop.arg2);
+               addInstr(env, LOONGARCH64Instr_FpMove(LAfpmove_MOVGR2FR_D, src, tmp));
+               set_rounding_mode(env, e->Iex.Binop.arg1);
+               addInstr(env, LOONGARCH64Instr_FpUnary(LAfpun_FFINT_D_L, tmp, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            case Iop_LogBF32: {
+               HReg dst = newVRegF(env);
+               HReg src = iselFltExpr(env, e->Iex.Binop.arg2);
+               set_rounding_mode(env, e->Iex.Binop.arg1);
+               addInstr(env, LOONGARCH64Instr_FpUnary(LAfpun_FLOGB_S, src, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            case Iop_LogBF64: {
+               HReg dst = newVRegF(env);
+               HReg src = iselFltExpr(env, e->Iex.Binop.arg2);
+               set_rounding_mode(env, e->Iex.Binop.arg1);
+               addInstr(env, LOONGARCH64Instr_FpUnary(LAfpun_FLOGB_D, src, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            case Iop_MaxNumAbsF32: {
+               HReg  dst = newVRegF(env);
+               HReg src2 = iselFltExpr(env, e->Iex.Binop.arg2);
+               HReg src1 = iselFltExpr(env, e->Iex.Binop.arg1);
+               addInstr(env, LOONGARCH64Instr_FpBinary(LAfpbin_FMAXA_S, src2, src1, dst));
+               return dst;
+            }
+            case Iop_MaxNumF32: {
+               HReg  dst = newVRegF(env);
+               HReg src2 = iselFltExpr(env, e->Iex.Binop.arg2);
+               HReg src1 = iselFltExpr(env, e->Iex.Binop.arg1);
+               addInstr(env, LOONGARCH64Instr_FpBinary(LAfpbin_FMAX_S, src2, src1, dst));
+               return dst;
+            }
+            case Iop_MaxNumAbsF64: {
+               HReg  dst = newVRegF(env);
+               HReg src2 = iselFltExpr(env, e->Iex.Binop.arg2);
+               HReg src1 = iselFltExpr(env, e->Iex.Binop.arg1);
+               addInstr(env, LOONGARCH64Instr_FpBinary(LAfpbin_FMAXA_D, src2, src1, dst));
+               return dst;
+            }
+            case Iop_MaxNumF64: {
+               HReg  dst = newVRegF(env);
+               HReg src2 = iselFltExpr(env, e->Iex.Binop.arg2);
+               HReg src1 = iselFltExpr(env, e->Iex.Binop.arg1);
+               addInstr(env, LOONGARCH64Instr_FpBinary(LAfpbin_FMAX_D, src2, src1, dst));
+               return dst;
+            }
+            case Iop_MinNumAbsF32: {
+               HReg  dst = newVRegF(env);
+               HReg src2 = iselFltExpr(env, e->Iex.Binop.arg2);
+               HReg src1 = iselFltExpr(env, e->Iex.Binop.arg1);
+               addInstr(env, LOONGARCH64Instr_FpBinary(LAfpbin_FMINA_S, src2, src1, dst));
+               return dst;
+            }
+            case Iop_MinNumF32: {
+               HReg  dst = newVRegF(env);
+               HReg src2 = iselFltExpr(env, e->Iex.Binop.arg2);
+               HReg src1 = iselFltExpr(env, e->Iex.Binop.arg1);
+               addInstr(env, LOONGARCH64Instr_FpBinary(LAfpbin_FMIN_S, src2, src1, dst));
+               return dst;
+            }
+            case Iop_MinNumAbsF64: {
+               HReg  dst = newVRegF(env);
+               HReg src2 = iselFltExpr(env, e->Iex.Binop.arg2);
+               HReg src1 = iselFltExpr(env, e->Iex.Binop.arg1);
+               addInstr(env, LOONGARCH64Instr_FpBinary(LAfpbin_FMINA_D, src2, src1, dst));
+               return dst;
+            }
+            case Iop_MinNumF64: {
+               HReg  dst = newVRegF(env);
+               HReg src2 = iselFltExpr(env, e->Iex.Binop.arg2);
+               HReg src1 = iselFltExpr(env, e->Iex.Binop.arg1);
+               addInstr(env, LOONGARCH64Instr_FpBinary(LAfpbin_FMIN_D, src2, src1, dst));
+               return dst;
+            }
+            case Iop_RoundF32toInt: {
+               HReg dst = newVRegF(env);
+               HReg src = iselFltExpr(env, e->Iex.Binop.arg2);
+               set_rounding_mode(env, e->Iex.Binop.arg1);
+               addInstr(env, LOONGARCH64Instr_FpUnary(LAfpun_FRINT_S, src, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            case Iop_RoundF64toInt: {
+               HReg dst = newVRegF(env);
+               HReg src = iselFltExpr(env, e->Iex.Binop.arg2);
+               set_rounding_mode(env, e->Iex.Binop.arg1);
+               addInstr(env, LOONGARCH64Instr_FpUnary(LAfpun_FRINT_D, src, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            case Iop_RSqrtF32: {
+               HReg dst = newVRegF(env);
+               HReg src = iselFltExpr(env, e->Iex.Binop.arg2);
+               set_rounding_mode(env, e->Iex.Binop.arg1);
+               addInstr(env, LOONGARCH64Instr_FpUnary(LAfpun_FRSQRT_S, src, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            case Iop_RSqrtF64: {
+               HReg dst = newVRegF(env);
+               HReg src = iselFltExpr(env, e->Iex.Binop.arg2);
+               set_rounding_mode(env, e->Iex.Binop.arg1);
+               addInstr(env, LOONGARCH64Instr_FpUnary(LAfpun_FRSQRT_D, src, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            case Iop_SqrtF32: {
+               HReg dst = newVRegF(env);
+               HReg src = iselFltExpr(env, e->Iex.Binop.arg2);
+               set_rounding_mode(env, e->Iex.Binop.arg1);
+               addInstr(env, LOONGARCH64Instr_FpUnary(LAfpun_FSQRT_S, src, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            case Iop_SqrtF64: {
+               HReg dst = newVRegF(env);
+               HReg src = iselFltExpr(env, e->Iex.Binop.arg2);
+               set_rounding_mode(env, e->Iex.Binop.arg1);
+               addInstr(env, LOONGARCH64Instr_FpUnary(LAfpun_FSQRT_D, src, dst));
+               set_rounding_mode_default(env);
+               return dst;
+            }
+            default:
+               goto irreducible;
+         }
+      }
+
+      /* --------- UNARY OP --------- */
+      case Iex_Unop: {
+         switch (e->Iex.Unop.op) {
+            case Iop_AbsF32: {
+               HReg dst = newVRegF(env);
+               HReg src = iselFltExpr(env, e->Iex.Unop.arg);
+               addInstr(env, LOONGARCH64Instr_FpUnary(LAfpun_FABS_S, src, dst));
+               return dst;
+            }
+            case Iop_AbsF64: {
+               HReg dst = newVRegF(env);
+               HReg src = iselFltExpr(env, e->Iex.Unop.arg);
+               addInstr(env, LOONGARCH64Instr_FpUnary(LAfpun_FABS_D, src, dst));
+               return dst;
+            }
+            case Iop_F32toF64: {
+               HReg dst = newVRegF(env);
+               HReg src = iselFltExpr(env, e->Iex.Unop.arg);
+               addInstr(env, LOONGARCH64Instr_FpUnary(LAfpun_FCVT_D_S, src, dst));
+               return dst;
+            }
+            case Iop_I32StoF64: {
+               HReg tmp = newVRegF(env);
+               HReg dst = newVRegF(env);
+               HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               addInstr(env, LOONGARCH64Instr_FpMove(LAfpmove_MOVGR2FR_D, src, tmp));
+               addInstr(env, LOONGARCH64Instr_FpUnary(LAfpun_FFINT_D_W, tmp, dst));
+               return dst;
+            }
+            case Iop_NegF32: {
+               HReg dst = newVRegF(env);
+               HReg src = iselFltExpr(env, e->Iex.Unop.arg);
+               addInstr(env, LOONGARCH64Instr_FpUnary(LAfpun_FNEG_S, src, dst));
+               return dst;
+            }
+            case Iop_NegF64: {
+               HReg dst = newVRegF(env);
+               HReg src = iselFltExpr(env, e->Iex.Unop.arg);
+               addInstr(env, LOONGARCH64Instr_FpUnary(LAfpun_FNEG_D, src, dst));
+               return dst;
+            }
+            case Iop_ReinterpI32asF32: {
+               HReg dst = newVRegF(env);
+               HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               addInstr(env, LOONGARCH64Instr_FpMove(LAfpmove_MOVGR2FR_W, src, dst));
+               return dst;
+            }
+            case Iop_ReinterpI64asF64: {
+               HReg dst = newVRegF(env);
+               HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               addInstr(env, LOONGARCH64Instr_FpMove(LAfpmove_MOVGR2FR_D, src, dst));
+               return dst;
+            }
+            default:
+               goto irreducible;
+         }
+      }
+
+      /* --------- LITERAL --------- */
+      case Iex_Const: {
+         /* Just handle the one case. */
+         IRConst* con = e->Iex.Const.con;
+         if (con->tag == Ico_F32i && con->Ico.F32i == 1) {
+            HReg          tmp = newVRegI(env);
+            HReg          dst = newVRegF(env);
+            LOONGARCH64RI* ri = LOONGARCH64RI_I(1, 12, True);
+            addInstr(env, LOONGARCH64Instr_Binary(LAbin_ADDI_W, ri, hregZERO(), tmp));
+            addInstr(env, LOONGARCH64Instr_FpMove(LAfpmove_MOVGR2FR_W, tmp, dst));
+            addInstr(env, LOONGARCH64Instr_FpUnary(LAfpun_FFINT_S_W, dst, dst));
+            return dst;
+         } else if (con->tag == Ico_F64i && con->Ico.F64i == 1) {
+            HReg          tmp = newVRegI(env);
+            HReg          dst = newVRegF(env);
+            LOONGARCH64RI* ri = LOONGARCH64RI_I(1, 12, True);
+            addInstr(env, LOONGARCH64Instr_Binary(LAbin_ADDI_D, ri, hregZERO(), tmp));
+            addInstr(env, LOONGARCH64Instr_FpMove(LAfpmove_MOVGR2FR_D, tmp, dst));
+            addInstr(env, LOONGARCH64Instr_FpUnary(LAfpun_FFINT_D_L, dst, dst));
+            return dst;
+         } else {
+            goto irreducible;
+         }
+      }
+
+      case Iex_ITE: {
+         HReg   r0 = iselFltExpr(env, e->Iex.ITE.iffalse);
+         HReg   r1 = iselFltExpr(env, e->Iex.ITE.iftrue);
+         HReg cond = iselCondCode_R(env, e->Iex.ITE.cond);
+         HReg  dst = newVRegF(env);
+         addInstr(env, LOONGARCH64Instr_CMove(cond, r0, r1, dst, False));
+         return dst;
+      }
+
+      default:
+         break;
+   }
+
+   /* We get here if no pattern matched. */
+irreducible:
+   ppIRExpr(e);
+   vpanic("iselFltExpr(loongarch64): cannot reduce tree");
+}
+
+/*---------------------------------------------------------*/
+/*--- ISEL: Vector expressions (128 bit)                ---*/
+/*---------------------------------------------------------*/
+
+/* Compute a vector value into a register, the identity of
+   which is returned.  As with iselIntExpr_R, the reg may be either
+   real or virtual; in any case it must not be changed by subsequent
+   code emitted by the caller.  */
+
+static HReg iselV128Expr ( ISelEnv* env, IRExpr* e )
+{
+   HReg r = iselV128Expr_wrk(env, e);
+
+   /* sanity checks ... */
+   vassert(hregClass(r) == HRcVec128);
+   vassert(hregIsVirtual(r));
+
+   return r;
+}
+
+/* DO NOT CALL THIS DIRECTLY */
+static HReg iselV128Expr_wrk ( ISelEnv* env, IRExpr* e )
+{
+   IRType ty = typeOfIRExpr(env->type_env, e);
+   vassert(e);
+   vassert(ty == Ity_V128);
+
+   switch (e->tag) {
+      /* --------- TEMP --------- */
+      case Iex_RdTmp:
+         return lookupIRTemp(env, e->Iex.RdTmp.tmp);
+
+      /* --------- LOAD --------- */
+      case Iex_Load: {
+         if (e->Iex.Load.end != Iend_LE)
+            goto irreducible;
+
+         HReg                dst = newVRegV(env);
+         LOONGARCH64AMode*    am = iselIntExpr_AMode(env, e->Iex.Load.addr, ty);
+         LOONGARCH64VecLoadOp op = (am->tag == LAam_RI) ? LAvecload_VLD : LAvecload_VLDX;
+         addInstr(env, LOONGARCH64Instr_VecLoad(op, am, dst));
+         return dst;
+      }
+
+      /* --------- GET --------- */
+      case Iex_Get: {
+         Bool ri  = e->Iex.Get.offset < 1024;
+         HReg dst = newVRegV(env);
+         HReg tmp;
+         LOONGARCH64AMode* am;
+         LOONGARCH64VecLoadOp op;
+         if (ri) {
+            op = LAvecload_VLD;
+            am = LOONGARCH64AMode_RI(hregGSP(), e->Iex.Get.offset);
+         } else {
+            op = LAvecload_VLDX;
+            tmp = newVRegI(env);
+            addInstr(env, LOONGARCH64Instr_LI(e->Iex.Get.offset, tmp));
+            am = LOONGARCH64AMode_RR(hregGSP(), tmp);
+         }
+         addInstr(env, LOONGARCH64Instr_VecLoad(op, am, dst));
+         return dst;
+      }
+
+      /* --------- TERNARY OP --------- */
+      case Iex_Triop: {
+         IRTriop *triop = e->Iex.Triop.details;
+         switch (triop->op) {
+            case Iop_SetElem8x16: case Iop_SetElem16x8: case Iop_SetElem32x4: {
+                  LOONGARCH64VecBinOp op;
+                  UChar size;
+                  switch (triop->op) {
+                     case Iop_SetElem8x16: op = LAvecbin_VINSGR2VR_B; size = 4; break;
+                     case Iop_SetElem16x8: op = LAvecbin_VINSGR2VR_H; size = 3; break;
+                     case Iop_SetElem32x4: op = LAvecbin_VINSGR2VR_W; size = 2; break;
+                     default:              vassert(0);                          break;
+                  }
+                  HReg            dst = newVRegV(env);
+                  HReg           src1 = iselV128Expr(env, triop->arg1);
+                  LOONGARCH64RI* src2 = iselIntExpr_RI(env, triop->arg2, size, False);
+                  HReg           src3 = iselIntExpr_R(env, triop->arg3);
+                  addInstr(env, LOONGARCH64Instr_VecMove(dst, src1));
+                  addInstr(env, LOONGARCH64Instr_VecBinary(op, src2, src3, dst));
+                  return dst;
+            }
+            case Iop_Add32Fx4: case Iop_Add64Fx2:
+            case Iop_Sub32Fx4: case Iop_Sub64Fx2:
+            case Iop_Mul32Fx4: case Iop_Mul64Fx2:
+            case Iop_Div32Fx4: case Iop_Div64Fx2: {
+                  LOONGARCH64VecBinOp op;
+                  switch (triop->op) {
+                     case Iop_Add32Fx4: op = LAvecbin_VFADD_S; break;
+                     case Iop_Add64Fx2: op = LAvecbin_VFADD_D; break;
+                     case Iop_Sub32Fx4: op = LAvecbin_VFSUB_S; break;
+                     case Iop_Sub64Fx2: op = LAvecbin_VFSUB_D; break;
+                     case Iop_Mul32Fx4: op = LAvecbin_VFMUL_S; break;
+                     case Iop_Mul64Fx2: op = LAvecbin_VFMUL_D; break;
+                     case Iop_Div32Fx4: op = LAvecbin_VFDIV_S; break;
+                     case Iop_Div64Fx2: op = LAvecbin_VFDIV_D; break;
+                     default:           vassert(0);            break;
+                  }
+                  HReg  dst  = newVRegV(env);
+                  HReg  src1 = iselV128Expr(env, triop->arg2);
+                  HReg  src2 = iselV128Expr(env, triop->arg3);
+                  set_rounding_mode(env, triop->arg1);
+                  addInstr(env, LOONGARCH64Instr_VecBinary(op, LOONGARCH64RI_R(src2), src1, dst));
+                  set_rounding_mode_default(env);
+                  return dst;
+            }
+            default: goto irreducible;
+         }
+      }
+
+      /* --------- BINARY OP --------- */
+      case Iex_Binop: {
+         switch (e->Iex.Binop.op) {
+            case Iop_AndV128: case Iop_OrV128:  case Iop_XorV128:
+            case Iop_Add8x16: case Iop_Add16x8: case Iop_Add32x4: case Iop_Add64x2: case Iop_Add128x1:
+            case Iop_Sub8x16: case Iop_Sub16x8: case Iop_Sub32x4: case Iop_Sub64x2: case Iop_Sub128x1:
+            case Iop_QAdd8Sx16: case Iop_QAdd16Sx8: case Iop_QAdd32Sx4: case Iop_QAdd64Sx2:
+            case Iop_QAdd8Ux16: case Iop_QAdd16Ux8: case Iop_QAdd32Ux4: case Iop_QAdd64Ux2:
+            case Iop_QSub8Sx16: case Iop_QSub16Sx8: case Iop_QSub32Sx4: case Iop_QSub64Sx2:
+            case Iop_QSub8Ux16: case Iop_QSub16Ux8: case Iop_QSub32Ux4: case Iop_QSub64Ux2:
+            case Iop_InterleaveHI8x16: case Iop_InterleaveHI16x8: case Iop_InterleaveHI32x4: case Iop_InterleaveHI64x2:
+            case Iop_InterleaveLO8x16: case Iop_InterleaveLO16x8: case Iop_InterleaveLO32x4: case Iop_InterleaveLO64x2:
+            case Iop_Max8Sx16: case Iop_Max16Sx8: case Iop_Max32Sx4: case Iop_Max64Sx2:
+            case Iop_Max8Ux16: case Iop_Max16Ux8: case Iop_Max32Ux4: case Iop_Max64Ux2:
+            case Iop_Min8Sx16: case Iop_Min16Sx8: case Iop_Min32Sx4: case Iop_Min64Sx2:
+            case Iop_Min8Ux16: case Iop_Min16Ux8: case Iop_Min32Ux4: case Iop_Min64Ux2:
+            case Iop_CmpEQ8x16: case Iop_CmpEQ16x8: case Iop_CmpEQ32x4: case Iop_CmpEQ64x2:
+            case Iop_PackOddLanes8x16: case Iop_PackOddLanes16x8: case Iop_PackOddLanes32x4:
+            case Iop_PackEvenLanes8x16: case Iop_PackEvenLanes16x8: case Iop_PackEvenLanes32x4:
+            case Iop_Avg8Ux16: case Iop_Avg16Ux8: case Iop_Avg32Ux4: case Iop_Avg64Ux2:
+            case Iop_Avg8Sx16: case Iop_Avg16Sx8: case Iop_Avg32Sx4: case Iop_Avg64Sx2:
+            case Iop_Mul8x16: case Iop_Mul16x8: case Iop_Mul32x4:
+            case Iop_MulHi8Ux16: case Iop_MulHi16Ux8: case Iop_MulHi32Ux4:
+            case Iop_MulHi8Sx16: case Iop_MulHi16Sx8: case Iop_MulHi32Sx4:
+            case Iop_Shl8x16: case Iop_Shl16x8: case Iop_Shl32x4: case Iop_Shl64x2:
+            case Iop_Shr8x16: case Iop_Shr16x8: case Iop_Shr32x4: case Iop_Shr64x2:
+            case Iop_Sar8x16: case Iop_Sar16x8: case Iop_Sar32x4: case Iop_Sar64x2:
+            case Iop_CmpGT8Sx16: case Iop_CmpGT16Sx8: case Iop_CmpGT32Sx4: case Iop_CmpGT64Sx2:
+            case Iop_CmpGT8Ux16: case Iop_CmpGT16Ux8: case Iop_CmpGT32Ux4: case Iop_CmpGT64Ux2:
+            case Iop_Max32Fx4: case Iop_Max64Fx2:
+            case Iop_Min32Fx4: case Iop_Min64Fx2: {
+               LOONGARCH64VecBinOp op;
+               Bool reverse = False;
+               switch (e->Iex.Binop.op) {
+                  case Iop_AndV128:  op = LAvecbin_VAND_V; break;
+                  case Iop_OrV128:   op = LAvecbin_VOR_V;  break;
+                  case Iop_XorV128:  op = LAvecbin_VXOR_V; break;
+                  case Iop_Add8x16:  op = LAvecbin_VADD_B; break;
+                  case Iop_Add16x8:  op = LAvecbin_VADD_H; break;
+                  case Iop_Add32x4:  op = LAvecbin_VADD_W; break;
+                  case Iop_Add64x2:  op = LAvecbin_VADD_D; break;
+                  case Iop_Add128x1: op = LAvecbin_VADD_Q; break;
+                  case Iop_Sub8x16:  op = LAvecbin_VSUB_B; break;
+                  case Iop_Sub16x8:  op = LAvecbin_VSUB_H; break;
+                  case Iop_Sub32x4:  op = LAvecbin_VSUB_W; break;
+                  case Iop_Sub64x2:  op = LAvecbin_VSUB_D; break;
+                  case Iop_Sub128x1: op = LAvecbin_VSUB_Q; break;
+                  case Iop_QAdd8Sx16: op = LAvecbin_VSADD_B; break;
+                  case Iop_QAdd16Sx8: op = LAvecbin_VSADD_H; break;
+                  case Iop_QAdd32Sx4: op = LAvecbin_VSADD_W; break;
+                  case Iop_QAdd64Sx2: op = LAvecbin_VSADD_D; break;
+                  case Iop_QAdd8Ux16: op = LAvecbin_VSADD_BU; break;
+                  case Iop_QAdd16Ux8: op = LAvecbin_VSADD_HU; break;
+                  case Iop_QAdd32Ux4: op = LAvecbin_VSADD_WU; break;
+                  case Iop_QAdd64Ux2: op = LAvecbin_VSADD_DU; break;
+                  case Iop_QSub8Sx16: op = LAvecbin_VSSUB_B; break;
+                  case Iop_QSub16Sx8: op = LAvecbin_VSSUB_H; break;
+                  case Iop_QSub32Sx4: op = LAvecbin_VSSUB_W; break;
+                  case Iop_QSub64Sx2: op = LAvecbin_VSSUB_D; break;
+                  case Iop_QSub8Ux16: op = LAvecbin_VSSUB_BU; break;
+                  case Iop_QSub16Ux8: op = LAvecbin_VSSUB_HU; break;
+                  case Iop_QSub32Ux4: op = LAvecbin_VSSUB_WU; break;
+                  case Iop_QSub64Ux2: op = LAvecbin_VSSUB_DU; break;
+                  case Iop_InterleaveHI8x16: op = LAvecbin_VILVH_B; break;
+                  case Iop_InterleaveHI16x8: op = LAvecbin_VILVH_H; break;
+                  case Iop_InterleaveHI32x4: op = LAvecbin_VILVH_W; break;
+                  case Iop_InterleaveHI64x2: op = LAvecbin_VILVH_D; break;
+                  case Iop_InterleaveLO8x16: op = LAvecbin_VILVL_B; break;
+                  case Iop_InterleaveLO16x8: op = LAvecbin_VILVL_H; break;
+                  case Iop_InterleaveLO32x4: op = LAvecbin_VILVL_W; break;
+                  case Iop_InterleaveLO64x2: op = LAvecbin_VILVL_D; break;
+                  case Iop_Max8Sx16:  op = LAvecbin_VMAX_B; break;
+                  case Iop_Max16Sx8:  op = LAvecbin_VMAX_H; break;
+                  case Iop_Max32Sx4:  op = LAvecbin_VMAX_W; break;
+                  case Iop_Max64Sx2:  op = LAvecbin_VMAX_D; break;
+                  case Iop_Max8Ux16:  op = LAvecbin_VMAX_BU; break;
+                  case Iop_Max16Ux8:  op = LAvecbin_VMAX_HU; break;
+                  case Iop_Max32Ux4:  op = LAvecbin_VMAX_WU; break;
+                  case Iop_Max64Ux2:  op = LAvecbin_VMAX_DU; break;
+                  case Iop_Min8Sx16:  op = LAvecbin_VMIN_B; break;
+                  case Iop_Min16Sx8:  op = LAvecbin_VMIN_H; break;
+                  case Iop_Min32Sx4:  op = LAvecbin_VMIN_W; break;
+                  case Iop_Min64Sx2:  op = LAvecbin_VMIN_D; break;
+                  case Iop_Min8Ux16:  op = LAvecbin_VMIN_BU; break;
+                  case Iop_Min16Ux8:  op = LAvecbin_VMIN_HU; break;
+                  case Iop_Min32Ux4:  op = LAvecbin_VMIN_WU; break;
+                  case Iop_Min64Ux2:  op = LAvecbin_VMIN_DU; break;
+                  case Iop_CmpEQ8x16: op = LAvecbin_VSEQ_B; break;
+                  case Iop_CmpEQ16x8: op = LAvecbin_VSEQ_H; break;
+                  case Iop_CmpEQ32x4: op = LAvecbin_VSEQ_W; break;
+                  case Iop_CmpEQ64x2: op = LAvecbin_VSEQ_D; break;
+                  case Iop_PackOddLanes8x16: op = LAvecbin_VPICKOD_B; break;
+                  case Iop_PackOddLanes16x8: op = LAvecbin_VPICKOD_H; break;
+                  case Iop_PackOddLanes32x4: op = LAvecbin_VPICKOD_W; break;
+                  case Iop_PackEvenLanes8x16: op = LAvecbin_VPICKEV_B; break;
+                  case Iop_PackEvenLanes16x8: op = LAvecbin_VPICKEV_H; break;
+                  case Iop_PackEvenLanes32x4: op = LAvecbin_VPICKEV_W; break;
+                  case Iop_Avg8Ux16: op = LAvecbin_VAVGR_BU; break;
+                  case Iop_Avg16Ux8: op = LAvecbin_VAVGR_HU; break;
+                  case Iop_Avg32Ux4: op = LAvecbin_VAVGR_WU; break;
+                  case Iop_Avg64Ux2: op = LAvecbin_VAVGR_DU; break;
+                  case Iop_Avg8Sx16: op = LAvecbin_VAVGR_B;  break;
+                  case Iop_Avg16Sx8: op = LAvecbin_VAVGR_H;  break;
+                  case Iop_Avg32Sx4: op = LAvecbin_VAVGR_W;  break;
+                  case Iop_Avg64Sx2: op = LAvecbin_VAVGR_D;  break;
+                  case Iop_Mul8x16:  op = LAvecbin_VMUL_B; break;
+                  case Iop_Mul16x8:  op = LAvecbin_VMUL_H; break;
+                  case Iop_Mul32x4:  op = LAvecbin_VMUL_W; break;
+                  case Iop_MulHi8Ux16: op = LAvecbin_VMUH_BU; break;
+                  case Iop_MulHi16Ux8: op = LAvecbin_VMUH_HU; break;
+                  case Iop_MulHi32Ux4: op = LAvecbin_VMUH_WU; break;
+                  case Iop_MulHi8Sx16: op = LAvecbin_VMUH_B; break;
+                  case Iop_MulHi16Sx8: op = LAvecbin_VMUH_H; break;
+                  case Iop_MulHi32Sx4: op = LAvecbin_VMUH_W; break;
+                  case Iop_Shl8x16: op = LAvecbin_VSLL_B; break;
+                  case Iop_Shl16x8: op = LAvecbin_VSLL_H; break;
+                  case Iop_Shl32x4: op = LAvecbin_VSLL_W; break;
+                  case Iop_Shl64x2: op = LAvecbin_VSLL_D; break;
+                  case Iop_Shr8x16: op = LAvecbin_VSRL_B; break;
+                  case Iop_Shr16x8: op = LAvecbin_VSRL_H; break;
+                  case Iop_Shr32x4: op = LAvecbin_VSRL_W; break;
+                  case Iop_Shr64x2: op = LAvecbin_VSRL_D; break;
+                  case Iop_Sar8x16: op = LAvecbin_VSRA_B; break;
+                  case Iop_Sar16x8: op = LAvecbin_VSRA_H; break;
+                  case Iop_Sar32x4: op = LAvecbin_VSRA_W; break;
+                  case Iop_Sar64x2: op = LAvecbin_VSRA_D; break;
+                  case Iop_CmpGT8Sx16: op = LAvecbin_VSLT_B;  reverse = True; break;
+                  case Iop_CmpGT16Sx8: op = LAvecbin_VSLT_H;  reverse = True; break;
+                  case Iop_CmpGT32Sx4: op = LAvecbin_VSLT_W;  reverse = True; break;
+                  case Iop_CmpGT64Sx2: op = LAvecbin_VSLT_D;  reverse = True; break;
+                  case Iop_CmpGT8Ux16: op = LAvecbin_VSLT_BU; reverse = True; break;
+                  case Iop_CmpGT16Ux8: op = LAvecbin_VSLT_HU; reverse = True; break;
+                  case Iop_CmpGT32Ux4: op = LAvecbin_VSLT_WU; reverse = True; break;
+                  case Iop_CmpGT64Ux2: op = LAvecbin_VSLT_DU; reverse = True; break;
+                  case Iop_Max32Fx4: op = LAvecbin_VFMAX_S; break;
+                  case Iop_Max64Fx2: op = LAvecbin_VFMAX_D; break;
+                  case Iop_Min32Fx4: op = LAvecbin_VFMIN_S; break;
+                  case Iop_Min64Fx2: op = LAvecbin_VFMIN_D; break;
+                  default:           vassert(0);            break;
+               }
+               HReg dst  = newVRegV(env);
+               HReg src1 = iselV128Expr(env, e->Iex.Binop.arg1);
+               HReg src2 = iselV128Expr(env, e->Iex.Binop.arg2);
+               if (reverse)
+                  addInstr(env, LOONGARCH64Instr_VecBinary(op, LOONGARCH64RI_R(src1), src2, dst));
+               else
+                  addInstr(env, LOONGARCH64Instr_VecBinary(op, LOONGARCH64RI_R(src2), src1, dst));
+               return dst;
+            }
+            case Iop_ShlN8x16: case Iop_ShlN16x8: case Iop_ShlN32x4: case Iop_ShlN64x2:
+            case Iop_ShrN8x16: case Iop_ShrN16x8: case Iop_ShrN32x4: case Iop_ShrN64x2:
+            case Iop_SarN8x16: case Iop_SarN16x8: case Iop_SarN32x4: case Iop_SarN64x2:
+            case Iop_ShlV128:  case Iop_ShrV128: {
+               UChar size;
+               LOONGARCH64VecBinOp op;
+               switch (e->Iex.Binop.op) {
+                  case Iop_ShlN8x16: op = LAvecbin_VSLLI_B; size = 3; break;
+                  case Iop_ShlN16x8: op = LAvecbin_VSLLI_H; size = 4; break;
+                  case Iop_ShlN32x4: op = LAvecbin_VSLLI_W; size = 5; break;
+                  case Iop_ShlN64x2: op = LAvecbin_VSLLI_D; size = 6; break;
+                  case Iop_ShrN8x16: op = LAvecbin_VSRLI_B; size = 3; break;
+                  case Iop_ShrN16x8: op = LAvecbin_VSRLI_H; size = 4; break;
+                  case Iop_ShrN32x4: op = LAvecbin_VSRLI_W; size = 5; break;
+                  case Iop_ShrN64x2: op = LAvecbin_VSRLI_D; size = 6; break;
+                  case Iop_SarN8x16: op = LAvecbin_VSRAI_B; size = 3; break;
+                  case Iop_SarN16x8: op = LAvecbin_VSRAI_H; size = 4; break;
+                  case Iop_SarN32x4: op = LAvecbin_VSRAI_W; size = 5; break;
+                  case Iop_SarN64x2: op = LAvecbin_VSRAI_D; size = 6; break;
+                  case Iop_ShlV128:  op = LAvecbin_VBSLL_V; size = 5; break;
+                  case Iop_ShrV128:  op = LAvecbin_VBSRL_V; size = 5; break;
+                  default:           vassert(0);                      break;
+               }
+               HReg dst            = newVRegV(env);
+               HReg src1           = iselV128Expr(env, e->Iex.Binop.arg1);
+               LOONGARCH64RI* src2 = iselIntExpr_RI(env, e->Iex.Binop.arg2, size, False);
+               vassert(e->Iex.Binop.arg2->tag == Iex_Const);
+               vassert(e->Iex.Binop.arg2->Iex.Const.con->tag == Ico_U8);
+               vassert(e->Iex.Binop.arg2->Iex.Const.con->Ico.U8 <= 63);
+               addInstr(env, LOONGARCH64Instr_VecBinary(op, src2, src1, dst));
+               return dst;
+            }
+            case Iop_64HLtoV128: {
+               HReg dst  = newVRegV(env);
+               HReg sHi  = iselIntExpr_R(env, e->Iex.Binop.arg1);
+               HReg sLow = iselIntExpr_R(env, e->Iex.Binop.arg2);
+               addInstr(env, LOONGARCH64Instr_VecBinary(LAvecbin_VINSGR2VR_D,
+                                                        LOONGARCH64RI_I(0, 1, False), sLow, dst));
+               addInstr(env, LOONGARCH64Instr_VecBinary(LAvecbin_VINSGR2VR_D,
+                                                        LOONGARCH64RI_I(1, 1, False), sHi, dst));
+               return dst;
+            }
+            default: goto irreducible;
+         }
+      }
+
+      /* --------- UNARY OP --------- */
+      case Iex_Unop: {
+         switch (e->Iex.Unop.op) {
+            case Iop_32UtoV128: {
+               HReg dst = newVRegV(env);
+               HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               addInstr(env, LOONGARCH64Instr_VecBinary(LAvecbin_VINSGR2VR_W,
+                                                        LOONGARCH64RI_I(0, 2, False), src, dst));
+               addInstr(env, LOONGARCH64Instr_VecBinary(LAvecbin_VINSGR2VR_W,
+                                                        LOONGARCH64RI_I(1, 2, False), hregZERO(), dst));
+               addInstr(env, LOONGARCH64Instr_VecBinary(LAvecbin_VINSGR2VR_W,
+                                                        LOONGARCH64RI_I(2, 2, False), hregZERO(), dst));
+               addInstr(env, LOONGARCH64Instr_VecBinary(LAvecbin_VINSGR2VR_W,
+                                                        LOONGARCH64RI_I(3, 2, False), hregZERO(), dst));
+	            return dst;
+            }
+            case Iop_64UtoV128: {
+               HReg dst = newVRegV(env);
+               HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               addInstr(env, LOONGARCH64Instr_VecBinary(LAvecbin_VINSGR2VR_D,
+                                                        LOONGARCH64RI_I(0, 1, False), src, dst));
+               addInstr(env, LOONGARCH64Instr_VecBinary(LAvecbin_VINSGR2VR_D,
+                                                        LOONGARCH64RI_I(1, 1, False), hregZERO(), dst));
+	            return dst;
+            }
+            case Iop_NotV128: {
+               HReg dst = newVRegV(env);
+               HReg src = iselV128Expr(env, e->Iex.Unop.arg);
+               addInstr(env, LOONGARCH64Instr_VecBinary(LAvecbin_VNOR_V,
+                                                        LOONGARCH64RI_R(src), src, dst));
+	            return dst;
+            }
+            case Iop_Abs8x16: case Iop_Abs16x8:
+            case Iop_Abs32x4: case Iop_Abs64x2: {
+               LOONGARCH64VecBinOp subOp, addOp;
+               switch (e->Iex.Unop.op) {
+                  case Iop_Abs8x16:
+                     subOp = LAvecbin_VSUB_B;
+                     addOp = LAvecbin_VADDA_B;
+                     break;
+                  case Iop_Abs16x8:
+                     subOp = LAvecbin_VSUB_H;
+                     addOp = LAvecbin_VADDA_H;
+                     break;
+                  case Iop_Abs32x4:
+                     subOp = LAvecbin_VSUB_W;
+                     addOp = LAvecbin_VADDA_W;
+                     break;
+                  case Iop_Abs64x2:
+                     subOp = LAvecbin_VSUB_D;
+                     addOp = LAvecbin_VADDA_D;
+                     break;
+                  default:
+                     vassert(0);
+                     break;
+               };
+               HReg dst = newVRegV(env);
+               HReg src = iselV128Expr(env, e->Iex.Unop.arg);
+               HReg sub = newVRegV(env);
+               addInstr(env, LOONGARCH64Instr_VecBinary(subOp, LOONGARCH64RI_R(src), src, sub));
+               addInstr(env, LOONGARCH64Instr_VecBinary(addOp, LOONGARCH64RI_R(src), sub, dst));
+               return dst;
+            }
+            case Iop_Dup8x16: case Iop_Dup16x8: case Iop_Dup32x4: {
+               HReg dst = newVRegV(env);
+               HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+               LOONGARCH64VecUnOp op;
+               switch (e->Iex.Unop.op) {
+                  case Iop_Dup8x16: op = LAvecun_VREPLGR2VR_B; break;
+                  case Iop_Dup16x8: op = LAvecun_VREPLGR2VR_H; break;
+                  case Iop_Dup32x4: op = LAvecun_VREPLGR2VR_W; break;
+                  default:          vassert(0);                break;
+               }
+               addInstr(env, LOONGARCH64Instr_VecUnary(op, src, dst));
+               return dst;
+            }
+            case Iop_V256toV128_0:
+            case Iop_V256toV128_1: {
+               HReg vHi, vLo;
+               iselV256Expr(&vHi, &vLo, env, e->Iex.Unop.arg);
+               return (e->Iex.Unop.op == Iop_V256toV128_1) ? vHi : vLo;
+            }
+            default:
+               goto irreducible;
+         }
+      }
+
+      case Iex_Const: {
+         IRConst *con = e->Iex.Const.con;
+
+         if (con->tag != Ico_V128) {
+            vpanic("iselV128Expr.const(LoongArch)");
+            goto irreducible;
+         }
+
+         HReg dst   = newVRegV(env);
+         UShort val = con->Ico.V128;
+
+         if (val == 0) {
+            addInstr(env, LOONGARCH64Instr_VecUnary(LAvecun_VREPLGR2VR_D, hregZERO(), dst));
+         } else {
+            HReg r_tmp = newVRegI(env);
+            UInt i;
+            addInstr(env, LOONGARCH64Instr_LI(0xfful, r_tmp));
+            if (val & 1) {
+               addInstr(env, LOONGARCH64Instr_VecUnary(LAvecun_VREPLGR2VR_B, r_tmp, dst));
+            } else {
+               addInstr(env, LOONGARCH64Instr_VecUnary(LAvecun_VREPLGR2VR_B, hregZERO(), dst));
+            }
+            for (i = 1; i < 16; i++) {
+               val >>= 1;
+               if (val & 1) {
+                  addInstr(env, LOONGARCH64Instr_VecBinary(LAvecbin_VINSGR2VR_B,
+                                                           LOONGARCH64RI_I(i, 4, False),
+                                                           r_tmp, dst));
+               } else {
+                  addInstr(env, LOONGARCH64Instr_VecBinary(LAvecbin_VINSGR2VR_B,
+                                                           LOONGARCH64RI_I(i, 4, False),
+                                                           hregZERO(), dst));
+               }
+            }
+         }
+
+         return dst;
+      }
+
+      default:
+         break;
+   }
+
+   /* We get here if no pattern matched. */
+irreducible:
+   ppIRExpr(e);
+   vpanic("iselV128Expr(loongarch64): cannot reduce tree");
+}
+
+
+/*---------------------------------------------------------*/
+/*--- ISEL: Vector expressions (256 bit)                ---*/
+/*---------------------------------------------------------*/
+
+/* Compute a vector value into a register, the identity of
+   which is returned.  As with iselIntExpr_R, the reg may be either
+   real or virtual; in any case it must not be changed by subsequent
+   code emitted by the caller.  */
+
+static void iselV256Expr ( HReg* hi, HReg* lo,
+                           ISelEnv* env, IRExpr* e )
+{
+   iselV256Expr_wrk( hi, lo, env, e );
+
+   /* sanity checks ... */
+   vassert(hregClass(*hi) == HRcVec128);
+   vassert(hregIsVirtual(*hi));
+   vassert(hregClass(*lo) == HRcVec128);
+   vassert(hregIsVirtual(*lo));
+}
+
+/* DO NOT CALL THIS DIRECTLY */
+static void iselV256Expr_wrk ( HReg* hi, HReg* lo,
+                               ISelEnv* env, IRExpr* e )
+{
+   IRType ty = typeOfIRExpr(env->type_env, e);
+   vassert(e);
+   vassert(ty == Ity_V256);
+
+   switch (e->tag) {
+      /* --------- TEMP --------- */
+      case Iex_RdTmp: {
+         lookupIRTempPair(hi, lo, env, e->Iex.RdTmp.tmp);
+         return;
+      }
+
+      /* --------- LOAD --------- */
+      case Iex_Load: {
+         if (e->Iex.Load.end != Iend_LE)
+            goto irreducible;
+
+         HReg              dstHi = newVRegV(env);
+         HReg              dstLo = newVRegV(env);
+         LOONGARCH64AMode*    am = iselIntExpr_AMode(env, e->Iex.Load.addr, ty);
+         LOONGARCH64VecLoadOp op = (am->tag == LAam_RI) ? LAvecload_VLD : LAvecload_VLDX;
+         HReg               addr = iselIntExpr_R(env, e->Iex.Load.addr);
+         LOONGARCH64AMode*  am16 = LOONGARCH64AMode_RI(addr, 16);
+         addInstr(env, LOONGARCH64Instr_VecLoad(op, am, dstLo));
+         addInstr(env, LOONGARCH64Instr_VecLoad(LAvecload_VLD, am16, dstHi));
+         *hi = dstHi;
+         *lo = dstLo;
+         return;
+      }
+
+      /* --------- GET --------- */
+      case Iex_Get: {
+         Bool ri = e->Iex.Get.offset < 1024;
+         HReg dstHi = newVRegV(env);
+         HReg dstLo = newVRegV(env);
+         LOONGARCH64VecLoadOp op, op2;
+         HReg tmp, tmp2;
+         LOONGARCH64AMode* am;
+         LOONGARCH64AMode* am2;
+         if (ri) {
+            op = LAvecload_VLD;
+            am = LOONGARCH64AMode_RI(hregGSP(), e->Iex.Get.offset);
+         }  else {
+            op = LAvecload_VLDX;
+            tmp = newVRegI(env);
+            addInstr(env, LOONGARCH64Instr_LI(e->Iex.Get.offset, tmp));
+            am = LOONGARCH64AMode_RR(hregGSP(), tmp);
+         }
+         if (e->Iex.Get.offset + 16 < 1024) {
+            op2 = LAvecload_VLD;
+            am2 = LOONGARCH64AMode_RI(hregGSP(), e->Iex.Get.offset + 16);
+         } else {
+            op2 = LAvecload_VLDX;
+            tmp2 = newVRegI(env);
+            addInstr(env, LOONGARCH64Instr_LI(e->Iex.Get.offset + 16, tmp2));
+            am2 = LOONGARCH64AMode_RR(hregGSP(), tmp2);
+         }
+         addInstr(env, LOONGARCH64Instr_VecLoad(op, am, dstLo));
+         addInstr(env, LOONGARCH64Instr_VecLoad(op2, am2, dstHi));
+         *hi = dstHi;
+         *lo = dstLo;
+         return;
+      }
+
+      /* --------- BINARY OP --------- */
+      case Iex_Binop: {
+         switch (e->Iex.Binop.op) {
+            case Iop_V128HLtoV256: {
+               *hi = iselV128Expr(env, e->Iex.Binop.arg1);
+               *lo = iselV128Expr(env, e->Iex.Binop.arg2);
+               return;
+            }
+            case Iop_XorV256:
+            case Iop_CmpEQ8x32: case Iop_CmpEQ16x16: case Iop_CmpEQ32x8: case Iop_CmpEQ64x4:
+            case Iop_Max8Sx32: case Iop_Max16Sx16: case Iop_Max32Sx8: case Iop_Max64Sx4:
+            case Iop_Max8Ux32: case Iop_Max16Ux16: case Iop_Max32Ux8: case Iop_Max64Ux4:
+            case Iop_Min8Sx32: case Iop_Min16Sx16: case Iop_Min32Sx8: case Iop_Min64Sx4:
+            case Iop_Min8Ux32: case Iop_Min16Ux16: case Iop_Min32Ux8: case Iop_Min64Ux4: {
+               LOONGARCH64VecBinOp op;
+               switch (e->Iex.Binop.op) {
+                  case Iop_XorV256:    op = LAvecbin_VXOR_V; break;
+                  case Iop_CmpEQ8x32:  op = LAvecbin_VSEQ_B; break;
+                  case Iop_CmpEQ16x16: op = LAvecbin_VSEQ_H; break;
+                  case Iop_CmpEQ32x8:  op = LAvecbin_VSEQ_W; break;
+                  case Iop_CmpEQ64x4:  op = LAvecbin_VSEQ_D; break;
+                  case Iop_Max8Sx32:   op = LAvecbin_VMAX_B; break;
+                  case Iop_Max16Sx16:  op = LAvecbin_VMAX_H; break;
+                  case Iop_Max32Sx8:   op = LAvecbin_VMAX_W; break;
+                  case Iop_Max64Sx4:   op = LAvecbin_VMAX_D; break;
+                  case Iop_Max8Ux32:   op = LAvecbin_VMAX_BU; break;
+                  case Iop_Max16Ux16:  op = LAvecbin_VMAX_HU; break;
+                  case Iop_Max32Ux8:   op = LAvecbin_VMAX_WU; break;
+                  case Iop_Max64Ux4:   op = LAvecbin_VMAX_DU; break;
+                  case Iop_Min8Sx32:   op = LAvecbin_VMIN_B; break;
+                  case Iop_Min16Sx16:  op = LAvecbin_VMIN_H; break;
+                  case Iop_Min32Sx8:   op = LAvecbin_VMIN_W; break;
+                  case Iop_Min64Sx4:   op = LAvecbin_VMIN_D; break;
+                  case Iop_Min8Ux32:   op = LAvecbin_VMIN_BU; break;
+                  case Iop_Min16Ux16:  op = LAvecbin_VMIN_HU; break;
+                  case Iop_Min32Ux8:   op = LAvecbin_VMIN_WU; break;
+                  case Iop_Min64Ux4:   op = LAvecbin_VMIN_DU; break;
+                  default:             vassert(0);            break;
+               }
+               HReg src1Hi, src1Lo, src2Hi, src2Lo;
+               iselV256Expr(&src1Hi, &src1Lo, env, e->Iex.Binop.arg1);
+               iselV256Expr(&src2Hi, &src2Lo, env, e->Iex.Binop.arg2);
+               HReg dstHi = newVRegV(env);
+               HReg dstLo = newVRegV(env);
+               addInstr(env, LOONGARCH64Instr_VecBinary(op, LOONGARCH64RI_R(src2Hi), src1Hi, dstHi));
+               addInstr(env, LOONGARCH64Instr_VecBinary(op, LOONGARCH64RI_R(src2Lo), src1Lo, dstLo));
+               *hi = dstHi;
+               *lo = dstLo;
+               return;
+            }
+            default: goto irreducible;
+         }
+      }
+
+      default:
+         break;
+   }
+
+   /* We get here if no pattern matched. */
+irreducible:
+   ppIRExpr(e);
+   vpanic("iselV256Expr(loongarch64): cannot reduce tree");
+}
+
+
+/*---------------------------------------------------------*/
+/*--- ISEL: Statements                                  ---*/
+/*---------------------------------------------------------*/
+
+static void iselStmtStore ( ISelEnv* env, IRStmt* stmt )
+{
+   IRType tya = typeOfIRExpr(env->type_env, stmt->Ist.Store.addr);
+   IRType tyd = typeOfIRExpr(env->type_env, stmt->Ist.Store.data);
+
+   if (tya != Ity_I64 || stmt->Ist.Store.end != Iend_LE)
+      vpanic("iselStmt(loongarch64): Ist_Store");
+
+   LOONGARCH64AMode* am = iselIntExpr_AMode(env, stmt->Ist.Store.addr, tyd);
+   LOONGARCH64StoreOp op;
+   Bool ok = True;
+   switch (tyd) {
+      case Ity_I8:
+         op = (am->tag == LAam_RI) ? LAstore_ST_B : LAstore_STX_B;
+         break;
+      case Ity_I16:
+         op = (am->tag == LAam_RI) ? LAstore_ST_H : LAstore_STX_H;
+         break;
+      case Ity_I32:
+         op = (am->tag == LAam_RI) ? LAstore_ST_W : LAstore_STX_W;
+         break;
+      case Ity_I64:
+         op = (am->tag == LAam_RI) ? LAstore_ST_D : LAstore_STX_D;
+         break;
+      default:
+         ok = False;
+         break;
+   }
+   if (ok) {
+      HReg src = iselIntExpr_R(env, stmt->Ist.Store.data);
+      addInstr(env, LOONGARCH64Instr_Store(op, am, src));
+      return;
+   }
+
+   LOONGARCH64FpStoreOp fop;
+   ok = True;
+   switch (tyd) {
+      case Ity_F32:
+         fop = (am->tag == LAam_RI) ? LAfpstore_FST_S : LAfpstore_FSTX_S;
+         break;
+      case Ity_F64:
+         fop = (am->tag == LAam_RI) ? LAfpstore_FST_D : LAfpstore_FSTX_D;
+         break;
+      default:
+         ok = False;
+         break;
+   }
+   if (ok) {
+      HReg src = iselFltExpr(env, stmt->Ist.Store.data);
+      addInstr(env, LOONGARCH64Instr_FpStore(fop, am, src));
+      return;
+   }
+
+   if (tyd == Ity_V128) {
+      LOONGARCH64VecStoreOp vop = (am->tag == LAam_RI) ? LAvecstore_VST : LAvecstore_VSTX;
+      HReg src = iselV128Expr(env, stmt->Ist.Store.data);
+      addInstr(env, LOONGARCH64Instr_VecStore(vop, am, src));
+   } else if (tyd == Ity_V256) {
+      LOONGARCH64VecStoreOp vop = (am->tag == LAam_RI) ? LAvecstore_VST : LAvecstore_VSTX;
+      HReg addr = iselIntExpr_R(env, stmt->Ist.Store.addr);
+      LOONGARCH64AMode* am16 = LOONGARCH64AMode_RI(addr, 16);
+      HReg hi, lo;
+      iselV256Expr(&hi, &lo, env, stmt->Ist.Store.data);
+      addInstr(env, LOONGARCH64Instr_VecStore(vop, am, lo));
+      addInstr(env, LOONGARCH64Instr_VecStore(LAvecstore_VST, am16, hi));
+   } else {
+      vpanic("iselStmt(loongarch64): Ist_Store");
+   }
+}
+
+static void iselStmtPut ( ISelEnv* env, IRStmt* stmt )
+{
+   IRType ty = typeOfIRExpr(env->type_env, stmt->Ist.Put.data);
+
+   Bool              ri = stmt->Ist.Put.offset < 1024;
+   HReg              tmp;
+   LOONGARCH64AMode* am;
+
+   if (ri) {
+      am = LOONGARCH64AMode_RI(hregGSP(), stmt->Ist.Put.offset);
+   }  else {
+      tmp = newVRegI(env);
+      addInstr(env, LOONGARCH64Instr_LI(stmt->Ist.Put.offset, tmp));
+      am = LOONGARCH64AMode_RR(hregGSP(), tmp);
+   }
+
+   LOONGARCH64StoreOp op;
+   Bool ok = True;
+   switch (ty) {
+      case Ity_I8:
+         op = ri ? LAstore_ST_B : LAstore_STX_B;
+         break;
+      case Ity_I16:
+         op = ri ? LAstore_ST_H : LAstore_STX_H;
+         break;
+      case Ity_I32:
+         op = ri ? LAstore_ST_W : LAstore_STX_W;
+         break;
+      case Ity_I64:
+         op = ri ? LAstore_ST_D : LAstore_STX_D;
+         break;
+      default:
+         ok = False;
+         break;
+   }
+   if (ok) {
+      HReg src = iselIntExpr_R(env, stmt->Ist.Put.data);
+      addInstr(env, LOONGARCH64Instr_Store(op, am, src));
+      return;
+   }
+
+   LOONGARCH64FpStoreOp fop;
+   ok = True;
+   switch (ty) {
+      case Ity_F32:
+         fop = ri ? LAfpstore_FST_S : LAfpstore_FSTX_S;
+         break;
+      case Ity_F64:
+         fop = ri ? LAfpstore_FST_D : LAfpstore_FSTX_D;
+         break;
+      default:
+         ok = False;
+         break;
+   }
+   if (ok) {
+      HReg src = iselFltExpr(env, stmt->Ist.Put.data);
+      addInstr(env, LOONGARCH64Instr_FpStore(fop, am, src));
+      return;
+   }
+
+   if (ty == Ity_V128) {
+      LOONGARCH64VecStoreOp vop = ri ? LAvecstore_VST : LAvecstore_VSTX;
+      HReg src = iselV128Expr(env, stmt->Ist.Put.data);
+      addInstr(env, LOONGARCH64Instr_VecStore(vop, am, src));
+   } else if (ty == Ity_V256) {
+      LOONGARCH64VecStoreOp vop = ri ? LAvecstore_VST : LAvecstore_VSTX;
+      LOONGARCH64VecStoreOp vop2;
+      HReg hi, lo;
+      HReg tmp2;
+      LOONGARCH64AMode* am2;
+      if (stmt->Ist.Put.offset + 16 < 1024) {
+         vop2 = LAvecstore_VST;
+         am2 = LOONGARCH64AMode_RI(hregGSP(), stmt->Ist.Put.offset + 16);
+      } else {
+         vop2 = LAvecstore_VSTX;
+         tmp2 = newVRegI(env);
+         addInstr(env, LOONGARCH64Instr_LI(stmt->Ist.Put.offset + 16, tmp2));
+         am2 = LOONGARCH64AMode_RR(hregGSP(), tmp2);
+      }
+      iselV256Expr(&hi, &lo, env, stmt->Ist.Put.data);
+      addInstr(env, LOONGARCH64Instr_VecStore(vop, am, lo));
+      addInstr(env, LOONGARCH64Instr_VecStore(vop2, am2, hi));
+   } else {
+      vpanic("iselStmt(loongarch64): Ist_Put");
+   }
+}
+
+static void iselStmtTmp ( ISelEnv* env, IRStmt* stmt )
+{
+   IRTemp tmp = stmt->Ist.WrTmp.tmp;
+   IRType ty  = typeOfIRTemp(env->type_env, tmp);
+
+   switch (ty) {
+      case Ity_I8:
+      case Ity_I16:
+      case Ity_I32:
+      case Ity_I64: {
+         HReg dst = lookupIRTemp(env, tmp);
+         HReg src = iselIntExpr_R(env, stmt->Ist.WrTmp.data);
+         addInstr(env, LOONGARCH64Instr_Move(dst, src));
+         break;
+      }
+      case Ity_I1: {
+         HReg dst = lookupIRTemp(env, tmp);
+         HReg src = iselCondCode_R(env, stmt->Ist.WrTmp.data);
+         addInstr(env, LOONGARCH64Instr_Move(dst, src));
+         break;
+      }
+      case Ity_F32: {
+         HReg dst = lookupIRTemp(env, tmp);
+         HReg src = iselFltExpr(env, stmt->Ist.WrTmp.data);
+         addInstr(env, LOONGARCH64Instr_FpMove(LAfpmove_FMOV_S, src, dst));
+         break;
+      }
+      case Ity_F64: {
+         HReg dst = lookupIRTemp(env, tmp);
+         HReg src = iselFltExpr(env, stmt->Ist.WrTmp.data);
+         addInstr(env, LOONGARCH64Instr_FpMove(LAfpmove_FMOV_D, src, dst));
+         break;
+      }
+      case Ity_V128: {
+         HReg dst = lookupIRTemp(env, tmp);
+         HReg src = iselV128Expr(env, stmt->Ist.WrTmp.data);
+         addInstr(env, LOONGARCH64Instr_VecMove(dst, src));
+         break;
+      }
+      case Ity_V256: {
+         HReg hi, lo, dstHi, dstLo;
+         lookupIRTempPair(&dstHi, &dstLo, env, tmp);
+         iselV256Expr(&hi, &lo, env, stmt->Ist.WrTmp.data);
+         addInstr(env, LOONGARCH64Instr_VecMove(dstHi, hi));
+         addInstr(env, LOONGARCH64Instr_VecMove(dstLo, lo));
+         break;
+      }
+      default:
+         vpanic("iselStmt(loongarch64): Ist_WrTmp");
+         break;
+   }
+}
+
+static void iselStmtDirty ( ISelEnv* env, IRStmt* stmt )
+{
+   IRDirty* d = stmt->Ist.Dirty.details;
+
+   /* Figure out the return type, if any. */
+   IRType retty = Ity_INVALID;
+   if (d->tmp != IRTemp_INVALID)
+      retty = typeOfIRTemp(env->type_env, d->tmp);
+
+   Bool retty_ok = False;
+   switch (retty) {
+      case Ity_INVALID: /* function doesn't return anything */
+      case Ity_I8: case Ity_I16: case Ity_I32: case Ity_I64:
+      case Ity_V128: case Ity_V256:
+         retty_ok = True;
+         break;
+      default:
+         break;
+   }
+   if (!retty_ok)
+      vpanic("iselStmt(loongarch64): Ist_Dirty");
+
+   /* Marshal args, do the call, and set the return value to 0x555..555
+      if this is a conditional call that returns a value and the
+      call is skipped. */
+   UInt   addToSp = 0;
+   RetLoc rloc    = mk_RetLoc_INVALID();
+   doHelperCall(&addToSp, &rloc, env, d->guard, d->cee, retty, d->args);
+   vassert(is_sane_RetLoc(rloc));
+
+   /* Now figure out what to do with the returned value, if any. */
+   switch (retty) {
+      case Ity_INVALID: {
+         /* No return value.  Nothing to do. */
+         vassert(d->tmp == IRTemp_INVALID);
+         vassert(rloc.pri == RLPri_None);
+         vassert(addToSp == 0);
+         break;
+      }
+      case Ity_I8: case Ity_I16: case Ity_I32: case Ity_I64: {
+         vassert(rloc.pri == RLPri_Int);
+         vassert(addToSp == 0);
+         /* The returned value is in $a0.  Park it in the register
+            associated with tmp. */
+         HReg dst = lookupIRTemp(env, d->tmp);
+         addInstr(env, LOONGARCH64Instr_Move(dst, hregLOONGARCH64_R4()));
+         break;
+      }
+      case Ity_V128: {
+         /* The returned value is on the stack, and *retloc tells
+            us where.  Fish it off the stack and then move the
+            stack pointer upwards to clear it, as directed by
+            doHelperCall. */
+         vassert(rloc.pri == RLPri_V128SpRel);
+         vassert((rloc.spOff < 512) && (rloc.spOff > -512));
+         vassert(addToSp >= 16);
+         HReg dst = lookupIRTemp(env, d->tmp);
+         HReg tmp = newVRegI(env);
+         addInstr(env, LOONGARCH64Instr_Move(tmp, hregSP()));
+         addInstr(env, LOONGARCH64Instr_Binary(LAbin_ADDI_D,
+                                               LOONGARCH64RI_I(rloc.spOff, 12, True),
+                                               tmp, tmp));
+         addInstr(env, LOONGARCH64Instr_VecLoad(LAvecload_VLD,
+                                                LOONGARCH64AMode_RI(tmp, 0),
+                                                dst));
+         addInstr(env, LOONGARCH64Instr_Binary(LAbin_ADDI_D,
+                                               LOONGARCH64RI_I(addToSp, 12, True),
+                                               hregSP(), hregSP()));
+         break;
+      }
+      case Ity_V256: {
+         /* See comments for Ity_V128. */
+         vassert(rloc.pri == RLPri_V256SpRel);
+         vassert((rloc.spOff + 16 < 512) && (rloc.spOff > -512));
+         vassert(addToSp >= 32);
+         HReg dstLo, dstHi;
+         lookupIRTempPair(&dstHi, &dstLo, env, d->tmp);
+         HReg tmp = newVRegI(env);
+         addInstr(env, LOONGARCH64Instr_Move(tmp, hregSP()));
+         addInstr(env, LOONGARCH64Instr_Binary(LAbin_ADDI_D,
+                                               LOONGARCH64RI_I(rloc.spOff, 12, True),
+                                               tmp, tmp));
+         addInstr(env, LOONGARCH64Instr_VecLoad(LAvecload_VLD,
+                                                LOONGARCH64AMode_RI(tmp, 0),
+                                                dstLo));
+         addInstr(env, LOONGARCH64Instr_Binary(LAbin_ADDI_D,
+                                               LOONGARCH64RI_I(16, 12, True),
+                                               tmp, tmp));
+         addInstr(env, LOONGARCH64Instr_VecLoad(LAvecload_VLD,
+                                                LOONGARCH64AMode_RI(tmp, 0),
+                                                dstHi));
+         addInstr(env, LOONGARCH64Instr_Binary(LAbin_ADDI_D,
+                                               LOONGARCH64RI_I(addToSp, 12, True),
+                                               hregSP(), hregSP()));
+         break;
+      }
+      default:
+         /*NOTREACHED*/
+         vassert(0);
+         break;
+   }
+}
+
+static void iselStmtLLSC ( ISelEnv* env, IRStmt* stmt )
+{
+   IRTemp res = stmt->Ist.LLSC.result;
+   IRType tya = typeOfIRExpr(env->type_env, stmt->Ist.LLSC.addr);
+
+   /* Temporary solution; this need to be rewritten again for LOONGARCH64.
+      On LOONGARCH64 you can not read from address that is locked with LL
+      before SC. If you read from address that is locked than SC will fall.
+    */
+   if (stmt->Ist.LLSC.storedata == NULL) {
+      /* LL */
+      IRType ty = typeOfIRTemp(env->type_env, res);
+      LOONGARCH64LLSCOp op;
+      switch (ty) {
+         case Ity_I32:
+            op = LAllsc_LL_W;
+            break;
+         case Ity_I64:
+            op = LAllsc_LL_D;
+            break;
+         default:
+            vpanic("iselStmt(loongarch64): Ist_LLSC");
+            break;
+      }
+      LOONGARCH64AMode* addr = iselIntExpr_AMode(env, stmt->Ist.LLSC.addr, tya);
+      HReg               val = lookupIRTemp(env, res);
+      addInstr(env, LOONGARCH64Instr_LLSC(op, True, addr, val));
+   } else {
+      /* SC */
+      IRType tyd = typeOfIRExpr(env->type_env, stmt->Ist.LLSC.storedata);
+      LOONGARCH64LLSCOp op;
+      switch (tyd) {
+         case Ity_I32:
+            op = LAllsc_SC_W;
+            break;
+         case Ity_I64:
+            op = LAllsc_SC_D;
+            break;
+         default:
+            vpanic("iselStmt(loongarch64): Ist_LLSC");
+            break;
+      }
+      LOONGARCH64AMode* addr = iselIntExpr_AMode(env, stmt->Ist.LLSC.addr, tya);
+      HReg               val = iselIntExpr_R(env, stmt->Ist.LLSC.storedata);
+      HReg               dst = lookupIRTemp(env, res);
+      HReg               tmp = newVRegI(env);
+      addInstr(env, LOONGARCH64Instr_Move(tmp, val));
+      addInstr(env, LOONGARCH64Instr_LLSC(op, False, addr, tmp));
+      addInstr(env, LOONGARCH64Instr_Move(dst, tmp));
+   }
+}
+
+static void iselStmtCas ( ISelEnv* env, IRStmt* stmt )
+{
+   IRCAS* cas = stmt->Ist.CAS.details;
+   if (cas->oldHi == IRTemp_INVALID && cas->end == Iend_LE) {
+      /* "normal" singleton CAS */
+      HReg   old = lookupIRTemp(env, cas->oldLo);
+      HReg  addr = iselIntExpr_R(env, cas->addr);
+      HReg  expd = iselIntExpr_R(env, cas->expdLo);
+      HReg  data = iselIntExpr_R(env, cas->dataLo);
+      IRType  ty = typeOfIRTemp(env->type_env, cas->oldLo);
+      Bool size64;
+      switch (ty) {
+         case Ity_I32:
+            size64 = False;
+            break;
+         case Ity_I64:
+            size64 = True;
+            break;
+         default:
+            vpanic("iselStmt(loongarch64): Ist_CAS");
+            break;
+      }
+      addInstr(env, LOONGARCH64Instr_Cas(old, addr, expd, data, size64));
+   } else {
+      vpanic("iselStmt(loongarch64): Ist_CAS");
+   }
+}
+
+static void iselStmtMBE ( ISelEnv* env, IRStmt* stmt )
+{
+   switch (stmt->Ist.MBE.event) {
+      case Imbe_Fence:
+      case Imbe_CancelReservation:
+         addInstr(env, LOONGARCH64Instr_Bar(LAbar_DBAR, 0));
+         break;
+      case Imbe_InsnFence:
+         addInstr(env, LOONGARCH64Instr_Bar(LAbar_IBAR, 0));
+         break;
+      default:
+         vpanic("iselStmt(loongarch64): Ist_MBE");
+         break;
+   }
+}
+
+static void iselStmtExit ( ISelEnv* env, IRStmt* stmt )
+{
+   if (stmt->Ist.Exit.dst->tag != Ico_U64)
+      vpanic("iselStmt(loongarch64): Ist_Exit: dst is not a 64-bit value");
+
+   HReg            cond = iselCondCode_R(env, stmt->Ist.Exit.guard);
+   LOONGARCH64AMode* am = mkLOONGARCH64AMode_RI(hregGSP(), stmt->Ist.Exit.offsIP);
+
+   /* Case: boring transfer to known address */
+   if (stmt->Ist.Exit.jk == Ijk_Boring || stmt->Ist.Exit.jk == Ijk_Call) {
+      if (env->chainingAllowed) {
+         /* .. almost always true .. */
+         /* Skip the event check at the dst if this is a forwards edge. */
+         Bool toFastEP = ((Addr64)stmt->Ist.Exit.dst->Ico.U64) > env->max_ga;
+         addInstr(env, LOONGARCH64Instr_XDirect(stmt->Ist.Exit.dst->Ico.U64,
+                                                am, cond, toFastEP));
+      } else {
+         /* .. very occasionally .. */
+         /* We can't use chaining, so ask for an assisted transfer,
+            as that's the only alternative that is allowable. */
+         HReg dst = iselIntExpr_R(env, IRExpr_Const(stmt->Ist.Exit.dst));
+         addInstr(env, LOONGARCH64Instr_XAssisted(dst, am, cond, Ijk_Boring));
+      }
+      return;
+   }
+
+   /* Case: assisted transfer to arbitrary address */
+   switch (stmt->Ist.Exit.jk) {
+      /* Keep this list in sync with that for iselNext below */
+      case Ijk_ClientReq:
+      case Ijk_Yield:
+      case Ijk_NoDecode:
+      case Ijk_InvalICache:
+      case Ijk_NoRedir:
+      case Ijk_SigILL:
+      case Ijk_SigTRAP:
+      case Ijk_SigSEGV:
+      case Ijk_SigBUS:
+      case Ijk_SigFPE_IntDiv:
+      case Ijk_SigFPE_IntOvf:
+      case Ijk_SigSYS:
+      case Ijk_Sys_syscall: {
+         HReg dst = iselIntExpr_R(env, IRExpr_Const(stmt->Ist.Exit.dst));
+         addInstr(env, LOONGARCH64Instr_XAssisted(dst, am, cond, stmt->Ist.Exit.jk));
+         break;
+      }
+      default:
+         /* Do we ever expect to see any other kind? */
+         ppIRJumpKind(stmt->Ist.Exit.jk);
+         vpanic("iselStmt(loongarch64): Ist_Exit: unexpected jump kind");
+         break;
+   }
+}
+
+static void iselStmt(ISelEnv* env, IRStmt* stmt)
+{
+   if (vex_traceflags & VEX_TRACE_VCODE) {
+      vex_printf("\n-- ");
+      ppIRStmt(stmt);
+      vex_printf("\n");
+   }
+
+   switch (stmt->tag) {
+      /* --------- STORE --------- */
+      /* little-endian write to memory */
+      case Ist_Store:
+         iselStmtStore(env, stmt);
+         break;
+
+      /* --------- PUT --------- */
+      /* write guest state, fixed offset */
+      case Ist_Put:
+         iselStmtPut(env, stmt);
+         break;
+
+      /* --------- TMP --------- */
+      /* assign value to temporary */
+      case Ist_WrTmp:
+         iselStmtTmp(env, stmt);
+         break;
+
+      /* --------- Call to DIRTY helper --------- */
+      /* call complex ("dirty") helper function */
+      case Ist_Dirty:
+         iselStmtDirty(env, stmt);
+         break;
+
+      /* --------- Load Linked and Store Conditional --------- */
+      case Ist_LLSC:
+         iselStmtLLSC(env, stmt);
+         break;
+
+      /* --------- CAS --------- */
+      case Ist_CAS:
+         iselStmtCas(env, stmt);
+         break;
+
+      /* --------- MEM FENCE --------- */
+      case Ist_MBE:
+         iselStmtMBE(env, stmt);
+         break;
+
+      /* --------- INSTR MARK --------- */
+      /* Doesn't generate any executable code ... */
+      case Ist_IMark:
+         break;
+
+      /* --------- ABI HINT --------- */
+      /* These have no meaning (denotation in the IR) and so we ignore
+         them ... if any actually made it this far. */
+      case Ist_AbiHint:
+         break;
+
+      /* --------- NO-OP --------- */
+      case Ist_NoOp:
+         break;
+
+      /* --------- EXIT --------- */
+      case Ist_Exit:
+         iselStmtExit(env, stmt);
+         break;
+
+      default:
+         ppIRStmt(stmt);
+         vpanic("iselStmt(loongarch64)");
+         break;
+   }
+}
+
+
+/*---------------------------------------------------------*/
+/*--- ISEL: Basic block terminators (Nexts)             ---*/
+/*---------------------------------------------------------*/
+
+static void iselNext ( ISelEnv* env, IRExpr* next, IRJumpKind jk, Int offsIP )
+{
+   if (vex_traceflags & VEX_TRACE_VCODE) {
+      vex_printf("\n-- PUT(%d) = ", offsIP);
+      ppIRExpr(next);
+      vex_printf("; exit-");
+      ppIRJumpKind(jk);
+      vex_printf("\n");
+   }
+
+   /* Case: boring transfer to known address */
+   if (next->tag == Iex_Const) {
+      IRConst* cdst = next->Iex.Const.con;
+      vassert(cdst->tag == Ico_U64);
+      if (jk == Ijk_Boring || jk == Ijk_Call) {
+         /* Boring transfer to known address */
+         LOONGARCH64AMode* am = mkLOONGARCH64AMode_RI(hregGSP(), offsIP);
+         if (env->chainingAllowed) {
+            /* .. almost always true .. */
+            /* Skip the event check at the dst if this is a forwards edge. */
+            Bool toFastEP = ((Addr64)cdst->Ico.U64) > env->max_ga;
+            addInstr(env, LOONGARCH64Instr_XDirect(cdst->Ico.U64, am,
+                                                   INVALID_HREG, toFastEP));
+         } else {
+            /* .. very occasionally .. */
+            /* We can't use chaining, so ask for an assisted transfer,
+               as that's the only alternative that is allowable. */
+            HReg dst = iselIntExpr_R(env, next);
+            addInstr(env, LOONGARCH64Instr_XAssisted(dst, am, INVALID_HREG, Ijk_Boring));
+         }
+         return;
+      }
+   }
+
+   /* Case: call/return (==boring) transfer to any address */
+   switch (jk) {
+      case Ijk_Boring:
+      case Ijk_Ret:
+      case Ijk_Call:  {
+         HReg dst = iselIntExpr_R(env, next);
+         LOONGARCH64AMode* am = mkLOONGARCH64AMode_RI(hregGSP(), offsIP);
+         if (env->chainingAllowed) {
+            addInstr(env, LOONGARCH64Instr_XIndir(dst, am, INVALID_HREG));
+         } else {
+            addInstr(env, LOONGARCH64Instr_XAssisted(dst, am,
+                                                     INVALID_HREG, Ijk_Boring));
+         }
+         return;
+      }
+      default:
+         break;
+   }
+
+   /* Case: assisted transfer to arbitrary address */
+   switch (jk) {
+      /* Keep this list in sync with that for Ist_Exit above */
+      case Ijk_ClientReq:
+      case Ijk_Yield:
+      case Ijk_NoDecode:
+      case Ijk_InvalICache:
+      case Ijk_NoRedir:
+      case Ijk_SigILL:
+      case Ijk_SigTRAP:
+      case Ijk_SigSEGV:
+      case Ijk_SigBUS:
+      case Ijk_SigFPE_IntDiv:
+      case Ijk_SigFPE_IntOvf:
+      case Ijk_SigSYS:
+      case Ijk_Sys_syscall: {
+         HReg dst = iselIntExpr_R(env, next);
+         LOONGARCH64AMode* am = mkLOONGARCH64AMode_RI(hregGSP(), offsIP);
+         addInstr(env, LOONGARCH64Instr_XAssisted(dst, am, INVALID_HREG, jk));
+         return;
+      }
+      default:
+         break;
+   }
+
+   vex_printf("\n-- PUT(%d) = ", offsIP);
+   ppIRExpr(next);
+   vex_printf("; exit-");
+   ppIRJumpKind(jk);
+   vex_printf("\n");
+   vassert(0); // are we expecting any other kind?
+}
+
+
+/*---------------------------------------------------------*/
+/*--- Insn selector top-level                           ---*/
+/*---------------------------------------------------------*/
+
+/* Translate an entire BB to LOONGARCH64 code. */
+HInstrArray* iselSB_LOONGARCH64 ( const IRSB* bb,
+                                  VexArch arch_host,
+                                  const VexArchInfo* archinfo_host,
+                                  const VexAbiInfo* vbi,
+                                  Int offs_Host_EvC_Counter,
+                                  Int offs_Host_EvC_FailAddr,
+                                  Bool chainingAllowed,
+                                  Bool addProfInc,
+                                  Addr max_ga )
+{
+   Int        i, j;
+   HReg       hreg, hregHI;
+   ISelEnv*   env;
+   UInt       hwcaps_host = archinfo_host->hwcaps;
+   LOONGARCH64AMode *amCounter, *amFailAddr;
+
+   /* sanity ... */
+   vassert(arch_host == VexArchLOONGARCH64);
+   vassert((hwcaps_host & ~(VEX_HWCAPS_LOONGARCH_CPUCFG
+                          | VEX_HWCAPS_LOONGARCH_LAM
+                          | VEX_HWCAPS_LOONGARCH_UAL
+                          | VEX_HWCAPS_LOONGARCH_FP
+                          | VEX_HWCAPS_LOONGARCH_LSX
+                          | VEX_HWCAPS_LOONGARCH_LASX
+                          | VEX_HWCAPS_LOONGARCH_COMPLEX
+                          | VEX_HWCAPS_LOONGARCH_CRYPTO
+                          | VEX_HWCAPS_LOONGARCH_LVZP
+                          | VEX_HWCAPS_LOONGARCH_X86BT
+                          | VEX_HWCAPS_LOONGARCH_ARMBT
+                          | VEX_HWCAPS_LOONGARCH_MIPSBT
+                          | VEX_HWCAPS_LOONGARCH_ISA_32BIT
+                          | VEX_HWCAPS_LOONGARCH_ISA_64BIT)) == 0);
+
+   /* Check that the host's endianness is as expected. */
+   vassert(archinfo_host->endness == VexEndnessLE);
+
+   /* Make up an initial environment to use. */
+   env = LibVEX_Alloc_inline(sizeof(ISelEnv));
+   env->vreg_ctr = 0;
+
+   /* Set up output code array. */
+   env->code = newHInstrArray();
+
+   /* Copy BB's type env. */
+   env->type_env = bb->tyenv;
+
+   /* Make up an IRTemp -> virtual HReg mapping.  This doesn't
+      change as we go along. */
+   env->n_vregmap = bb->tyenv->types_used;
+   env->vregmap   = LibVEX_Alloc_inline(env->n_vregmap * sizeof(HReg));
+   env->vregmapHI = LibVEX_Alloc_inline(env->n_vregmap * sizeof(HReg));
+
+   /* and finally ... */
+   env->chainingAllowed = chainingAllowed;
+   env->hwcaps          = hwcaps_host;
+   env->max_ga          = max_ga;
+
+   /* For each IR temporary, allocate a suitably-kinded virtual register. */
+   j = 0;
+   for (i = 0; i < env->n_vregmap; i++) {
+      hregHI = hreg = INVALID_HREG;
+      switch (bb->tyenv->types[i]) {
+         case Ity_I1:
+         case Ity_I8:
+         case Ity_I16:
+         case Ity_I32:
+         case Ity_I64:
+            hreg = mkHReg(True, HRcInt64, 0, j++);
+            break;
+         case Ity_I128:
+            hreg   = mkHReg(True, HRcInt64, 0, j++);
+            hregHI = mkHReg(True, HRcInt64, 0, j++);
+            break;
+         case Ity_F16: // we'll use HRcFlt64 regs for F16 too
+         case Ity_F32: // we'll use HRcFlt64 regs for F32 too
+         case Ity_F64:
+            hreg = mkHReg(True, HRcFlt64, 0, j++);
+            break;
+         case Ity_V128:
+            hreg = mkHReg(True, HRcVec128, 0, j++);
+            break;
+         case Ity_V256:
+            hreg   = mkHReg(True, HRcVec128, 0, j++);
+            hregHI = mkHReg(True, HRcVec128, 0, j++);
+            break;
+         default:
+            ppIRType(bb->tyenv->types[i]);
+            vpanic("iselBB(loongarch64): IRTemp type");
+            break;
+      }
+      env->vregmap[i]   = hreg;
+      env->vregmapHI[i] = hregHI;
+   }
+   env->vreg_ctr = j;
+
+   /* The very first instruction must be an event check. */
+   amCounter  = mkLOONGARCH64AMode_RI(hregGSP(), offs_Host_EvC_Counter);
+   amFailAddr = mkLOONGARCH64AMode_RI(hregGSP(), offs_Host_EvC_FailAddr);
+   addInstr(env, LOONGARCH64Instr_EvCheck(amCounter, amFailAddr));
+
+   /* Possibly a block counter increment (for profiling).  At this
+      point we don't know the address of the counter, so just pretend
+      it is zero.  It will have to be patched later, but before this
+      translation is used, by a call to LibVEX_patchProfCtr. */
+   if (addProfInc) {
+      addInstr(env, LOONGARCH64Instr_ProfInc());
+   }
+
+   /* Ok, finally we can iterate over the statements. */
+   for (i = 0; i < bb->stmts_used; i++)
+      iselStmt(env, bb->stmts[i]);
+
+   iselNext(env, bb->next, bb->jumpkind, bb->offsIP);
+
+   /* record the number of vregs we used. */
+   env->code->n_vregs = env->vreg_ctr;
+   return env->code;
+}
+
+
+/*---------------------------------------------------------------*/
+/*--- end                             host_loongarch64_isel.c ---*/
+/*---------------------------------------------------------------*/
diff --git a/VEX/priv/ir_defs.c b/VEX/priv/ir_defs.c
index 9e7fbf9..8729f2f 100644
--- a/VEX/priv/ir_defs.c
+++ b/VEX/priv/ir_defs.c
@@ -280,6 +280,8 @@ void ppIROp ( IROp op )
       case Iop_SubF64:    vex_printf("SubF64"); return;
       case Iop_MulF64:    vex_printf("MulF64"); return;
       case Iop_DivF64:    vex_printf("DivF64"); return;
+      case Iop_ScaleBF64: vex_printf("ScaleBF64"); return;
+      case Iop_ScaleBF32: vex_printf("ScaleBF32"); return;
       case Iop_AddF64r32: vex_printf("AddF64r32"); return;
       case Iop_SubF64r32: vex_printf("SubF64r32"); return;
       case Iop_MulF64r32: vex_printf("MulF64r32"); return;
@@ -356,6 +358,10 @@ void ppIROp ( IROp op )
       case Iop_SqrtF64:       vex_printf("SqrtF64"); return;
       case Iop_SqrtF32:       vex_printf("SqrtF32"); return;
       case Iop_SqrtF16:       vex_printf("SqrtF16"); return;
+      case Iop_RSqrtF32:      vex_printf("RSqrtF32"); return;
+      case Iop_RSqrtF64:      vex_printf("RSqrtF64"); return;
+      case Iop_LogBF32:       vex_printf("LogBF32"); return;
+      case Iop_LogBF64:       vex_printf("LogBF64"); return;
       case Iop_SinF64:    vex_printf("SinF64"); return;
       case Iop_CosF64:    vex_printf("CosF64"); return;
       case Iop_TanF64:    vex_printf("TanF64"); return;
@@ -379,8 +385,12 @@ void ppIROp ( IROp op )
 
       case Iop_MaxNumF64: vex_printf("MaxNumF64"); return;
       case Iop_MinNumF64: vex_printf("MinNumF64"); return;
+      case Iop_MaxNumAbsF64: vex_printf("MaxNumAbsF64"); return;
+      case Iop_MinNumAbsF64: vex_printf("MinNumAbsF64"); return;
       case Iop_MaxNumF32: vex_printf("MaxNumF32"); return;
       case Iop_MinNumF32: vex_printf("MinNumF32"); return;
+      case Iop_MaxNumAbsF32: vex_printf("MaxNumAbsF32"); return;
+      case Iop_MinNumAbsF32: vex_printf("MinNumAbsF32"); return;
 
       case Iop_F16toF64: vex_printf("F16toF64"); return;
       case Iop_F64toF16: vex_printf("F64toF16"); return;
@@ -1311,16 +1321,20 @@ void ppIROp ( IROp op )
       case Iop_Max8Sx32:  vex_printf("Max8Sx32"); return;
       case Iop_Max16Sx16: vex_printf("Max16Sx16"); return;
       case Iop_Max32Sx8:  vex_printf("Max32Sx8"); return;
+      case Iop_Max64Sx4:  vex_printf("Max64Sx4"); return;
       case Iop_Max8Ux32:  vex_printf("Max8Ux32"); return;
       case Iop_Max16Ux16: vex_printf("Max16Ux16"); return;
       case Iop_Max32Ux8:  vex_printf("Max32Ux8"); return;
+      case Iop_Max64Ux4:  vex_printf("Max64Ux4"); return;
 
       case Iop_Min8Sx32:  vex_printf("Min8Sx32"); return;
       case Iop_Min16Sx16: vex_printf("Min16Sx16"); return;
       case Iop_Min32Sx8:  vex_printf("Min32Sx8"); return;
+      case Iop_Min64Sx4:  vex_printf("Min64Sx4"); return;
       case Iop_Min8Ux32:  vex_printf("Min8Ux32"); return;
       case Iop_Min16Ux16: vex_printf("Min16Ux16"); return;
       case Iop_Min32Ux8:  vex_printf("Min32Ux8"); return;
+      case Iop_Min64Ux4:  vex_printf("Min64Ux4"); return;
 
       case Iop_CmpEQ8x32:   vex_printf("CmpEQ8x32"); return;
       case Iop_CmpEQ16x16:  vex_printf("CmpEQ16x16"); return;
@@ -1438,10 +1452,13 @@ Bool primopMightTrap ( IROp op )
    case Iop_1Uto8: case Iop_1Uto32: case Iop_1Uto64: case Iop_1Sto8:
    case Iop_1Sto16: case Iop_1Sto32: case Iop_1Sto64:
    case Iop_AddF64: case Iop_SubF64: case Iop_MulF64: case Iop_DivF64:
+   case Iop_ScaleBF64: case Iop_ScaleBF32:
    case Iop_AddF32: case Iop_SubF32: case Iop_MulF32: case Iop_DivF32:
    case Iop_AddF64r32: case Iop_SubF64r32: case Iop_MulF64r32:
    case Iop_DivF64r32: case Iop_NegF64: case Iop_AbsF64:
    case Iop_NegF32: case Iop_AbsF32: case Iop_SqrtF64: case Iop_SqrtF32:
+   case Iop_RSqrtF64: case Iop_RSqrtF32:
+   case Iop_LogBF64: case Iop_LogBF32:
    case Iop_NegF16: case Iop_AbsF16: case Iop_SqrtF16: case Iop_SubF16:
    case Iop_AddF16:
    case Iop_CmpF64: case Iop_CmpF32: case Iop_CmpF16: case Iop_CmpF128:
@@ -1483,8 +1500,11 @@ Bool primopMightTrap ( IROp op )
    case Iop_RSqrtEst5GoodF64: case Iop_RoundF64toF64_NEAREST:
    case Iop_RoundF64toF64_NegINF: case Iop_RoundF64toF64_PosINF:
    case Iop_RoundF64toF64_ZERO: case Iop_TruncF64asF32: case Iop_RoundF64toF32:
-   case Iop_RecpExpF64: case Iop_RecpExpF32: case Iop_MaxNumF64:
-   case Iop_MinNumF64: case Iop_MaxNumF32: case Iop_MinNumF32:
+   case Iop_RecpExpF64: case Iop_RecpExpF32:
+   case Iop_MaxNumF64: case Iop_MinNumF64:
+   case Iop_MaxNumAbsF64: case Iop_MinNumAbsF64:
+   case Iop_MaxNumF32: case Iop_MinNumF32:
+   case Iop_MaxNumAbsF32: case Iop_MinNumAbsF32:
    case Iop_F16toF64: case Iop_F64toF16: case Iop_F16toF32:
    case Iop_F32toF16: case Iop_QAdd32S: case Iop_QSub32S:
    case Iop_Add16x2: case Iop_Sub16x2:
@@ -1792,10 +1812,10 @@ Bool primopMightTrap ( IROp op )
    case Iop_ShlN16x16: case Iop_ShlN32x8: case Iop_ShlN64x4:
    case Iop_ShrN16x16: case Iop_ShrN32x8: case Iop_ShrN64x4:
    case Iop_SarN16x16: case Iop_SarN32x8:
-   case Iop_Max8Sx32: case Iop_Max16Sx16: case Iop_Max32Sx8:
-   case Iop_Max8Ux32: case Iop_Max16Ux16: case Iop_Max32Ux8:
-   case Iop_Min8Sx32: case Iop_Min16Sx16: case Iop_Min32Sx8:
-   case Iop_Min8Ux32: case Iop_Min16Ux16: case Iop_Min32Ux8:
+   case Iop_Max8Sx32: case Iop_Max16Sx16: case Iop_Max32Sx8: case Iop_Max64Sx4:
+   case Iop_Max8Ux32: case Iop_Max16Ux16: case Iop_Max32Ux8: case Iop_Max64Ux4:
+   case Iop_Min8Sx32: case Iop_Min16Sx16: case Iop_Min32Sx8: case Iop_Min64Sx4:
+   case Iop_Min8Ux32: case Iop_Min16Ux16: case Iop_Min32Ux8: case Iop_Min64Ux4:
    case Iop_Mul16x16: case Iop_Mul32x8:
    case Iop_MulHi16Ux16: case Iop_MulHi16Sx16:
    case Iop_QAdd8Ux32: case Iop_QAdd16Ux16:
@@ -2081,6 +2101,7 @@ void ppIRJumpKind ( IRJumpKind kind )
       case Ijk_SigFPE:        vex_printf("SigFPE"); break;
       case Ijk_SigFPE_IntDiv: vex_printf("SigFPE_IntDiv"); break;
       case Ijk_SigFPE_IntOvf: vex_printf("SigFPE_IntOvf"); break;
+      case Ijk_SigSYS:        vex_printf("SigSYS"); break;
       case Ijk_Sys_syscall:   vex_printf("Sys_syscall"); break;
       case Ijk_Sys_int32:     vex_printf("Sys_int32"); break;
       case Ijk_Sys_int128:    vex_printf("Sys_int128"); break;
@@ -2101,6 +2122,8 @@ void ppIRMBusEvent ( IRMBusEvent event )
          vex_printf("Fence"); break;
       case Imbe_CancelReservation:
          vex_printf("CancelReservation"); break;
+      case Imbe_InsnFence:
+         vex_printf("InsnFence"); break;
       default:
          vpanic("ppIRMBusEvent");
    }
@@ -3379,12 +3402,14 @@ void typeOfPrimop ( IROp op,
 
       case Iop_AddF64:    case Iop_SubF64: 
       case Iop_MulF64:    case Iop_DivF64:
+      case Iop_ScaleBF64:
       case Iop_AddF64r32: case Iop_SubF64r32: 
       case Iop_MulF64r32: case Iop_DivF64r32:
          TERNARY(ity_RMode,Ity_F64,Ity_F64, Ity_F64);
 
       case Iop_AddF32: case Iop_SubF32:
       case Iop_MulF32: case Iop_DivF32:
+      case Iop_ScaleBF32:
          TERNARY(ity_RMode,Ity_F32,Ity_F32, Ity_F32);
 
       case Iop_AddF16:
@@ -3401,10 +3426,14 @@ void typeOfPrimop ( IROp op,
          UNARY(Ity_F16, Ity_F16);
 
       case Iop_SqrtF64:
+      case Iop_RSqrtF64:
+      case Iop_LogBF64:
       case Iop_RecpExpF64:
          BINARY(ity_RMode,Ity_F64, Ity_F64);
 
       case Iop_SqrtF32:
+      case Iop_RSqrtF32:
+      case Iop_LogBF32:
       case Iop_RoundF32toInt:
       case Iop_RecpExpF32:
          BINARY(ity_RMode,Ity_F32, Ity_F32);
@@ -3417,9 +3446,11 @@ void typeOfPrimop ( IROp op,
          BINARY(ity_RMode, Ity_F16, Ity_F16);
 
       case Iop_MaxNumF64: case Iop_MinNumF64:
+      case Iop_MaxNumAbsF64: case Iop_MinNumAbsF64:
          BINARY(Ity_F64,Ity_F64, Ity_F64);
 
       case Iop_MaxNumF32: case Iop_MinNumF32:
+      case Iop_MaxNumAbsF32: case Iop_MinNumAbsF32:
          BINARY(Ity_F32,Ity_F32, Ity_F32);
 
      case Iop_CmpF16:
@@ -4112,10 +4143,10 @@ void typeOfPrimop ( IROp op,
       case Iop_Mul16x16: case Iop_Mul32x8:
       case Iop_MulHi16Ux16: case Iop_MulHi16Sx16:
       case Iop_Avg8Ux32: case Iop_Avg16Ux16:
-      case Iop_Max8Sx32: case Iop_Max16Sx16: case Iop_Max32Sx8:
-      case Iop_Max8Ux32: case Iop_Max16Ux16: case Iop_Max32Ux8:
-      case Iop_Min8Sx32: case Iop_Min16Sx16: case Iop_Min32Sx8:
-      case Iop_Min8Ux32: case Iop_Min16Ux16: case Iop_Min32Ux8:
+      case Iop_Max8Sx32: case Iop_Max16Sx16: case Iop_Max32Sx8: case Iop_Max64Sx4:
+      case Iop_Max8Ux32: case Iop_Max16Ux16: case Iop_Max32Ux8: case Iop_Max64Ux4:
+      case Iop_Min8Sx32: case Iop_Min16Sx16: case Iop_Min32Sx8: case Iop_Min64Sx4:
+      case Iop_Min8Ux32: case Iop_Min16Ux16: case Iop_Min32Ux8: case Iop_Min64Ux4:
       case Iop_CmpEQ8x32:  case Iop_CmpEQ16x16:
       case Iop_CmpEQ32x8:  case Iop_CmpEQ64x4:
       case Iop_CmpGT8Sx32: case Iop_CmpGT16Sx16:
@@ -5259,7 +5290,9 @@ void tcStmt ( const IRSB* bb, const IRStmt* stmt, IRType gWordTy )
          break;
       case Ist_MBE:
          switch (stmt->Ist.MBE.event) {
-            case Imbe_Fence: case Imbe_CancelReservation:
+            case Imbe_Fence:
+            case Imbe_CancelReservation:
+            case Imbe_InsnFence:
                break;
             default: sanityCheckFail(bb,stmt,"IRStmt.MBE.event: unknown");
                break;
diff --git a/VEX/priv/main_main.c b/VEX/priv/main_main.c
index e6c788d..23988fb 100644
--- a/VEX/priv/main_main.c
+++ b/VEX/priv/main_main.c
@@ -43,6 +43,7 @@
 #include "libvex_guest_s390x.h"
 #include "libvex_guest_mips32.h"
 #include "libvex_guest_mips64.h"
+#include "libvex_guest_loongarch64.h"
 
 #include "main_globals.h"
 #include "main_util.h"
@@ -57,6 +58,7 @@
 #include "host_s390_defs.h"
 #include "host_mips_defs.h"
 #include "host_nanomips_defs.h"
+#include "host_loongarch64_defs.h"
 
 #include "guest_generic_bb_to_IR.h"
 #include "guest_x86_defs.h"
@@ -67,6 +69,7 @@
 #include "guest_s390_defs.h"
 #include "guest_mips_defs.h"
 #include "guest_nanomips_defs.h"
+#include "guest_loongarch64_defs.h"
 
 #include "host_generic_simd128.h"
 
@@ -163,6 +166,14 @@
 #define NANOMIPSST(f) vassert(0)
 #endif
 
+#if defined(VGA_loongarch64) || defined(VEXMULTIARCH)
+#define LOONGARCH64FN(f) f
+#define LOONGARCH64ST(f) f
+#else
+#define LOONGARCH64FN(f) NULL
+#define LOONGARCH64ST(f) vassert(0)
+#endif
+
 /* This file contains the top level interface to the library. */
 
 /* --------- fwds ... --------- */
@@ -541,6 +552,23 @@ IRSB* LibVEX_FrontEnd ( /*MOD*/ VexTranslateArgs* vta,
          vassert(sizeof( ((VexGuestMIPS32State*)0)->guest_NRADDR ) == 4);
          break;
 
+      case VexArchLOONGARCH64:
+         preciseMemExnsFn
+            = LOONGARCH64FN(guest_loongarch64_state_requires_precise_mem_exns);
+         disInstrFn              = LOONGARCH64FN(disInstr_LOONGARCH64);
+         specHelper              = LOONGARCH64FN(guest_loongarch64_spechelper);
+         guest_layout            = LOONGARCH64FN(&loongarch64Guest_layout);
+         offB_CMSTART            = offsetof(VexGuestLOONGARCH64State, guest_CMSTART);
+         offB_CMLEN              = offsetof(VexGuestLOONGARCH64State, guest_CMLEN);
+         offB_GUEST_IP           = offsetof(VexGuestLOONGARCH64State, guest_PC);
+         szB_GUEST_IP            = sizeof( ((VexGuestLOONGARCH64State*)0)->guest_PC );
+         vassert(vta->archinfo_guest.endness == VexEndnessLE);
+         vassert(sizeof(VexGuestLOONGARCH64State) % LibVEX_GUEST_STATE_ALIGN == 0);
+         vassert(sizeof( ((VexGuestLOONGARCH64State*)0)->guest_CMSTART) == 8);
+         vassert(sizeof( ((VexGuestLOONGARCH64State*)0)->guest_CMLEN  ) == 8);
+         vassert(sizeof( ((VexGuestLOONGARCH64State*)0)->guest_NRADDR ) == 8);
+         break;
+
       default:
          vpanic("LibVEX_Translate: unsupported guest insn set");
    }
@@ -878,6 +906,14 @@ static void libvex_BackEnd ( const VexTranslateArgs *vta,
          offB_HOST_EvC_FAILADDR = offsetof(VexGuestMIPS32State,host_EvC_FAILADDR);
          break;
 
+      case VexArchLOONGARCH64:
+         preciseMemExnsFn
+            = LOONGARCH64FN(guest_loongarch64_state_requires_precise_mem_exns);
+         guest_sizeB            = sizeof(VexGuestLOONGARCH64State);
+         offB_HOST_EvC_COUNTER  = offsetof(VexGuestLOONGARCH64State, host_EvC_COUNTER);
+         offB_HOST_EvC_FAILADDR = offsetof(VexGuestLOONGARCH64State, host_EvC_FAILADDR);
+         break;
+
       default:
          vpanic("LibVEX_Codegen: unsupported guest insn set");
    }
@@ -1052,6 +1088,23 @@ static void libvex_BackEnd ( const VexTranslateArgs *vta,
                  || vta->archinfo_host.endness == VexEndnessBE);
          break;
 
+      case VexArchLOONGARCH64:
+         mode64       = True;
+         rRegUniv     = LOONGARCH64FN(getRRegUniverse_LOONGARCH64());
+         getRegUsage
+            = CAST_TO_TYPEOF(getRegUsage) LOONGARCH64FN(getRegUsage_LOONGARCH64Instr);
+         mapRegs      = CAST_TO_TYPEOF(mapRegs) LOONGARCH64FN(mapRegs_LOONGARCH64Instr);
+         genSpill     = CAST_TO_TYPEOF(genSpill) LOONGARCH64FN(genSpill_LOONGARCH64);
+         genReload    = CAST_TO_TYPEOF(genReload) LOONGARCH64FN(genReload_LOONGARCH64);
+         genMove      = CAST_TO_TYPEOF(genMove) LOONGARCH64FN(genMove_LOONGARCH64);
+         ppInstr      = CAST_TO_TYPEOF(ppInstr) LOONGARCH64FN(ppLOONGARCH64Instr);
+         ppReg        = CAST_TO_TYPEOF(ppReg) LOONGARCH64FN(ppHRegLOONGARCH64);
+         iselSB       = LOONGARCH64FN(iselSB_LOONGARCH64);
+         emit         = CAST_TO_TYPEOF(emit) LOONGARCH64FN(emit_LOONGARCH64Instr);
+         vassert(vta->archinfo_host.endness == VexEndnessLE
+                 || vta->archinfo_host.endness == VexEndnessBE);
+         break;
+
       default:
          vpanic("LibVEX_Translate: unsupported host insn set");
    }
@@ -1297,6 +1350,11 @@ VexInvalRange LibVEX_Chain ( VexArch     arch_host,
                                                  place_to_chain,
                                                  disp_cp_chain_me_EXPECTED,
                                                  place_to_jump_to));
+      case VexArchLOONGARCH64:
+         LOONGARCH64ST(return chainXDirect_LOONGARCH64(endness_host,
+                                                       place_to_chain,
+                                                       disp_cp_chain_me_EXPECTED,
+                                                       place_to_jump_to));
       default:
          vassert(0);
    }
@@ -1359,6 +1417,11 @@ VexInvalRange LibVEX_UnChain ( VexArch     arch_host,
                                                  place_to_unchain,
                                                  place_to_jump_to_EXPECTED,
                                                  disp_cp_chain_me));
+      case VexArchLOONGARCH64:
+         LOONGARCH64ST(return unchainXDirect_LOONGARCH64(endness_host,
+                                                         place_to_unchain,
+                                                         place_to_jump_to_EXPECTED,
+                                                         disp_cp_chain_me));
       default:
          vassert(0);
    }
@@ -1389,6 +1452,8 @@ Int LibVEX_evCheckSzB ( VexArch    arch_host )
             MIPS64ST(cached = evCheckSzB_MIPS()); break;
         case VexArchNANOMIPS:
             NANOMIPSST(cached = evCheckSzB_NANOMIPS()); break;
+         case VexArchLOONGARCH64:
+            LOONGARCH64ST(cached = evCheckSzB_LOONGARCH64()); break;
          default:
             vassert(0);
       }
@@ -1432,6 +1497,10 @@ VexInvalRange LibVEX_PatchProfInc ( VexArch    arch_host,
       case VexArchNANOMIPS:
          NANOMIPSST(return patchProfInc_NANOMIPS(endness_host, place_to_patch,
                                                  location_of_counter));
+      case VexArchLOONGARCH64:
+         LOONGARCH64ST(return patchProfInc_LOONGARCH64(endness_host,
+                                                       place_to_patch,
+                                                       location_of_counter));
       default:
          vassert(0);
    }
@@ -1515,6 +1584,7 @@ const HChar* LibVEX_ppVexArch ( VexArch arch )
       case VexArchMIPS32:   return "MIPS32";
       case VexArchMIPS64:   return "MIPS64";
       case VexArchNANOMIPS: return "NANOMIPS";
+      case VexArchLOONGARCH64: return "LOONGARCH64";
       default:              return "VexArch???";
    }
 }
@@ -1586,6 +1656,7 @@ static IRType arch_word_size (VexArch arch) {
       case VexArchMIPS64:
       case VexArchPPC64:
       case VexArchS390X:
+      case VexArchLOONGARCH64:
          return Ity_I64;
 
       default:
@@ -1929,6 +2000,38 @@ static const HChar* show_hwcaps_mips64 ( UInt hwcaps )
    return "Unsupported baseline";
 }
 
+static const HChar* show_hwcaps_loongarch64 ( UInt hwcaps )
+{
+   static const HChar prefix[] = "loongarch64";
+   static const struct {
+      UInt  hwcaps_bit;
+      HChar name[16];
+   } hwcaps_list[] = {
+      { VEX_HWCAPS_LOONGARCH_CPUCFG,  "cpucfg"   },
+      { VEX_HWCAPS_LOONGARCH_LAM,     "lam"      },
+      { VEX_HWCAPS_LOONGARCH_UAL,     "ual"      },
+      { VEX_HWCAPS_LOONGARCH_FP,      "fpu"      },
+      { VEX_HWCAPS_LOONGARCH_LSX,     "lsx"      },
+      { VEX_HWCAPS_LOONGARCH_LASX,    "lasx"     },
+      { VEX_HWCAPS_LOONGARCH_COMPLEX, "complex"  },
+      { VEX_HWCAPS_LOONGARCH_CRYPTO,  "crypto"   },
+      { VEX_HWCAPS_LOONGARCH_LVZP,    "lvz"      },
+      { VEX_HWCAPS_LOONGARCH_X86BT,   "lbt_x86"  },
+      { VEX_HWCAPS_LOONGARCH_ARMBT,   "lbt_arm"  },
+      { VEX_HWCAPS_LOONGARCH_MIPSBT,  "lbt_mips" }
+   };
+   static HChar buf[sizeof(prefix) +
+                    NUM_HWCAPS * (sizeof hwcaps_list[0].name + 1) + 1]; // '\0'
+
+   HChar *p = buf + vex_sprintf(buf, "%s", prefix);
+   UInt i;
+   for (i = 0 ; i < NUM_HWCAPS; ++i) {
+      if (hwcaps & hwcaps_list[i].hwcaps_bit)
+         p = p + vex_sprintf(p, "-%s", hwcaps_list[i].name);
+   }
+   return buf;
+}
+
 #undef NUM_HWCAPS
 
 /* Thie function must not return NULL. */
@@ -1945,6 +2048,7 @@ static const HChar* show_hwcaps ( VexArch arch, UInt hwcaps )
       case VexArchS390X:  return show_hwcaps_s390x(hwcaps);
       case VexArchMIPS32: return show_hwcaps_mips32(hwcaps);
       case VexArchMIPS64: return show_hwcaps_mips64(hwcaps);
+      case VexArchLOONGARCH64: return show_hwcaps_loongarch64(hwcaps);
       default: return NULL;
    }
 }
@@ -2207,6 +2311,11 @@ static void check_hwcaps ( VexArch arch, UInt hwcaps )
             return;
          invalid_hwcaps(arch, hwcaps, "Unsupported baseline\n");
 
+      case VexArchLOONGARCH64:
+         if (!(hwcaps & VEX_HWCAPS_LOONGARCH_ISA_64BIT))
+            invalid_hwcaps(arch, hwcaps, "Unsupported baseline\n");
+         return;
+
       default:
          vpanic("unknown architecture");
    }
diff --git a/VEX/pub/libvex.h b/VEX/pub/libvex.h
index 1681077..9400adf 100644
--- a/VEX/pub/libvex.h
+++ b/VEX/pub/libvex.h
@@ -60,6 +60,7 @@ typedef
       VexArchMIPS32,
       VexArchMIPS64,
       VexArchNANOMIPS,
+      VexArchLOONGARCH64,
    }
    VexArch;
 
@@ -306,6 +307,22 @@ typedef
                               (VEX_MIPS_PROC_ID(x) == VEX_PRID_IMP_P5600) && \
                               (VEX_MIPS_HOST_FP_MODE(x)))
 
+/* LoongArch baseline capability */
+#define VEX_HWCAPS_LOONGARCH_CPUCFG    (1 << 0)   /* CPU has CPUCFG */
+#define VEX_HWCAPS_LOONGARCH_LAM       (1 << 1)   /* CPU has Atomic instructions */
+#define VEX_HWCAPS_LOONGARCH_UAL       (1 << 2)   /* CPU has Unaligned Access support */
+#define VEX_HWCAPS_LOONGARCH_FP        (1 << 3)   /* CPU has FPU */
+#define VEX_HWCAPS_LOONGARCH_LSX       (1 << 4)   /* CPU has 128-bit SIMD instructions */
+#define VEX_HWCAPS_LOONGARCH_LASX      (1 << 5)   /* CPU has 256-bit SIMD instructions */
+#define VEX_HWCAPS_LOONGARCH_COMPLEX   (1 << 6)   /* CPU has Complex instructions */
+#define VEX_HWCAPS_LOONGARCH_CRYPTO    (1 << 7)   /* CPU has Crypto instructions */
+#define VEX_HWCAPS_LOONGARCH_LVZP      (1 << 8)   /* CPU has Virtualization extension */
+#define VEX_HWCAPS_LOONGARCH_X86BT     (1 << 9)   /* CPU has X86 Binary Translation */
+#define VEX_HWCAPS_LOONGARCH_ARMBT     (1 << 10)  /* CPU has ARM Binary Translation */
+#define VEX_HWCAPS_LOONGARCH_MIPSBT    (1 << 11)  /* CPU has MIPS Binary Translation */
+#define VEX_HWCAPS_LOONGARCH_ISA_32BIT (1 << 30)  /* 32-bit ISA */
+#define VEX_HWCAPS_LOONGARCH_ISA_64BIT (1 << 31)  /* 64-bit ISA */
+
 /* These return statically allocated strings. */
 
 extern const HChar* LibVEX_ppVexArch    ( VexArch );
@@ -1033,6 +1050,10 @@ extern void LibVEX_InitIRI ( const IRICB * );
    ~~~~~
    r21 is GSP.
 
+   loongarch64
+   ~~~~~
+   r31 is GSP.
+
    ALL GUEST ARCHITECTURES
    ~~~~~~~~~~~~~~~~~~~~~~~
    The guest state must contain two pseudo-registers, guest_CMSTART
diff --git a/VEX/pub/libvex_basictypes.h b/VEX/pub/libvex_basictypes.h
index e3f1485..b4c81bf 100644
--- a/VEX/pub/libvex_basictypes.h
+++ b/VEX/pub/libvex_basictypes.h
@@ -198,6 +198,10 @@ typedef  unsigned long HWord;
 #   define VEX_HOST_WORDSIZE 4
 #   define VEX_REGPARM(_n) /* */
 
+#elif defined(__loongarch__) && (__loongarch_grlen == 64)
+#   define VEX_HOST_WORDSIZE 8
+#   define VEX_REGPARM(_n) /* */
+
 #else
 #   error "Vex: Fatal: Can't establish the host architecture"
 #endif
diff --git a/VEX/pub/libvex_guest_loongarch64.h b/VEX/pub/libvex_guest_loongarch64.h
new file mode 100644
index 0000000..fde111f
--- /dev/null
+++ b/VEX/pub/libvex_guest_loongarch64.h
@@ -0,0 +1,172 @@
+
+/*---------------------------------------------------------------*/
+/*--- begin                        libvex_guest_loongarch64.h ---*/
+/*---------------------------------------------------------------*/
+
+/*
+   This file is part of Valgrind, a dynamic binary instrumentation
+   framework.
+
+   Copyright (C) 2021-2022 Loongson Technology Corporation Limited
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, see <http://www.gnu.org/licenses/>.
+
+   The GNU General Public License is contained in the file COPYING.
+
+   Neither the names of the U.S. Department of Energy nor the
+   University of California nor the names of its contributors may be
+   used to endorse or promote products derived from this software
+   without prior written permission.
+*/
+
+#ifndef __LIBVEX_PUB_GUEST_LOONGARCH64_H
+#define __LIBVEX_PUB_GUEST_LOONGARCH64_H
+
+#include "libvex_basictypes.h"
+
+
+/*---------------------------------------------------------------*/
+/*--- Vex's representation of the LOONGARCH64 CPU state.      ---*/
+/*---------------------------------------------------------------*/
+
+typedef
+   struct {
+      /* Event check fail addr and counter. */
+      /*    0 */ ULong host_EvC_FAILADDR;
+      /*    8 */ UInt  host_EvC_COUNTER;
+      /*   12 */ UInt  _padding0;
+
+      /* CPU Registers */
+      /*   16 */ ULong guest_R0;   /* Constant zero */
+      /*   24 */ ULong guest_R1;   /* Return address */
+      /*   32 */ ULong guest_R2;   /* Thread pointer */
+      /*   40 */ ULong guest_R3;   /* Stack pointer */
+      /*   48 */ ULong guest_R4;   /* Argument registers / Return value */
+      /*   56 */ ULong guest_R5;
+      /*   64 */ ULong guest_R6;   /* Argument registers */
+      /*   72 */ ULong guest_R7;
+      /*   80 */ ULong guest_R8;
+      /*   88 */ ULong guest_R9;
+      /*   96 */ ULong guest_R10;
+      /*  104 */ ULong guest_R11;
+      /*  112 */ ULong guest_R12;  /* Temporary registers */
+      /*  120 */ ULong guest_R13;
+      /*  128 */ ULong guest_R14;
+      /*  136 */ ULong guest_R15;
+      /*  144 */ ULong guest_R16;
+      /*  152 */ ULong guest_R17;
+      /*  160 */ ULong guest_R18;
+      /*  168 */ ULong guest_R19;
+      /*  176 */ ULong guest_R20;
+      /*  184 */ ULong guest_R21;  /* Reserved */
+      /*  192 */ ULong guest_R22;  /* Frame pointer / Static register */
+      /*  200 */ ULong guest_R23;  /* Static registers */
+      /*  208 */ ULong guest_R24;
+      /*  216 */ ULong guest_R25;
+      /*  224 */ ULong guest_R26;
+      /*  232 */ ULong guest_R27;
+      /*  240 */ ULong guest_R28;
+      /*  248 */ ULong guest_R29;
+      /*  256 */ ULong guest_R30;
+      /*  264 */ ULong guest_R31;
+
+      /*  272 */ ULong guest_PC;    /* Program counter */
+
+      /*  280 */ UChar guest_FCC0;  /* Condition Flag Registers */
+      /*  281 */ UChar guest_FCC1;
+      /*  282 */ UChar guest_FCC2;
+      /*  283 */ UChar guest_FCC3;
+      /*  284 */ UChar guest_FCC4;
+      /*  285 */ UChar guest_FCC5;
+      /*  286 */ UChar guest_FCC6;
+      /*  287 */ UChar guest_FCC7;
+      /*  288 */ UInt  guest_FCSR;  /* FP/SIMD Control and Status Register */
+
+      /* Various pseudo-regs mandated by Vex or Valgrind. */
+      /* Emulation notes */
+      /*  292 */ UInt  guest_EMNOTE;
+
+      /* For clflush: record start and length of area to invalidate */
+      /*  296 */ ULong guest_CMSTART;
+      /*  304 */ ULong guest_CMLEN;
+
+      /* Used to record the unredirected guest address at the start of
+         a translation whose start has been redirected.  By reading
+         this pseudo-register shortly afterwards, the translation can
+         find out what the corresponding no-redirection address was.
+         Note, this is only set for wrap-style redirects, not for
+         replace-style ones. */
+      /*  312 */ ULong guest_NRADDR;
+
+      /* Fallback LL/SC support. */
+      /*  320 */ ULong guest_LLSC_SIZE; /* 0==no transaction, else 4 or 8. */
+      /*  328 */ ULong guest_LLSC_ADDR; /* Address of the transaction. */
+      /*  336 */ ULong guest_LLSC_DATA; /* Original value at ADDR. */
+
+      /*  344 */ ULong _padding1;
+
+      /* FPU/SIMD Registers */
+      /*  352 */ U256 guest_X0;
+      /*  384 */ U256 guest_X1;
+      /*  416 */ U256 guest_X2;
+      /*  448 */ U256 guest_X3;
+      /*  480 */ U256 guest_X4;
+      /*  512 */ U256 guest_X5;
+      /*  544 */ U256 guest_X6;
+      /*  576 */ U256 guest_X7;
+      /*  608 */ U256 guest_X8;
+      /*  640 */ U256 guest_X9;
+      /*  672 */ U256 guest_X10;
+      /*  704 */ U256 guest_X11;
+      /*  736 */ U256 guest_X12;
+      /*  768 */ U256 guest_X13;
+      /*  800 */ U256 guest_X14;
+      /*  832 */ U256 guest_X15;
+      /*  864 */ U256 guest_X16;
+      /*  896 */ U256 guest_X17;
+      /*  928 */ U256 guest_X18;
+      /*  960 */ U256 guest_X19;
+      /*  992 */ U256 guest_X20;
+      /* 1024 */ U256 guest_X21;
+      /* 1056 */ U256 guest_X22;
+      /* 1088 */ U256 guest_X23;
+      /* 1120 */ U256 guest_X24;
+      /* 1152 */ U256 guest_X25;
+      /* 1184 */ U256 guest_X26;
+      /* 1216 */ U256 guest_X27;
+      /* 1248 */ U256 guest_X28;
+      /* 1280 */ U256 guest_X29;
+      /* 1312 */ U256 guest_X30;
+      /* 1244 */ U256 guest_X31;
+
+      /* 1376 */ /* VexGuestLOONGARCH64State should have a 16-aligned size */
+} VexGuestLOONGARCH64State;
+
+/*---------------------------------------------------------------*/
+/*--- Utility functions for LOONGARCH64 guest stuff.          ---*/
+/*---------------------------------------------------------------*/
+
+/* ALL THE FOLLOWING ARE VISIBLE TO LIBRARY CLIENT. */
+
+/* Initialise all guest LOONGARCH64 state. */
+
+extern
+void LibVEX_GuestLOONGARCH64_initialise ( /*OUT*/
+                                          VexGuestLOONGARCH64State* vex_state );
+
+#endif /* ndef __LIBVEX_PUB_GUEST_LOONGARCH64_H */
+
+/*---------------------------------------------------------------*/
+/*---                              libvex_guest_loongarch64.h ---*/
+/*---------------------------------------------------------------*/
diff --git a/VEX/pub/libvex_ir.h b/VEX/pub/libvex_ir.h
index f6f347a..9f98532 100644
--- a/VEX/pub/libvex_ir.h
+++ b/VEX/pub/libvex_ir.h
@@ -588,10 +588,10 @@ typedef
 
       /* Binary operations, with rounding. */
       /* :: IRRoundingMode(I32) x F64 x F64 -> F64 */ 
-      Iop_AddF64, Iop_SubF64, Iop_MulF64, Iop_DivF64,
+      Iop_AddF64, Iop_SubF64, Iop_MulF64, Iop_DivF64, Iop_ScaleBF64,
 
       /* :: IRRoundingMode(I32) x F32 x F32 -> F32 */ 
-      Iop_AddF32, Iop_SubF32, Iop_MulF32, Iop_DivF32,
+      Iop_AddF32, Iop_SubF32, Iop_MulF32, Iop_DivF32, Iop_ScaleBF32,
 
       /* Variants of the above which produce a 64-bit result but which
          round their result to a IEEE float range first. */
@@ -610,10 +610,10 @@ typedef
 
       /* Unary operations, with rounding. */
       /* :: IRRoundingMode(I32) x F64 -> F64 */
-      Iop_SqrtF64,
+      Iop_SqrtF64, Iop_RSqrtF64, Iop_LogBF64,
 
       /* :: IRRoundingMode(I32) x F32 -> F32 */
-      Iop_SqrtF32,
+      Iop_SqrtF32, Iop_RSqrtF32, Iop_LogBF32,
 
       /* :: IRRoundingMode(I32) x F16 -> F16 */
       Iop_SqrtF16,
@@ -834,10 +834,14 @@ typedef
 
       /* --------- Possibly required by IEEE 754-2008. --------- */
 
-      Iop_MaxNumF64,  /* max, F64, numerical operand if other is a qNaN */
-      Iop_MinNumF64,  /* min, F64, ditto */
-      Iop_MaxNumF32,  /* max, F32, ditto */
-      Iop_MinNumF32,  /* min, F32, ditto */
+      Iop_MaxNumF64,    /* max, F64, numerical operand if other is a qNaN */
+      Iop_MinNumF64,    /* min, F64, ditto */
+      Iop_MaxNumAbsF64, /* max abs, F64, ditto */
+      Iop_MinNumAbsF64, /* min abs, F64, ditto */
+      Iop_MaxNumF32,    /* max, F32, ditto */
+      Iop_MinNumF32,    /* min, F32, ditto */
+      Iop_MaxNumAbsF32, /* max abs, F32, ditto */
+      Iop_MinNumAbsF32, /* min abs, F32, ditto */
 
       /* ------------------ 16-bit scalar FP ------------------ */
 
@@ -2011,10 +2015,10 @@ typedef
       Iop_ShrN16x16, Iop_ShrN32x8, Iop_ShrN64x4,
       Iop_SarN16x16, Iop_SarN32x8,
 
-      Iop_Max8Sx32, Iop_Max16Sx16, Iop_Max32Sx8,
-      Iop_Max8Ux32, Iop_Max16Ux16, Iop_Max32Ux8,
-      Iop_Min8Sx32, Iop_Min16Sx16, Iop_Min32Sx8,
-      Iop_Min8Ux32, Iop_Min16Ux16, Iop_Min32Ux8,
+      Iop_Max8Sx32, Iop_Max16Sx16, Iop_Max32Sx8, Iop_Max64Sx4,
+      Iop_Max8Ux32, Iop_Max16Ux16, Iop_Max32Ux8, Iop_Max64Ux4,
+      Iop_Min8Sx32, Iop_Min16Sx16, Iop_Min32Sx8, Iop_Min64Sx4,
+      Iop_Min8Ux32, Iop_Min16Ux16, Iop_Min32Ux8, Iop_Min64Ux4,
 
       Iop_Mul16x16, Iop_Mul32x8,
       Iop_MulHi16Ux16, Iop_MulHi16Sx16,
@@ -2513,6 +2517,7 @@ typedef
       Ijk_SigFPE,         /* current instruction synths generic SIGFPE */
       Ijk_SigFPE_IntDiv,  /* current instruction synths SIGFPE - IntDiv */
       Ijk_SigFPE_IntOvf,  /* current instruction synths SIGFPE - IntOvf */
+      Ijk_SigSYS,         /* current instruction synths SIGSYS */
       /* Unfortunately, various guest-dependent syscall kinds.  They
 	 all mean: do a syscall before continuing. */
       Ijk_Sys_syscall,    /* amd64/x86 'syscall', ppc 'sc', arm 'svc #0' */
@@ -2673,7 +2678,12 @@ typedef
       /* Needed only on ARM.  It cancels a reservation made by a
          preceding Linked-Load, and needs to be handed through to the
          back end, just as LL and SC themselves are. */
-      Imbe_CancelReservation
+      Imbe_CancelReservation,
+      /* Needed only on LOONGARCH64.  It completes the synchronization
+         between the store operation and the instruction fetch operation
+         within a single processor core, and needs to be handed through
+         to the back end. */
+      Imbe_InsnFence
    }
    IRMBusEvent;
 
diff --git a/VEX/pub/libvex_trc_values.h b/VEX/pub/libvex_trc_values.h
index c9adcb7..987d5cc 100644
--- a/VEX/pub/libvex_trc_values.h
+++ b/VEX/pub/libvex_trc_values.h
@@ -57,6 +57,7 @@
                                       continuing */
 #define VEX_TRC_JMP_SIGBUS     93  /* deliver SIGBUS before continuing */
 #define VEX_TRC_JMP_SIGFPE    105  /* deliver SIGFPE before continuing */
+#define VEX_TRC_JMP_SIGSYS    115  /* deliver SIGSYS before continuing */
 
 #define VEX_TRC_JMP_SIGFPE_INTDIV     97  /* deliver SIGFPE (integer divide
                                              by zero) before continuing */
diff --git a/coregrind/m_scheduler/scheduler.c b/coregrind/m_scheduler/scheduler.c
index cc8d070..eb9db94 100644
--- a/coregrind/m_scheduler/scheduler.c
+++ b/coregrind/m_scheduler/scheduler.c
@@ -272,6 +272,7 @@ const HChar* name_of_sched_event ( UInt event )
       case VEX_TRC_JMP_SIGBUS:         return "SIGBUS";
       case VEX_TRC_JMP_SIGFPE_INTOVF:
       case VEX_TRC_JMP_SIGFPE_INTDIV:  return "SIGFPE";
+      case VEX_TRC_JMP_SIGSYS:         return "SIGSYS";
       case VEX_TRC_JMP_EMWARN:         return "EMWARN";
       case VEX_TRC_JMP_EMFAIL:         return "EMFAIL";
       case VEX_TRC_JMP_CLIENTREQ:      return "CLIENTREQ";
@@ -1687,6 +1688,10 @@ VgSchedReturnCode VG_(scheduler) ( ThreadId tid )
          VG_(synth_sigfpe)(tid, VKI_FPE_INTOVF);
          break;
 
+      case VEX_TRC_JMP_SIGSYS:
+         VG_(synth_sigsys)(tid);
+         break;
+
       case VEX_TRC_JMP_NODECODE: {
          Addr addr = VG_(get_IP)(tid);
 
diff --git a/coregrind/m_signals.c b/coregrind/m_signals.c
index 66fbbfe..1bf485f 100644
--- a/coregrind/m_signals.c
+++ b/coregrind/m_signals.c
@@ -2342,6 +2342,30 @@ void VG_(synth_sigfpe)(ThreadId tid, UInt code)
 #endif
 }
 
+// Synthesise a SIGSYS.
+void VG_(synth_sigsys)(ThreadId tid)
+{
+// Only tested on loongarch64-linux.
+#if !defined(VGP_loongarch64_linux)
+   vg_assert(0);
+#else
+   vki_siginfo_t info;
+
+   vg_assert(VG_(threads)[tid].status == VgTs_Runnable);
+
+   VG_(memset)(&info, 0, sizeof(info));
+   info.si_signo = VKI_SIGSYS;
+   info.si_code  = VKI_SI_KERNEL;
+
+   if (VG_(gdbserver_report_signal) (&info, tid)) {
+      resume_scheduler(tid);
+      deliver_signal(tid, &info, NULL);
+   }
+   else
+      resume_scheduler(tid);
+#endif
+}
+
 /* Make a signal pending for a thread, for later delivery.
    VG_(poll_signals) will arrange for it to be delivered at the right
    time. 
diff --git a/coregrind/pub_core_signals.h b/coregrind/pub_core_signals.h
index ae8555b..c53323f 100644
--- a/coregrind/pub_core_signals.h
+++ b/coregrind/pub_core_signals.h
@@ -77,6 +77,7 @@ extern void VG_(synth_sigill)       (ThreadId tid, Addr addr);
 extern void VG_(synth_sigtrap)      (ThreadId tid);
 extern void VG_(synth_sigbus)       (ThreadId tid);
 extern void VG_(synth_sigfpe)       (ThreadId tid, UInt code);
+extern void VG_(synth_sigsys)       (ThreadId tid);
 
 /* Extend the stack to cover addr, if possible */
 extern Bool VG_(extend_stack)(ThreadId tid, Addr addr);
diff --git a/drd/drd_load_store.c b/drd/drd_load_store.c
index 80d326a..04bb3bd 100644
--- a/drd/drd_load_store.c
+++ b/drd/drd_load_store.c
@@ -634,6 +634,8 @@ IRSB* DRD_(instrument)(VgCallbackClosure* const closure,
             break; /* not interesting to DRD */
          case Imbe_CancelReservation:
             break; /* not interesting to DRD */
+         case Imbe_InsnFence:
+            break; /* not interesting to DRD */
          default:
             tl_assert(0);
          }
diff --git a/helgrind/hg_main.c b/helgrind/hg_main.c
index bbf95c0..a32e4d8 100644
--- a/helgrind/hg_main.c
+++ b/helgrind/hg_main.c
@@ -4888,6 +4888,7 @@ IRSB* hg_instrument ( VgCallbackClosure* closure,
             switch (st->Ist.MBE.event) {
                case Imbe_Fence:
                case Imbe_CancelReservation:
+               case Imbe_InsnFence:
                   break; /* not interesting */
                default:
                   goto unhandled;
diff --git a/include/vki/vki-linux.h b/include/vki/vki-linux.h
index 006f16d..eef3440 100644
--- a/include/vki/vki-linux.h
+++ b/include/vki/vki-linux.h
@@ -531,6 +531,7 @@ typedef struct vki_siginfo {
  * Digital reserves positive values for kernel-generated signals.
  */
 #define VKI_SI_USER	0		/* sent by kill, sigsend, raise */
+#define VKI_SI_KERNEL	0x80		/* sent by the kernel from somewhere */
 #define VKI_SI_TKILL	-6		/* sent by tkill system call */
 
 /*
diff --git a/memcheck/mc_translate.c b/memcheck/mc_translate.c
index 05e6d59..52d1939 100644
--- a/memcheck/mc_translate.c
+++ b/memcheck/mc_translate.c
@@ -3505,6 +3505,7 @@ IRAtom* expr2vbits_Triop ( MCEnv* mce,
       case Iop_MulD64:
       case Iop_MulF64r32:
       case Iop_DivF64:
+      case Iop_ScaleBF64:
       case Iop_DivD64:
       case Iop_DivF64r32:
       case Iop_ScaleF64:
@@ -3524,6 +3525,7 @@ IRAtom* expr2vbits_Triop ( MCEnv* mce,
       case Iop_SubF32:
       case Iop_MulF32:
       case Iop_DivF32:
+      case Iop_ScaleBF32:
          /* I32(rm) x F32 x F32 -> I32 */
          return mkLazy3(mce, Ity_I32, vatom1, vatom2, vatom3);
       case Iop_AddF16:
@@ -4532,6 +4534,8 @@ IRAtom* expr2vbits_Binop ( MCEnv* mce,
       case Iop_TanF64:
       case Iop_2xm1F64:
       case Iop_SqrtF64:
+      case Iop_RSqrtF64:
+      case Iop_LogBF64:
       case Iop_RecpExpF64:
          /* I32(rm) x I64/F64 -> I64/F64 */
          return mkLazy2(mce, Ity_I64, vatom1, vatom2);
@@ -4593,6 +4597,8 @@ IRAtom* expr2vbits_Binop ( MCEnv* mce,
 
       case Iop_RoundF32toInt:
       case Iop_SqrtF32:
+      case Iop_RSqrtF32:
+      case Iop_LogBF32:
       case Iop_RecpExpF32:
          /* I32(rm) x I32/F32 -> I32/F32 */
          return mkLazy2(mce, Ity_I32, vatom1, vatom2);
@@ -4675,11 +4681,15 @@ IRAtom* expr2vbits_Binop ( MCEnv* mce,
 
       case Iop_MaxNumF32:
       case Iop_MinNumF32:
+      case Iop_MaxNumAbsF32:
+      case Iop_MinNumAbsF32:
          /* F32 x F32 -> F32 */
          return mkLazy2(mce, Ity_I32, vatom1, vatom2);
 
       case Iop_MaxNumF64:
       case Iop_MinNumF64:
+      case Iop_MaxNumAbsF64:
+      case Iop_MinNumAbsF64:
          /* F64 x F64 -> F64 */
          return mkLazy2(mce, Ity_I64, vatom1, vatom2);
 
@@ -5026,6 +5036,10 @@ IRAtom* expr2vbits_Binop ( MCEnv* mce,
       case Iop_Add64x4:
       case Iop_CmpEQ64x4:
       case Iop_CmpGT64Sx4:
+      case Iop_Max64Sx4:
+      case Iop_Max64Ux4:
+      case Iop_Min64Sx4:
+      case Iop_Min64Ux4:
          return binary64Ix4(mce, vatom1, vatom2);
 
       case Iop_I32StoF32x8:
diff --git a/memcheck/tests/vbit-test/irops.c b/memcheck/tests/vbit-test/irops.c
index 4755ce4..edcbf72 100644
--- a/memcheck/tests/vbit-test/irops.c
+++ b/memcheck/tests/vbit-test/irops.c
@@ -34,291 +34,301 @@
    That is not necessary but helpful when supporting a new architecture.
 */
 static irop_t irops[] = {
-  { DEFOP(Iop_Add8,    UNDEF_LEFT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_Add16,   UNDEF_LEFT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_Add32,   UNDEF_INT_ADD), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_Add64,   UNDEF_INT_ADD), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 }, // mips asserts
-  { DEFOP(Iop_Sub8,    UNDEF_LEFT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_Sub16,   UNDEF_LEFT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_Sub32,   UNDEF_INT_SUB), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_Sub64,   UNDEF_INT_SUB), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1 }, // ppc32, mips assert
-  { DEFOP(Iop_Mul8,    UNDEF_LEFT), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_Mul16,   UNDEF_LEFT), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_Mul32,   UNDEF_LEFT), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_Mul64,   UNDEF_LEFT), .s390x = 0, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1 }, // ppc32, mips assert
-  { DEFOP(Iop_Or1,     UNDEF_OR),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_Or8,     UNDEF_OR),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_Or16,    UNDEF_OR),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_Or32,    UNDEF_OR),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_Or64,    UNDEF_OR),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 }, // mips asserts
-  { DEFOP(Iop_And1,    UNDEF_AND),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_And8,    UNDEF_AND),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_And16,   UNDEF_AND),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_And32,   UNDEF_AND),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_And64,   UNDEF_AND),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_Xor8,    UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_Xor16,   UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_Xor32,   UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_Xor64,   UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_Shl8,    UNDEF_SHL),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_Shl16,   UNDEF_SHL),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_Shl32,   UNDEF_SHL),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_Shl64,   UNDEF_SHL),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1 }, // ppc32 asserts
-  { DEFOP(Iop_Shr8,    UNDEF_SHR),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 }, // ppc32/64 assert
-  { DEFOP(Iop_Shr16,   UNDEF_SHR),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1 }, // ppc32/64 assert
-  { DEFOP(Iop_Shr32,   UNDEF_SHR),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_Shr64,   UNDEF_SHR),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1 }, // ppc32 asserts
-  { DEFOP(Iop_Sar8,    UNDEF_SAR),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 }, // ppc32/64 assert
-  { DEFOP(Iop_Sar16,   UNDEF_SAR),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1 }, // ppc32/64 assert
-  { DEFOP(Iop_Sar32,   UNDEF_SAR),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_Sar64,   UNDEF_SAR),  .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 1, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1 }, // ppc32 asserts
-  { DEFOP(Iop_CmpEQ8,  UNDEF_CMP_EQ_NE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_CmpEQ16, UNDEF_CMP_EQ_NE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_CmpEQ32, UNDEF_CMP_EQ_NE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_CmpEQ64, UNDEF_CMP_EQ_NE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1 }, // ppc32, mips assert
-  { DEFOP(Iop_CmpNE8,  UNDEF_CMP_EQ_NE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_CmpNE16, UNDEF_CMP_EQ_NE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_CmpNE32, UNDEF_CMP_EQ_NE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_CmpNE64, UNDEF_CMP_EQ_NE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1 }, // ppc32, mips assert
-  { DEFOP(Iop_Not1,       UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_Not8,       UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_Not16,      UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_Not32,      UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_Not64,      UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_CasCmpEQ8,  UNDEF_NONE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_CasCmpEQ16, UNDEF_NONE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_CasCmpEQ32, UNDEF_NONE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_CasCmpEQ64, UNDEF_NONE), .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
+  { DEFOP(Iop_Add8,    UNDEF_LEFT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_Add16,   UNDEF_LEFT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_Add32,   UNDEF_INT_ADD), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_Add64,   UNDEF_INT_ADD), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // mips asserts
+  { DEFOP(Iop_Sub8,    UNDEF_LEFT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_Sub16,   UNDEF_LEFT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_Sub32,   UNDEF_INT_SUB), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_Sub64,   UNDEF_INT_SUB), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // ppc32, mips assert
+  { DEFOP(Iop_Mul8,    UNDEF_LEFT), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_Mul16,   UNDEF_LEFT), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_Mul32,   UNDEF_LEFT), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_Mul64,   UNDEF_LEFT), .s390x = 0, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1, .loongarch64 = 0 }, // ppc32, mips assert
+  { DEFOP(Iop_Or1,     UNDEF_OR),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_Or8,     UNDEF_OR),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_Or16,    UNDEF_OR),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_Or32,    UNDEF_OR),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_Or64,    UNDEF_OR),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // mips asserts
+  { DEFOP(Iop_And1,    UNDEF_AND),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_And8,    UNDEF_AND),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_And16,   UNDEF_AND),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_And32,   UNDEF_AND),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_And64,   UNDEF_AND),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_Xor8,    UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_Xor16,   UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_Xor32,   UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_Xor64,   UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_Shl8,    UNDEF_SHL),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_Shl16,   UNDEF_SHL),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_Shl32,   UNDEF_SHL),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_Shl64,   UNDEF_SHL),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // ppc32 asserts
+  { DEFOP(Iop_Shr8,    UNDEF_SHR),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 }, // ppc32/64 assert
+  { DEFOP(Iop_Shr16,   UNDEF_SHR),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 }, // ppc32/64 assert
+  { DEFOP(Iop_Shr32,   UNDEF_SHR),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_Shr64,   UNDEF_SHR),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // ppc32 asserts
+  { DEFOP(Iop_Sar8,    UNDEF_SAR),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 }, // ppc32/64 assert
+  { DEFOP(Iop_Sar16,   UNDEF_SAR),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 }, // ppc32/64 assert
+  { DEFOP(Iop_Sar32,   UNDEF_SAR),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_Sar64,   UNDEF_SAR),  .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 1, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // ppc32 asserts
+  { DEFOP(Iop_CmpEQ8,  UNDEF_CMP_EQ_NE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_CmpEQ16, UNDEF_CMP_EQ_NE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_CmpEQ32, UNDEF_CMP_EQ_NE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_CmpEQ64, UNDEF_CMP_EQ_NE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // ppc32, mips assert
+  { DEFOP(Iop_CmpNE8,  UNDEF_CMP_EQ_NE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_CmpNE16, UNDEF_CMP_EQ_NE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_CmpNE32, UNDEF_CMP_EQ_NE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_CmpNE64, UNDEF_CMP_EQ_NE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // ppc32, mips assert
+  { DEFOP(Iop_Not1,       UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_Not8,       UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_Not16,      UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_Not32,      UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_Not64,      UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0, .loongarch64 = 1 },
+  { DEFOP(Iop_CasCmpEQ8,  UNDEF_NONE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_CasCmpEQ16, UNDEF_NONE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_CasCmpEQ32, UNDEF_NONE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 1 },
+  { DEFOP(Iop_CasCmpEQ64, UNDEF_NONE), .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 1 },
 
-  { DEFOP(Iop_CasCmpNE8,  UNDEF_NONE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_CasCmpNE16, UNDEF_NONE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_CasCmpNE32, UNDEF_NONE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_CasCmpNE64, UNDEF_NONE), .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
+  { DEFOP(Iop_CasCmpNE8,  UNDEF_NONE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_CasCmpNE16, UNDEF_NONE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_CasCmpNE32, UNDEF_NONE), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 1 },
+  { DEFOP(Iop_CasCmpNE64, UNDEF_NONE), .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 1 },
   { DEFOP(Iop_ExpCmpNE8,  UNDEF_UNKNOWN), }, // exact (expensive) equality
   { DEFOP(Iop_ExpCmpNE16, UNDEF_UNKNOWN), }, // exact (expensive) equality
   { DEFOP(Iop_ExpCmpNE32, UNDEF_UNKNOWN), }, // exact (expensive) equality
   { DEFOP(Iop_ExpCmpNE64, UNDEF_UNKNOWN), }, // exact (expensive) equality
-  { DEFOP(Iop_MullS8,     UNDEF_LEFT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_MullS16,    UNDEF_LEFT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_MullS32,    UNDEF_LEFT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 }, // mips asserts
+  { DEFOP(Iop_MullS8,     UNDEF_LEFT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_MullS16,    UNDEF_LEFT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_MullS32,    UNDEF_LEFT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // mips asserts
   // s390 has signed multiplication of 64-bit values but the result
   // is 64-bit (not 128-bit). So we cannot test this op standalone.
-  { DEFOP(Iop_MullS64,    UNDEF_LEFT), .s390x = 0, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 =0, .mips64 = 1 }, // ppc32, mips assert
-  { DEFOP(Iop_MullU8,     UNDEF_LEFT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 =0, .mips64 = 0 },
-  { DEFOP(Iop_MullU16,    UNDEF_LEFT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 =0, .mips64 = 0 },
-  { DEFOP(Iop_MullU32,    UNDEF_LEFT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 =0, .mips64 = 1 }, // mips asserts
-  { DEFOP(Iop_MullU64,    UNDEF_LEFT), .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 =0, .mips64 = 1 }, // ppc32, mips assert
-  { DEFOP(Iop_Clz64,      UNDEF_ALL),  .s390x = 0, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 =0, .mips64 = 1 }, // ppc32 asserts
-  { DEFOP(Iop_Clz32,      UNDEF_ALL),  .s390x = 0, .amd64 = 0, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 =1, .mips64 = 1 },
-  { DEFOP(Iop_Ctz64,      UNDEF_ALL),  .s390x = 0, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 =0, .mips64 = 0 },
-  { DEFOP(Iop_Ctz32,      UNDEF_ALL),  .s390x = 0, .amd64 = 0, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 =0, .mips64 = 0 },
-  { DEFOP(Iop_ClzNat64,   UNDEF_ALL),  .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 =0, .mips64 = 0 }, // ppc32 asserts
-  { DEFOP(Iop_ClzNat32,   UNDEF_ALL),  .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 =0, .mips64 = 0 },
-  { DEFOP(Iop_CtzNat64,   UNDEF_ALL),  .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 =0, .mips64 = 0 },
-  { DEFOP(Iop_CtzNat32,   UNDEF_ALL),  .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 1, .mips32 =0, .mips64 = 0 },
-  { DEFOP(Iop_PopCount64, UNDEF_ALL),  .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 =0, .mips64 = 0 },
-  { DEFOP(Iop_PopCount32, UNDEF_ALL),  .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 =0, .mips64 = 0 },
-  { DEFOP(Iop_CmpLT32S,   UNDEF_ALL),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 =1, .mips64 = 1 },
-  { DEFOP(Iop_CmpLT64S,   UNDEF_ALL),  .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 =0, .mips64 = 1 }, // ppc, mips assert
-  { DEFOP(Iop_CmpLE32S,   UNDEF_ALL),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 =1, .mips64 = 1 },
-  { DEFOP(Iop_CmpLE64S,   UNDEF_ALL),  .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 =0, .mips64 = 1 }, // ppc, mips assert
-  { DEFOP(Iop_CmpLT32U,   UNDEF_ALL),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 =1, .mips64 = 1 },
-  { DEFOP(Iop_CmpLT64U,   UNDEF_ALL),  .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 =0, .mips64 = 1}, // ppc32, mips assert
-  { DEFOP(Iop_CmpLE32U,   UNDEF_ALL),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 =1, .mips64 = 1 },
-  { DEFOP(Iop_CmpLE64U,   UNDEF_ALL),  .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 =0, .mips64 = 0 }, // ppc32 asserts
+  { DEFOP(Iop_MullS64,    UNDEF_LEFT), .s390x = 0, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 =0, .mips64 = 1, .loongarch64 = 1 }, // ppc32, mips assert
+  { DEFOP(Iop_MullU8,     UNDEF_LEFT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 =0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_MullU16,    UNDEF_LEFT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 =0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_MullU32,    UNDEF_LEFT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 =0, .mips64 = 1, .loongarch64 = 1 }, // mips asserts
+  { DEFOP(Iop_MullU64,    UNDEF_LEFT), .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 =0, .mips64 = 1, .loongarch64 = 1 }, // ppc32, mips assert
+  { DEFOP(Iop_Clz64,      UNDEF_ALL),  .s390x = 0, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 =0, .mips64 = 1, .loongarch64 = 1 }, // ppc32 asserts
+  { DEFOP(Iop_Clz32,      UNDEF_ALL),  .s390x = 0, .amd64 = 0, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 =1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_Ctz64,      UNDEF_ALL),  .s390x = 0, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 =0, .mips64 = 0, .loongarch64 = 1 },
+  { DEFOP(Iop_Ctz32,      UNDEF_ALL),  .s390x = 0, .amd64 = 0, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 =0, .mips64 = 0, .loongarch64 = 1 },
+  { DEFOP(Iop_ClzNat64,   UNDEF_ALL),  .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 =0, .mips64 = 0, .loongarch64 = 0 }, // ppc32 asserts
+  { DEFOP(Iop_ClzNat32,   UNDEF_ALL),  .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 =0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_CtzNat64,   UNDEF_ALL),  .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 =0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_CtzNat32,   UNDEF_ALL),  .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 1, .mips32 =0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_PopCount64, UNDEF_ALL),  .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 =0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_PopCount32, UNDEF_ALL),  .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 =0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_CmpLT32S,   UNDEF_ALL),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 =1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_CmpLT64S,   UNDEF_ALL),  .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 =0, .mips64 = 1, .loongarch64 = 1 }, // ppc, mips assert
+  { DEFOP(Iop_CmpLE32S,   UNDEF_ALL),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 =1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_CmpLE64S,   UNDEF_ALL),  .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 =0, .mips64 = 1, .loongarch64 = 1 }, // ppc, mips assert
+  { DEFOP(Iop_CmpLT32U,   UNDEF_ALL),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 =1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_CmpLT64U,   UNDEF_ALL),  .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 =0, .mips64 = 1, .loongarch64 = 1 }, // ppc32, mips assert
+  { DEFOP(Iop_CmpLE32U,   UNDEF_ALL),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 =1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_CmpLE64U,   UNDEF_ALL),  .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 =0, .mips64 = 0, .loongarch64 = 1 }, // ppc32 asserts
   { DEFOP(Iop_CmpNEZ8,    UNDEF_ALL), },   // not supported by mc_translate
   { DEFOP(Iop_CmpNEZ16,   UNDEF_ALL), },   // not supported by mc_translate
   { DEFOP(Iop_CmpNEZ32,   UNDEF_ALL), },   // not supported by mc_translate
   { DEFOP(Iop_CmpNEZ64,   UNDEF_ALL), },   // not supported by mc_translate
-  { DEFOP(Iop_CmpwNEZ32,  UNDEF_ALL),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_CmpwNEZ64,  UNDEF_ALL),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 }, // mips asserts
+  { DEFOP(Iop_CmpwNEZ32,  UNDEF_ALL),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_CmpwNEZ64,  UNDEF_ALL),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // mips asserts
   { DEFOP(Iop_Left8,      UNDEF_UNKNOWN), },  // not supported by mc_translate
   { DEFOP(Iop_Left16,     UNDEF_UNKNOWN), },  // not supported by mc_translate
   { DEFOP(Iop_Left32,     UNDEF_UNKNOWN), },  // not supported by mc_translate
   { DEFOP(Iop_Left64,     UNDEF_UNKNOWN), },  // not supported by mc_translate
   { DEFOP(Iop_Max32U,     UNDEF_UNKNOWN), },  // not supported by mc_translate
-  { DEFOP(Iop_CmpORD32U,  UNDEF_ORD), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0 }, // support added in vbit-test
-  { DEFOP(Iop_CmpORD64U,  UNDEF_ORD), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 0 }, // support added in vbit-test
-  { DEFOP(Iop_CmpORD32S,  UNDEF_ORD), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0 }, // support added in vbit-test
-  { DEFOP(Iop_CmpORD64S,  UNDEF_ORD), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 0 }, // support added in vbit-test
-  { DEFOP(Iop_DivU32,     UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_DivS32,     UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_DivU64,     UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 0 }, // ppc32 asserts
-  { DEFOP(Iop_DivS64,     UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 0 }, // ppc32 asserts
-  { DEFOP(Iop_DivU64E,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 0 }, // ppc32 asserts
-  { DEFOP(Iop_DivS64E,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 0 }, // ppc32 asserts
-  { DEFOP(Iop_DivU32E,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_DivS32E,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0 },
+  { DEFOP(Iop_CmpORD32U,  UNDEF_ORD), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 }, // support added in vbit-test
+  { DEFOP(Iop_CmpORD64U,  UNDEF_ORD), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 }, // support added in vbit-test
+  { DEFOP(Iop_CmpORD32S,  UNDEF_ORD), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 }, // support added in vbit-test
+  { DEFOP(Iop_CmpORD64S,  UNDEF_ORD), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 }, // support added in vbit-test
+  { DEFOP(Iop_DivU32,     UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0, .loongarch64 = 1 },
+  { DEFOP(Iop_DivS32,     UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0, .loongarch64 = 1 },
+  { DEFOP(Iop_DivU64,     UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 1 }, // ppc32 asserts
+  { DEFOP(Iop_DivS64,     UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 1 }, // ppc32 asserts
+  { DEFOP(Iop_DivU64E,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 }, // ppc32 asserts
+  { DEFOP(Iop_DivS64E,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 }, // ppc32 asserts
+  { DEFOP(Iop_DivU32E,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_DivS32E,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
   // On s390 the DivMod operations always appear in a certain context
   // So they cannot be tested in isolation on that platform.
-  { DEFOP(Iop_DivModU64to32,  UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_DivModS64to32,  UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_DivModU32to32,  UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_DivModS32to32,  UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_DivModU128to64, UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 }, // mips asserts
-  { DEFOP(Iop_DivModS128to64, UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 }, // mips asserts
-  { DEFOP(Iop_DivModS64to64,  UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 1 }, // mips asserts
-  { DEFOP(Iop_DivModU64to64,  UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 1 }, // mips asserts
-  { DEFOP(Iop_8Uto16,    UNDEF_ZEXT),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_8Uto32,    UNDEF_ZEXT),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_8Uto64,    UNDEF_ZEXT),   .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1 }, // ppc32 assert
-  { DEFOP(Iop_16Uto32,   UNDEF_ZEXT),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_16Uto64,   UNDEF_ZEXT),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1 }, // ppc32 assert
-  { DEFOP(Iop_32Uto64,   UNDEF_ZEXT),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 }, // mips asserts
-  { DEFOP(Iop_8Sto16,    UNDEF_SEXT),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_8Sto32,    UNDEF_SEXT),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_8Sto64,    UNDEF_SEXT),   .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1 }, // ppc32, mips assert
-  { DEFOP(Iop_16Sto32,   UNDEF_SEXT),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_16Sto64,   UNDEF_SEXT),   .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1 }, // ppc32, mips assert
-  { DEFOP(Iop_32Sto64,   UNDEF_SEXT),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 }, // mips asserts
-  { DEFOP(Iop_64to8,     UNDEF_TRUNC),  .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 1, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1 }, // ppc32, mips assert
-  { DEFOP(Iop_32to8,     UNDEF_TRUNC),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_64to16,    UNDEF_TRUNC),  .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1 }, // ppc32, mips assert
-  { DEFOP(Iop_16to8,     UNDEF_TRUNC),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_16HIto8,   UNDEF_UPPER),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_8HLto16,   UNDEF_CONCAT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1 },  // ppc isel
-  { DEFOP(Iop_32to16,    UNDEF_TRUNC),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_32HIto16,  UNDEF_UPPER),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_16HLto32,  UNDEF_CONCAT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1 },  // ppc isel
-  { DEFOP(Iop_64to32,    UNDEF_TRUNC),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 }, // mips asserts
-  { DEFOP(Iop_64HIto32,  UNDEF_UPPER),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 },
-  { DEFOP(Iop_32HLto64,  UNDEF_CONCAT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 }, // mips asserts
-  { DEFOP(Iop_128to64,   UNDEF_TRUNC),  .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 }, // mips asserts
-  { DEFOP(Iop_128HIto64, UNDEF_UPPER),  .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 }, // mips asserts
-  { DEFOP(Iop_64HLto128, UNDEF_CONCAT), .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 }, // mips asserts
-  { DEFOP(Iop_32to1,     UNDEF_TRUNC),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_64to1,     UNDEF_TRUNC),  .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1 }, // ppc32, mips assert
-  { DEFOP(Iop_1Uto8,     UNDEF_ZEXT),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_1Uto32,    UNDEF_ZEXT),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_1Uto64,    UNDEF_ZEXT),   .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1 }, // ppc32 assert
-  { DEFOP(Iop_1Sto8,     UNDEF_ALL),    .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
+  { DEFOP(Iop_DivModU64to32,  UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_DivModS64to32,  UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_DivModU32to32,  UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_DivModS32to32,  UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_DivModU128to64, UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 }, // mips asserts
+  { DEFOP(Iop_DivModS128to64, UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 }, // mips asserts
+  { DEFOP(Iop_DivModS64to64,  UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // mips asserts
+  { DEFOP(Iop_DivModU64to64,  UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // mips asserts
+  { DEFOP(Iop_8Uto16,    UNDEF_ZEXT),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_8Uto32,    UNDEF_ZEXT),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_8Uto64,    UNDEF_ZEXT),   .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // ppc32 assert
+  { DEFOP(Iop_16Uto32,   UNDEF_ZEXT),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_16Uto64,   UNDEF_ZEXT),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // ppc32 assert
+  { DEFOP(Iop_32Uto64,   UNDEF_ZEXT),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // mips asserts
+  { DEFOP(Iop_8Sto16,    UNDEF_SEXT),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_8Sto32,    UNDEF_SEXT),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_8Sto64,    UNDEF_SEXT),   .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // ppc32, mips assert
+  { DEFOP(Iop_16Sto32,   UNDEF_SEXT),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_16Sto64,   UNDEF_SEXT),   .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // ppc32, mips assert
+  { DEFOP(Iop_32Sto64,   UNDEF_SEXT),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // mips asserts
+  { DEFOP(Iop_64to8,     UNDEF_TRUNC),  .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 1, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // ppc32, mips assert
+  { DEFOP(Iop_32to8,     UNDEF_TRUNC),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_64to16,    UNDEF_TRUNC),  .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1, .loongarch64 = 0 }, // ppc32, mips assert
+  { DEFOP(Iop_16to8,     UNDEF_TRUNC),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_16HIto8,   UNDEF_UPPER),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_8HLto16,   UNDEF_CONCAT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },  // ppc isel
+  { DEFOP(Iop_32to16,    UNDEF_TRUNC),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_32HIto16,  UNDEF_UPPER),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_16HLto32,  UNDEF_CONCAT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },  // ppc isel
+  { DEFOP(Iop_64to32,    UNDEF_TRUNC),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // mips asserts
+  { DEFOP(Iop_64HIto32,  UNDEF_UPPER),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_32HLto64,  UNDEF_CONCAT), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // mips asserts
+  { DEFOP(Iop_128to64,   UNDEF_TRUNC),  .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // mips asserts
+  { DEFOP(Iop_128HIto64, UNDEF_UPPER),  .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // mips asserts
+  { DEFOP(Iop_64HLto128, UNDEF_CONCAT), .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // mips asserts
+  { DEFOP(Iop_32to1,     UNDEF_TRUNC),  .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_64to1,     UNDEF_TRUNC),  .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1, .loongarch64 = 0 }, // ppc32, mips assert
+  { DEFOP(Iop_1Uto8,     UNDEF_ZEXT),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_1Uto32,    UNDEF_ZEXT),   .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_1Uto64,    UNDEF_ZEXT),   .s390x = 1, .amd64 = 1, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 0, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // ppc32 assert
+  { DEFOP(Iop_1Sto8,     UNDEF_ALL),    .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 0 },
   { DEFOP(Iop_1Sto16,    UNDEF_ALL), }, // not handled by mc_translate
-  { DEFOP(Iop_1Sto32,    UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_1Sto64,    UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_AddF64,    UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 }, // mips asserts
-  { DEFOP(Iop_SubF64,    UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 }, // mips asserts
-  { DEFOP(Iop_MulF64,    UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 }, // mips asserts
-  { DEFOP(Iop_DivF64,    UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 }, // mips asserts
-  { DEFOP(Iop_AddF32,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_AddF16,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .arm64 = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_SubF32,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_SubF16,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .arm64 = 1,.ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_MulF32,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_DivF32,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_AddF64r32, UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_SubF64r32, UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_MulF64r32, UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_DivF64r32, UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_NegF64,    UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_AbsF64,    UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_NegF32,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_NegF16,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .arm64 = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_AbsF32,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_AbsF16,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .arm64 = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_SqrtF64,   UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 }, // mips asserts
-  { DEFOP(Iop_SqrtF32,   UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_SqrtF16,   UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .arm64 = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_CmpF64,    UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_CmpF32,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 }, // mips asserts
-  { DEFOP(Iop_CmpF16,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .arm64 = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_CmpF128,   UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_F64toI16S, UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_F64toI32S, UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_F64toI64S, UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_F64toI64U, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_F64toI32U, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_I32StoF64, UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_I64StoF64, UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 },
-  { DEFOP(Iop_I64UtoF64, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0 }, // mips asserts
-  { DEFOP(Iop_I64UtoF32, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_I32UtoF32, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_I32UtoF64, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_F32toI32S, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_F32toI64S, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 1 },
-  { DEFOP(Iop_F32toI32U, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_F32toI64U, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_I32StoF32, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_I64StoF32, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 1 },
-  { DEFOP(Iop_F32toF64,  UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_F64toF32,  UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 1 }, // mips asserts
-  { DEFOP(Iop_ReinterpF64asI64, UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 }, // mips asserts
-  { DEFOP(Iop_ReinterpI64asF64, UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 }, // mips asserts
-  { DEFOP(Iop_ReinterpF32asI32, UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1 },
+  { DEFOP(Iop_1Sto32,    UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_1Sto64,    UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_AddF64,    UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // mips asserts
+  { DEFOP(Iop_SubF64,    UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // mips asserts
+  { DEFOP(Iop_MulF64,    UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // mips asserts
+  { DEFOP(Iop_DivF64,    UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // mips asserts
+  { DEFOP(Iop_ScaleBF64, UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 1 },
+  { DEFOP(Iop_AddF32,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_AddF16,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .arm64 = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_SubF32,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_SubF16,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .arm64 = 1,.ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_MulF32,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_DivF32,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_ScaleBF32, UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 1 },
+  { DEFOP(Iop_AddF64r32, UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_SubF64r32, UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_MulF64r32, UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_DivF64r32, UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_NegF64,    UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_AbsF64,    UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_NegF32,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_NegF16,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .arm64 = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_AbsF32,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_AbsF16,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .arm64 = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_SqrtF64,   UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // mips asserts
+  { DEFOP(Iop_RSqrtF64,  UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 1 },
+  { DEFOP(Iop_LogBF64,   UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 1 },
+  { DEFOP(Iop_SqrtF32,   UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_RSqrtF32,  UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 1 },
+  { DEFOP(Iop_LogBF32,   UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 1 },
+  { DEFOP(Iop_SqrtF16,   UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .arm64 = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_CmpF64,    UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_CmpF32,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 1 }, // mips asserts
+  { DEFOP(Iop_CmpF16,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .arm64 = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_CmpF128,   UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_F64toI16S, UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_F64toI32S, UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_F64toI64S, UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0, .loongarch64 = 1 },
+  { DEFOP(Iop_F64toI64U, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_F64toI32U, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_I32StoF64, UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_I64StoF64, UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_I64UtoF64, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 }, // mips asserts
+  { DEFOP(Iop_I64UtoF32, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_I32UtoF32, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_I32UtoF64, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_F32toI32S, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 1 },
+  { DEFOP(Iop_F32toI64S, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_F32toI32U, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_F32toI64U, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_I32StoF32, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_I64StoF32, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_F32toF64,  UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_F64toF32,  UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // mips asserts
+  { DEFOP(Iop_ReinterpF64asI64, UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // mips asserts
+  { DEFOP(Iop_ReinterpI64asF64, UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 }, // mips asserts
+  { DEFOP(Iop_ReinterpF32asI32, UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 1, .ppc32 = 1, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
   // ppc requires this op to show up in a specific context. So it cannot be
   // tested standalone on that platform.
-  { DEFOP(Iop_ReinterpI32asF32, UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1 },
-  { DEFOP(Iop_F64HLtoF128, UNDEF_CONCAT),    .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_F128HItoF64, UNDEF_UPPER),     .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_F128LOtoF64, UNDEF_TRUNC), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_AddF128,       UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_SubF128,       UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_MulF128,       UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_DivF128,       UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_MAddF128,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_MSubF128,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_NegMAddF128, UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_NegMSubF128, UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_NegF128,       UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_AbsF128,       UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_SqrtF128,      UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_I32StoF128,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_I64StoF128,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_I32UtoF128,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_I64UtoF128,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_F32toF128,     UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_F64toF128,     UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_F128toI32S,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_F128toI64S,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_F128toI32U,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_F128toI64U,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_F128toI128S,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_F128toF64,     UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_F128toF32,     UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_RndF128,        UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_TruncF128toI32S,UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_TruncF128toI32U,UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_TruncF128toI64U,UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_TruncF128toI64S,UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_AtanF64,       UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_Yl2xF64,       UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_Yl2xp1F64,     UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_PRemF64,       UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_PRemC3210F64,  UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_PRem1F64,      UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_PRem1C3210F64, UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_ScaleF64,      UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_SinF64,        UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_CosF64,        UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_TanF64,        UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_2xm1F64,       UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_RoundF128toInt, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_RoundF64toInt, UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 1 },
+  { DEFOP(Iop_ReinterpI32asF32, UNDEF_SAME), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 1, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_F64HLtoF128, UNDEF_CONCAT),    .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_F128HItoF64, UNDEF_UPPER),     .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_F128LOtoF64, UNDEF_TRUNC), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_AddF128,       UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_SubF128,       UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_MulF128,       UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_DivF128,       UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_MAddF128,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_MSubF128,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_NegMAddF128, UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_NegMSubF128, UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_NegF128,       UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_AbsF128,       UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_SqrtF128,      UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_I32StoF128,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_I64StoF128,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_I32UtoF128,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_I64UtoF128,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_F32toF128,     UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_F64toF128,     UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_F128toI32S,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_F128toI64S,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_F128toI32U,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_F128toI64U,    UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_F128toI128S,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_F128toF64,     UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_F128toF32,     UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_RndF128,        UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_TruncF128toI32S,UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_TruncF128toI32U,UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_TruncF128toI64U,UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_TruncF128toI64S,UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_AtanF64,       UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_Yl2xF64,       UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_Yl2xp1F64,     UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_PRemF64,       UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_PRemC3210F64,  UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_PRem1F64,      UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_PRem1C3210F64, UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_ScaleF64,      UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_SinF64,        UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_CosF64,        UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_TanF64,        UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_2xm1F64,       UNDEF_ALL), .s390x = 0, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_RoundF128toInt, UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_RoundF64toInt, UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 },
   { DEFOP(Iop_RoundF64toIntA0, UNDEF_ALL), .arm64 = 1 },
   { DEFOP(Iop_RoundF64toIntE, UNDEF_ALL), .arm64 = 1  },
-  { DEFOP(Iop_RoundF32toInt, UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1 },
+  { DEFOP(Iop_RoundF32toInt, UNDEF_ALL), .s390x = 1, .amd64 = 1, .x86 = 1, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 1, .mips64 = 1, .loongarch64 = 1 },
   { DEFOP(Iop_RoundF32toIntA0, UNDEF_ALL), .arm64 = 1 },
   { DEFOP(Iop_RoundF32toIntE, UNDEF_ALL), .arm64 = 1 },
-  { DEFOP(Iop_MAddF32,       UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 1 },
-  { DEFOP(Iop_MSubF32,       UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 1 },
-  { DEFOP(Iop_MAddF64,       UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 },
-  { DEFOP(Iop_MSubF64,       UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 },
-  { DEFOP(Iop_MAddF64r32,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_MSubF64r32,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_RSqrtEst5GoodF64,      UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0 },
-  { DEFOP(Iop_RoundF64toF64_NEAREST, UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 },
-  { DEFOP(Iop_RoundF64toF64_NegINF,  UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 },
-  { DEFOP(Iop_RoundF64toF64_PosINF,  UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 },
-  { DEFOP(Iop_RoundF64toF64_ZERO,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 },
-  { DEFOP(Iop_TruncF64asF32, UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1 }, // mips asserts
-  { DEFOP(Iop_RoundF64toF32, UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0 },
+  { DEFOP(Iop_MAddF32,       UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_MSubF32,       UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 0, .ppc32 = 0, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_MAddF64,       UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_MSubF64,       UNDEF_ALL), .s390x = 1, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_MAddF64r32,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_MSubF64r32,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_RSqrtEst5GoodF64,      UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
+  { DEFOP(Iop_RoundF64toF64_NEAREST, UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_RoundF64toF64_NegINF,  UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_RoundF64toF64_PosINF,  UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_RoundF64toF64_ZERO,    UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 0 },
+  { DEFOP(Iop_TruncF64asF32, UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 1, .loongarch64 = 0 }, // mips asserts
+  { DEFOP(Iop_RoundF64toF32, UNDEF_ALL), .s390x = 0, .amd64 = 0, .x86 = 0, .arm = 0, .ppc64 = 1, .ppc32 = 1, .mips32 = 0, .mips64 = 0, .loongarch64 = 0 },
   { DEFOP(Iop_RecpExpF64, UNDEF_UNKNOWN), },
   { DEFOP(Iop_RecpExpF32, UNDEF_UNKNOWN), },
 
   /* --------- Possibly required by IEEE 754-2008. --------- */
-  { DEFOP(Iop_MaxNumF64, UNDEF_ALL), .arm = 1 },
-  { DEFOP(Iop_MinNumF64, UNDEF_ALL), .arm = 1 },
-  { DEFOP(Iop_MaxNumF32, UNDEF_ALL), .arm = 1 },
-  { DEFOP(Iop_MinNumF32, UNDEF_ALL), .arm = 1 },
+  { DEFOP(Iop_MaxNumF64, UNDEF_ALL), .arm = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_MinNumF64, UNDEF_ALL), .arm = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_MaxNumAbsF64, UNDEF_ALL), .arm = 0, .loongarch64 = 1 },
+  { DEFOP(Iop_MinNumAbsF64, UNDEF_ALL), .arm = 0, .loongarch64 = 1 },
+  { DEFOP(Iop_MaxNumF32, UNDEF_ALL), .arm = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_MinNumF32, UNDEF_ALL), .arm = 1, .loongarch64 = 1 },
+  { DEFOP(Iop_MaxNumAbsF32, UNDEF_ALL), .arm = 0, .loongarch64 = 1 },
+  { DEFOP(Iop_MinNumAbsF32, UNDEF_ALL), .arm = 0, .loongarch64 = 1 },
 
   /* ------------------ 16-bit scalar FP ------------------ */
   { DEFOP(Iop_F16toF64,  UNDEF_ALL), .arm64 = 1 },
@@ -1113,15 +1123,19 @@ static irop_t irops[] = {
   { DEFOP(Iop_Max8Sx32, UNDEF_UNKNOWN), },
   { DEFOP(Iop_Max16Sx16, UNDEF_UNKNOWN), },
   { DEFOP(Iop_Max32Sx8, UNDEF_UNKNOWN), },
+  { DEFOP(Iop_Max64Sx4, UNDEF_UNKNOWN), },
   { DEFOP(Iop_Max8Ux32, UNDEF_UNKNOWN), },
   { DEFOP(Iop_Max16Ux16, UNDEF_UNKNOWN), },
   { DEFOP(Iop_Max32Ux8, UNDEF_UNKNOWN), },
+  { DEFOP(Iop_Max64Ux4, UNDEF_UNKNOWN), },
   { DEFOP(Iop_Min8Sx32, UNDEF_UNKNOWN), },
   { DEFOP(Iop_Min16Sx16, UNDEF_UNKNOWN), },
   { DEFOP(Iop_Min32Sx8, UNDEF_UNKNOWN), },
+  { DEFOP(Iop_Min64Sx4, UNDEF_UNKNOWN), },
   { DEFOP(Iop_Min8Ux32, UNDEF_UNKNOWN), },
   { DEFOP(Iop_Min16Ux16, UNDEF_UNKNOWN), },
   { DEFOP(Iop_Min32Ux8, UNDEF_UNKNOWN), },
+  { DEFOP(Iop_Min64Ux4, UNDEF_UNKNOWN), },
   { DEFOP(Iop_Mul16x16, UNDEF_UNKNOWN), },
   { DEFOP(Iop_Mul32x8, UNDEF_UNKNOWN), },
   { DEFOP(Iop_MulHi16Ux16, UNDEF_UNKNOWN), },
diff --git a/memcheck/tests/vbit-test/vtest.h b/memcheck/tests/vbit-test/vtest.h
index 5f2b2e0..33eaf3c 100644
--- a/memcheck/tests/vbit-test/vtest.h
+++ b/memcheck/tests/vbit-test/vtest.h
@@ -188,15 +188,16 @@ typedef struct {
    unsigned    immediate_type;
 
    // Indicate whether IROp can be tested on a particular architecture
-   unsigned    s390x  : 1;
-   unsigned    amd64  : 1;
-   unsigned    ppc32  : 1;
-   unsigned    ppc64  : 1;
-   unsigned    arm    : 1;
-   unsigned    arm64  : 1;
-   unsigned    x86    : 1;
-   unsigned    mips32 : 1;
-   unsigned    mips64 : 1;
+   unsigned    s390x      : 1;
+   unsigned    amd64      : 1;
+   unsigned    ppc32      : 1;
+   unsigned    ppc64      : 1;
+   unsigned    arm        : 1;
+   unsigned    arm64      : 1;
+   unsigned    x86        : 1;
+   unsigned    mips32     : 1;
+   unsigned    mips64     : 1;
+   unsigned    loongarch64: 1;
 } irop_t;
 
 
-- 
2.47.1

